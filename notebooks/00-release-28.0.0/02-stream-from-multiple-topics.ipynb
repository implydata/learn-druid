{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Streaming ingestion from multipletopics \n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "This notebook demonstrates the use of the new Druid 28.0.0 feature that allows streaming ingestion to consume messages from multiple topics. Take a look at the [documentaiton of this new feature here](https://druid.apache.org/docs/latest/development/extensions-core/kafka-supervisor-reference#ingesting-from-multiple-topics). \n",
    "\n",
    "In this notebook you will:\n",
    "- Create a topic and initiate a data feed on it\n",
    "- Create a multi-topic ingestion job that users `topicPattern` to find new topics dynamically\n",
    "- Create an additional topic that fits the same topic name pattern \n",
    "- Query multiple topic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 28.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "<!-- Profiles are:\n",
    "`druid-jupyter` - just Jupyter and Druid\n",
    "`all-services` - includes Jupyter, Druid, and Kafka\n",
    " -->\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "\n",
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "rest_client = druid.rest\n",
    "\n",
    "# client for Data Generator API\n",
    "datagen = druidapi.rest.DruidRestClient(\"http://datagen:9999\")\n",
    "\n",
    "\n",
    "status_client.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464c204-bbf1-4a94-ae02-2356b606049f",
   "metadata": {},
   "source": [
    "## Create first Kafka topic and feed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63edf2-ffd0-48c1-bfb7-8651019d1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": \"gen_social_twitter\",\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": \"social-twitter\" },\n",
    "    \"config_file\": \"social/social_posts.json\", \n",
    "    \"time\":\"1h\",\n",
    "    \"concurrency\":100,\n",
    "    \"time_type\":\"REAL\"\n",
    "}\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e312f34-313f-4eb4-9d94-98682f09eb05",
   "metadata": {},
   "source": [
    "Check the status of the job with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e6d24-7809-4c3d-bc43-66e9a625abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1) # avoid race between start of the job and its status being available\n",
    "response = datagen.get('/status/gen_social_twitter')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd176f7-c443-41ec-af7a-1e50d69fae01",
   "metadata": {},
   "source": [
    "## Create multi-topic streaming ingestion job\n",
    "\n",
    "A multi-topic streaming spec is almost identical to the single topic ingestion specs. The only difference is:\n",
    "- single topic ingestions use the `\"topic\"` property to specify topic to read from\n",
    "- multi-topic jobs use  `\"topicPattern\"` property instead with a REGEX expression that use used to match from the topics available in kafka.\n",
    "\n",
    "The following spec uses `\"topicPattern\": \"social_.*\"` which will match any topic that begins with `social_`. If a specific list of topics is preferable REGEX multi option list form will work, e.g. `\"social-twitter|social_linkedin\"`.\n",
    "\n",
    "The ingestion job automatically adds a column called `\"kafka.topic\"` that is available as a dimension and will be captured by schema discovery which is enabled in the following spec:     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c5192-367e-4305-acfc-e6b946e5aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_ingestion_spec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": {\n",
    "      \"type\": \"kafka\",\n",
    "      \"consumerProperties\": { \"bootstrap.servers\": \"kafka:9092\" },\n",
    "        \n",
    "      \"topicPattern\": \"social-.*\",   # this is new \n",
    "        \n",
    "      \"inputFormat\": { \"type\": \"kafka\",\"valueFormat\": { \"type\": \"json\" } }\n",
    "    },\n",
    "    \"tuningConfig\": {\n",
    "      \"type\": \"kafka\"\n",
    "    },\n",
    "    \"dataSchema\": {\n",
    "      \"dataSource\": \"example-social-media\",\n",
    "      \"timestampSpec\": {\n",
    "        \"column\": \"time\",\n",
    "        \"format\": \"iso\"\n",
    "      },\n",
    "      \"dimensionsSpec\": {\n",
    "        \"dimensions\": [ ],\n",
    "        \"useSchemaDiscovery\": True\n",
    "      },\n",
    "      \"granularitySpec\": {\n",
    "        \"queryGranularity\": \"none\",\n",
    "        \"rollup\": False,\n",
    "        \"segmentGranularity\": \"hour\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d2db8-427d-4bba-9184-4c029fcfe764",
   "metadata": {},
   "source": [
    "Send the spec to Druid to start the streaming ingestion from Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee991b-a7de-484e-b8f1-52f24bc69c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "supervisor = rest_client.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)\n",
    "print(supervisor.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c73bb-10bb-4a97-b566-a9fdea8774ef",
   "metadata": {},
   "source": [
    "A `200` response indicates that the request was successful. You can view the running ingestion task and the new datasource in the web console's [ingestion view](http://localhost:8888/unified-console.html#ingestion).\n",
    "\n",
    "The following cell pauses further execution until the ingestion has started and the datasource is available for querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21510baa-e4d2-4a5b-9593-6f615ba8c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.sql.wait_until_ready('example-social-media', verify_load_status=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91b8bb-052c-421d-968f-e4cfbff9a51e",
   "metadata": {},
   "source": [
    "You can see the supervisor job in the [Druid Console Supervisor View](http://localhost:8888/unified-console.html#supervisors). Click the magnifying glass icon to view the status of the job. The `startingOffsets` property lists kafka partitions that the job is consuming from, each partition is identified by `\"<topic-name>:<partition #>\"` which will list all partitions from all topics that have been discovered :\n",
    "```\n",
    "{\n",
    "  \"dataSource\": \"social_media\",\n",
    "  \"stream\": \"social-.*\",\n",
    "  \"partitions\": 1,\n",
    "  \"replicas\": 1,\n",
    "  \"durationSeconds\": 3600,\n",
    "  \"activeTasks\": [\n",
    "    {\n",
    "      \"id\": \"index_kafka_social_media_77e5722b1640edd_cnnmjgpc\",\n",
    "      \"startingOffsets\": {\n",
    "        \"social-twitter:0\": 0   <<<<<<<<<<<<<<<<<<< SEE DISCOVERED TOPICS HERE\n",
    "      },\n",
    "      \"startTime\": \"2023-11-03T20:52:41.173Z\",\n",
    "      ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f92b7-10d1-41f6-84aa-8bacf11f98f2",
   "metadata": {},
   "source": [
    "The following query shows the last few minutes of activity from topics being captured so far. The data only includes `kafka.topic` = `social-twitter`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda117fb-14ed-4198-a5a7-6b1f2a461b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    SELECT TIME_FLOOR(\"__time\", 'PT1M') as \"minute\", \n",
    "       \"kafka.topic\",   SUM(views) as \"total_views\" \n",
    "    FROM \"example-social-media\" \n",
    "    WHERE __time >= CURRENT_TIMESTAMP - INTERVAL '5' MINUTE\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 1 DESC, 3 DESC\n",
    "    LIMIT 5\n",
    "'''\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1c903-dfd4-4b1d-855b-8e7ce0fafae7",
   "metadata": {},
   "source": [
    "## Add another topic that matches the pattern\n",
    "\n",
    "The supervisor job that controls the streaming ingestion will periodically check for new topics that match the `topicPattern` and automatically begin consuming from them when they appear.\n",
    "The following cell initiates a second topic called `social-linkedin` and begins streaming data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f04f6c-3112-4029-a7fa-da4364f8b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": \"gen_social_linkedin\",\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": \"social-linkedin\" },\n",
    "    \"config_file\": \"social/social_posts.json\", \n",
    "    \"time\":\"1h\",\n",
    "    \"concurrency\":500,\n",
    "    \"time_type\":\"REAL\"\n",
    "}\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers)\n",
    "time.sleep(1) # avoid race between start of the job and its status being available\n",
    "response = datagen.get('/status/gen_social_linkedin')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d214977-9ba3-48e7-b93d-16054b3a2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.get_json('/jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8c0de-1c13-4cca-b81a-bf69a3b2429a",
   "metadata": {},
   "source": [
    "## Query the multi-topic data\n",
    "Try the following query a few times. It will initially only show `social-twitter` activity, when the supervisor picks up the new topic you will see it appear. It can take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    SELECT TIME_FLOOR(\"__time\", 'PT1M') as \"minute\", \n",
    "       \"kafka.topic\",   SUM(views) as \"total_views\" \n",
    "    FROM \"example-social-media\" \n",
    "    WHERE __time >= CURRENT_TIMESTAMP - INTERVAL '5' MINUTE\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 1 DESC, 3 DESC\n",
    "    LIMIT 5\n",
    "'''\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98846a26-f46d-47ef-ad4c-e09a096367c6",
   "metadata": {},
   "source": [
    "## Cleanup \n",
    "The following cell stops data generation, ingestion jobs and removes the datasource from Druid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dec76f-c5f8-4787-ad20-c25e069b4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stop streaming generator: [{datagen.post('/stop/gen_social_linkedin','',require_ok=False)}]\")\n",
    "print(f\"Stop streaming generator: [{datagen.post('/stop/gen_social_twitter','',require_ok=False)}]\")\n",
    "\n",
    "print(f'Pause streaming ingestion: [{druid.rest.post(\"/druid/indexer/v1/supervisor/example-social-media/suspend\",\"\", require_ok=False)}]')\n",
    "print(f'Shutting down running tasks ...')\n",
    "tasks = druid.tasks.tasks(state='running', table='example-social-media')\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table='example-social-media')       \n",
    "print(f'Reset offsets for re-runnability: [{druid.rest.post(\"/druid/indexer/v1/supervisor/example-social-media/reset\",\"\", require_ok=False)}]')\n",
    "print(f'Terminate streaming ingestion: [{druid.rest.post(\"/druid/indexer/v1/supervisor/example-social-media/terminate\",\"\", require_ok=False)}]')\n",
    "\n",
    "print(f\"Drop datasource: [{druid.datasources.drop('example-social-media')}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96687b-6b35-40ed-baec-ef739bdb80b5",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "This tutorial showed you how to create a Kafka topic using a Python client for Kafka, send a simulated stream of data to Kafka using a data generator, and query and visualize results over time. For more information, see the following resources:\n",
    "\n",
    "* [Apache Kafka ingestion](https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html)\n",
    "* [Querying data](https://druid.apache.org/docs/latest/tutorials/tutorial-query.html)\n",
    "* [Tutorial: Run with Docker](https://druid.apache.org/docs/latest/tutorials/docker.html)"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
