{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Query from Deep Storage covering the whole timeline\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "Apache Druid is known for fast performance on both historical and real-time data. These queries are resolved through the synchronous API endpoints `/druid/v2` & `/druid/v2/sql` that use Druid's native engine.\n",
    "\n",
    "Asynchronous queries from Deep Storage with API endpoint `/druid/v2/sql/statements` were introduced in the 27.0.0 release along with Druid retention rules with no historical caching. This allows Druid users to query longer timeframes without the need to cache all data in the historical layer. This set of features enabled users to optimize their cluster cost by only cacheing recent data in the historical layer and leaving older segment data available for queries in deep storge. \n",
    "\n",
    "At that stage, asynchronous queries were only accessing segments that were avaialble in Deep Storage, so the latest streaming ingested data would not be visible in the query results.\n",
    "\n",
    "With Druid 28.0.0, the asynchronous query capability is expanded to query real-time tasks which allows this type of query to access the complete timeline.\n",
    "\n",
    "This tutorial demonstrates how to work with [Query From Deep Storage](https://druid.apache.org/docs/latest/api-reference/sql-api#query-from-deep-storage). In this tutorial you perform the following tasks:\n",
    "\n",
    "- Generate 3 months of data and use batch ingestion to load it.\n",
    "- Setup stream ingestion for live data from generator.\n",
    "- Setup Retention rules to only hold 1 month of data in Historical cache.\n",
    "- Use synchronous and asynchronous queries to show timeline coverage of each case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# get druid host from param if available\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "# get kafka host from param if available\n",
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "\n",
    "#setup Druid API clients\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display_client = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "rest_client = druid.rest\n",
    "\n",
    "# client for Data Generator API\n",
    "datagen = druidapi.rest.DruidRestClient(\"http://datagen:9999\")\n",
    "\n",
    "# define header for REST calls\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c40ebc-e4cd-424e-bd68-5ad1ad9cce43",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "This set of functions help to control the data flow in the notebook:\n",
    "- wait_for_datagen - check the status of a data generation job, displays it and loops until the status is COMPLETE.\n",
    "- monitor_ingestion - waits for the target table to have a certain number of rows, this is used to monitor the completion of an ingestion.\n",
    "- stop_streaming_job - gracefully shuts down a streaming ingestion job and resets its partition offsets so that it can be reexecuted from the start\n",
    "- drop_table - marks all of the segments in a table as unused and executes a `kill` operation to cleanup the metadata and Deep Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd925b-d062-443b-8093-ee0902c5ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wait for the messages to be fully published \n",
    "def wait_for_datagen( job_name): \n",
    "    done = False\n",
    "    while not done:\n",
    "        result = datagen.get_json(f\"/status/{job_name}\",'')\n",
    "        clear_output(wait=True)\n",
    "        print(json.dumps(result, indent=2))\n",
    "        if result[\"status\"] == 'COMPLETE':\n",
    "            done = True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "# monitor ingestion by counting the rows ingested until the expected number of rows have been loaded\n",
    "def monitor_ingestion( target_table:str, target_rows:int):\n",
    "    row_count=0\n",
    "    while row_count<target_rows:\n",
    "        res = sql_client.sql(f'SELECT count(1) as \"count\" FROM {target_table}')\n",
    "        clear_output(wait=True)\n",
    "        print(json.dumps(res, indent=2))\n",
    "        row_count = res[0]['count']\n",
    "        time.sleep(1)\n",
    "        \n",
    "# suspend the streaming ingestion job and wait for tasks to publish their segments\n",
    "def stop_streaming_job( target_table: str, reset_offsets: bool = False):\n",
    "    print(f'Pause streaming ingestion: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/suspend\",\"\", require_ok=False)}]')\n",
    "    \n",
    "\n",
    "    tasks = druid.tasks.tasks(state='running', table=target_table)\n",
    "    tasks_done = 0\n",
    "    while tasks_done<len(tasks):\n",
    "        tasks_done = 0\n",
    "        clear_output( wait=True)\n",
    "        print(f'Waiting for running tasks to publish their segments ...')\n",
    "        for task in tasks:\n",
    "            status = druid.tasks.task_status(task['id'])\n",
    "            print(f\"Task [{task['id']}] Status:{status['status']['statusCode']} RunnerStatus:{status['status']['runnerStatusCode']}\")\n",
    "            if (status['status']['statusCode']!='RUNNING'): \n",
    "                tasks_done += 1 \n",
    "        time.sleep(1)\n",
    "            \n",
    "    if reset_offsets:\n",
    "        print(f'Reset offsets for re-runnability: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/reset\",\"\", require_ok=False)}]')\n",
    "    print(f'Terminate streaming ingestion: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/terminate\",\"\", require_ok=False)}]')\n",
    "\n",
    "# Remove table data and metadata from Druid\n",
    "def drop_table( target_table: str):\n",
    "    # mark segments as unused \n",
    "    druid.datasources.drop(target_table)\n",
    "    # remove segment metadata and data for unused segments\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    kill_task = {\n",
    "      \"type\": \"kill\",\n",
    "      \"dataSource\": target_table,\n",
    "      \"interval\" : \"2000-09-12/2999-09-13\"\n",
    "    }\n",
    "    print(druid.rest.post(f\"/druid/indexer/v1/task\", json.dumps(kill_task),require_ok=False, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Generate history\n",
    "Run the following cell to create 3 months of history up to midnight last night. Later in the notebook we'll use retention rules to split the 3 months into one month that is available in historicals and the rest only when [queried from Deep Storage](https://druid.apache.org/docs/latest/querying/query-deep-storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5150583-1da1-4238-8614-2c741b222976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 90 days of click data\n",
    "days_of_history = 90\n",
    "\n",
    "start_time = datetime.now()\n",
    "start_time = start_time - timedelta(days=days_of_history)\n",
    "start_date = start_time.strftime('%Y-%m-%dT%H:%M:%S.001')\n",
    "print(f\"Starting to generate history at {start_date}.\")\n",
    "\n",
    "# Give the datagen job a name for use in subsequent API calls\n",
    "job_name=\"gen_clickstream_history\"\n",
    "\n",
    "# Generate a data file on the datagen server\n",
    "datagen_request = {\n",
    "    \"name\": job_name,\n",
    "    \"target\": { \"type\": \"file\", \"path\":\"clicks-90-days.json\"},\n",
    "    \"config_file\": \"clickstream/clickstream.json\", \n",
    "    \"time_type\": start_date,\n",
    "    \"time\": f\"{days_of_history*24}h\",\n",
    "    \"concurrency\":2\n",
    "}\n",
    "\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "\n",
    "wait_for_datagen(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e780633-14e0-427f-8d94-57af203d4a24",
   "metadata": {},
   "source": [
    "Wait here while the prior cell completes. Generation started with a timestamp 90 days ago. You can see progress of data generation by looking at the output above where `total_records` shows the number of events generated and `status_msg` displays the simulated time in `Sim Clock` which will catchup when it reaches 90 days from the `start_time` and the `status` shows \"COMPLETE\".\n",
    "\n",
    "In the following cell the data is ingested from this generated data file using SQL Based ingestion.\n",
    "When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1293bd-f8d6-42a5-92fd-5076ffd9150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate ingestion job\n",
    "sql='''\n",
    "REPLACE INTO \"example-clicks-full-timeline\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"http://datagen:9999/file/clicks-90-days.json\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"time\" VARCHAR, \"user_id\" VARCHAR, \"event_type\" VARCHAR, \"client_ip\" VARCHAR, \"client_device\" VARCHAR, \"client_lang\" VARCHAR, \"client_country\" VARCHAR, \"referrer\" VARCHAR, \"keyword\" VARCHAR, \"product\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"time\") AS \"__time\",\n",
    "  \"user_id\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\",\n",
    "  \"referrer\",\n",
    "  \"keyword\",\n",
    "  \"product\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "display_client.run_task(sql)\n",
    "sql_client.wait_until_ready('example-clicks-full-timeline')\n",
    "display_client.table('example-clicks-full-timeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca41d1-8b6d-4c84-9de9-42be52f09d1e",
   "metadata": {},
   "source": [
    "## Create data stream and streaming ingestion\n",
    "In order to setup the full test of this feature, we'll need to complement the batch ingestion above with real-time ingestion.\n",
    "The following cell initiates data generation in real-time streaming the events to kafka in topic `clicks` and it is setup to run for 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f7845-e56a-477a-88de-4fb31b6bd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the datagen job a name for use in subsequent API calls\n",
    "job_name=\"gen_clickstream_stream\"\n",
    "\n",
    "# Generate streaming data in real time\n",
    "datagen_request = {\n",
    "    \"name\": job_name,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": \"clicks\" },\n",
    "    \"config_file\": \"clickstream/clickstream.json\", \n",
    "    \"time_type\": \"REAL\",\n",
    "    \"time\": \"4h\",\n",
    "    \"concurrency\":2\n",
    "}\n",
    "output = datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969335a-ea34-42f3-96a6-984cf0a2f299",
   "metadata": {},
   "source": [
    "### Start the streaming ingestion job in Druid\n",
    "The following streaming ingestion job reads from topic `clicks` and it makes use of `useSchemaDiscovery: True` to [automatically detect the columns](https://druid.apache.org/docs/latest/ingestion/schema-design#schema-auto-discovery-for-dimensions) in the stream. It sets the `segmentGranularity` equivalent to the batch ingestion you ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f314edb-439d-413b-bbc2-7dd965ad24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start streaming ingestion job\n",
    "kafka_ingestion_spec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": { \"type\": \"kafka\",  \"consumerProperties\": { \"bootstrap.servers\": \"kafka:9092\" },\n",
    "      \"topic\": \"clicks\",\n",
    "      \"inputFormat\": { \"type\": \"kafka\", \"valueFormat\": { \"type\": \"json\" }   },\n",
    "       \"useEarliestOffset\": True\n",
    "    },\n",
    "    \"tuningConfig\": { \"type\": \"kafka\"  },\n",
    "    \"dataSchema\": {\n",
    "      \"dataSource\": \"example-clicks-full-timeline\",\n",
    "      \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "      \"dimensionsSpec\": {\n",
    "        \"dimensions\": [ ],\n",
    "        \"useSchemaDiscovery\": True\n",
    "      },\n",
    "      \"granularitySpec\": {\n",
    "        \"queryGranularity\": \"none\",\n",
    "        \"rollup\": False,\n",
    "        \"segmentGranularity\": \"day\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "druid.rest.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e253d-00ee-4e74-8e31-4e724cb0c324",
   "metadata": {},
   "source": [
    "### Nothing new so far\n",
    "\n",
    "It might take it a minute or two to start making the real-time data available. \n",
    "There are 3 months of data in the table and the next query shows when it has caught up to almost now. Run it a few times until you see the full 90+ days in the data and the most recent event within a minute. Events are being generated irregularly, but there should be at least one every minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd200ed0-5d3e-4d0d-ba0c-f4d310e38133",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      min(__time) \"min_time\", \n",
    "      max(__time) \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, max(__time), CURRENT_TIMESTAMP) \"recent_event_seconds_ago\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "display_client.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabea50b-bb3a-496b-a3b0-2c256922a446",
   "metadata": {},
   "source": [
    "You can see the number of segments assigned to the Historical and the Peon running the real-time ingestion with the following system table query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bac420-4ee7-4f77-abdf-be651e5752ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "  b.\"server\",\n",
    "  c.\"server_type\",\n",
    "  a.\"is_realtime\",\n",
    "  COUNT(*) AS \"segments\",\n",
    "  SUM(a.\"num_replicas\") AS \"total_replicas\"\n",
    "FROM \"sys\".\"segments\" a\n",
    "LEFT JOIN \"sys\".\"server_segments\" b ON a.\"segment_id\" = b.\"segment_id\"\n",
    "LEFT JOIN \"sys\".\"servers\" c ON b.\"server\" = c.\"server\"\n",
    "WHERE \"datasource\" = 'example-clicks-full-timeline'\n",
    "GROUP BY 1, 2, 3\n",
    "'''\n",
    "display_client.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662a937-71cc-4b65-95cf-ea2c4943f947",
   "metadata": {},
   "source": [
    "## Turn off pre-fetch for older data using retention rules\n",
    "The [load and drop rules](https://druid.apache.org/docs/latest/operations/rule-configuration#load-rules) in the next cell adjust the rules followed by the [coordinator](https://druid.apache.org/docs/latest/design/coordinator) when it distributes segment to the Historical processes. The setting for tieredReplicants is different according to the period of time that a given segment of example-clicks-full-timeline table data covers.\n",
    "\n",
    "- The first rule covers the period up to one month ago (P1M) and adjusts the number of replicas on the default historical tier to 1.\n",
    "- The second rule covers the period up to three months ago (P3M) and sets the number of replicas to an empty list, instructing the coordinator only keep this data in Deep Storage removing it from Historicals if they already have it.\n",
    "- The third rule, dropForever, catches all other data from the table and marks it for deletion.\n",
    "When you run the cell, the rules will be updated and used in the coordinator's next cycle. Since the data was already loaded and distributed to the Historical, the coordinator will instruct the historical to remove segments older than 1 month from its local cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa02315-6353-4cea-a6af-4d1fa90786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_rule = [\n",
    "    {\"type\":\"loadByPeriod\", \"period\":\"P1M\", \"tieredReplicants\": { \"_default_tier\": 1} },\n",
    "    {\"type\":\"loadByPeriod\", \"period\":\"P3M\", \"tieredReplicants\": { }, \"useDefaultTierForNull\": False }, \n",
    "    {\"type\":\"dropForever\" }\n",
    "]\n",
    "\n",
    "druid.rest.post(\"/druid/coordinator/v1/rules/example-clicks-full-timeline\", json.dumps(retention_rule), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376fc69-969b-4fe0-a408-79ad218aa4f1",
   "metadata": {},
   "source": [
    "## Synchronous queries with native engine \n",
    "It might take a minute or two for the cluster to re-organize the data.\n",
    "\n",
    "The coordinator will ask the historical servers to offload any copies of segment data that are older than 1 month ago from the first rule's period `P1M`. \n",
    "\n",
    "Try the following cell multiple times until you see this result.\n",
    "- `min_time` will now be 30 days ago \n",
    "- `max_time` will continue to keep up with real-time.\n",
    "- `days_of_data` will now report 31 days, the past 30 plus the one we continue to load\n",
    "- `recent_event_seconds_ago` should be up to date, events occur about 1 every minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcf5d7-5f0c-432c-8319-29c7060c0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      min(__time) \"min_time\", \n",
    "      max(__time) \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, max(__time), CURRENT_TIMESTAMP) \"recent_event_seconds_ago\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "display_client.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe1962-7a9c-44de-addd-bbde2d4967ab",
   "metadata": {},
   "source": [
    "Run the following cell to look at the segments now, notice that the historical segments were reduced and now most of the segments are not assigned a to a server at all, these segments only exist in Deep Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6ab11-3734-4e35-9c60-c8d0509fa173",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "  b.\"server\",\n",
    "  c.\"server_type\",\n",
    "  a.\"is_realtime\",\n",
    "  COUNT(*) AS \"segments\",\n",
    "  SUM(a.\"num_replicas\") AS \"total_replicas\"\n",
    "FROM \"sys\".\"segments\" a\n",
    "LEFT JOIN \"sys\".\"server_segments\" b ON a.\"segment_id\" = b.\"segment_id\"\n",
    "LEFT JOIN \"sys\".\"servers\" c ON b.\"server\" = c.\"server\"\n",
    "WHERE \"datasource\" = 'example-clicks-full-timeline'\n",
    "GROUP BY 1, 2, 3\n",
    "'''\n",
    "display_client.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05078320-e760-4556-9746-4937b0335ace",
   "metadata": {},
   "source": [
    "## Asynchronous queries with MSQ \n",
    "Use the API endpoint `/druid/v2/sql/statements` to run asynchronous queries using MSQ engine. It reads data directly from Deep Storage and can therefore access all 3 months of segments. The `druidapi` package includes the `async_sql` function which uses this API.  \n",
    "\n",
    "That was a long setup to describe the new feature in Druid 28.0.0!\n",
    "\n",
    "### Include real-time data in asynchrounous queries\n",
    "The `includeSegmentSource=realtime` context parameter instructs the database to include data from the real-time segments seen in the above SYS query results.\n",
    "\n",
    "Try without setting it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824a1b7-b374-4464-b5b6-2dc17a75fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      TIME_FORMAT( min(__time), 'YYYY-MM-dd hh:mm:ss') \"min_time\", \n",
    "      TIME_FORMAT(max(__time), 'YYYY-MM-dd hh:mm:ss') \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, max(__time), CURRENT_TIMESTAMP ) \"recent_event_seconds_ago\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "result = sql_client.async_sql(sql)\n",
    "result.wait_until_done() # wait until the query has completed\n",
    "display(result.rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8db109-3cf5-40e6-91db-3ac844a57c22",
   "metadata": {},
   "source": [
    "Notice that the query includes all the ingested segments starting from the date 90 days ago but the most recent events are not available, `recent_event_seconds_ago` is much larger than a 60 seconds ago.\n",
    "\n",
    "Now try with context parameter `includeSegmentSource = realtime`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761d86c-98d8-4d99-b7ad-fa6c1a2e8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      TIME_FORMAT( min(__time), 'YYYY-MM-dd hh:mm:ss') \"min_time\", \n",
    "      TIME_FORMAT(max(__time), 'YYYY-MM-dd hh:mm:ss') \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, max(__time), CURRENT_TIMESTAMP) \"most_recent_to_now_s\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"includeSegmentSource\", \"realtime\")\n",
    "result = sql_client.async_sql(req)\n",
    "result.wait_until_done()  # wait until the query has completed\n",
    "display(result.rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733c153-9a66-4ef8-889a-fe4b6f62c185",
   "metadata": {},
   "source": [
    "The result from the query covers the full timeline of data available, the ingested data available in Deep Storage plus the latest data being streamed into the table continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a32041-e9cb-4b14-a149-b63ccd4c0386",
   "metadata": {},
   "source": [
    "<a id='async_rows_per_page'></a>\n",
    "## Retrieving results page by page \n",
    "\n",
    "Use the `rowsPerPage` query context parameter to control the size of the results page.\n",
    "In the cell below, the page size is set by providing a parameter in the call to the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5a0a6-db48-45c5-9aa6-476ea9ab0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT * \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "  WHERE __time > CURRENT_TIMESTAMP - INTERVAL '2' HOUR\n",
    "  ORDER BY __time DESC\n",
    "'''\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"includeSegmentSource\", \"realtime\")\n",
    "req.add_context(\"selectDestination\", \"durableStorage\")  # needed to enable paging results\n",
    "\n",
    "# use rowsPerPage parameter to define page size, defaults to 100000\n",
    "result = sql_client.async_sql(req, rowsPerPage=10)\n",
    "\n",
    "# wait for query to be processed\n",
    "result.wait_until_done()\n",
    "\n",
    "# retrieve results one page at a time\n",
    "print(\"\\nPAGE #0\\n\")\n",
    "display(result.paged_rows(pageNum=0))\n",
    "\n",
    "print(\"\\nPAGE #1\\n\")\n",
    "display(result.paged_rows(pageNum=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove everything used in this notebook from the database and data generation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_streaming_job(\"example-clicks-full-timeline\")\n",
    "\n",
    "display(datagen.post(f\"/stop/gen_clickstream_stream\", '', require_ok=False).json())\n",
    "display(datagen.post(f\"/stop/gen_clickstream_history\", '', require_ok=False).json())\n",
    "\n",
    "drop_table(\"example-clicks-full-timeline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned about setting up retention rules for different periods to:\n",
    "* cache recent segments in historical tier\n",
    "* keep older segments available for async queries from deep storage\n",
    "* use async queries that also retrieve real-time data\n",
    "* how to retrieve results a page at a time"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
