{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Using INSERT with EXTERN to export data (experimental)\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Data ingestion into Druid can and usually will add value to the raw data by transforming or aggregating it in some form.\n",
    "Downstream systems can benefit from this ingestion time and/or query time data transforms by exporting the results of a query asynchronously.\n",
    "\n",
    "Note: [Exporting data](https://druid.apache.org/docs/latest/multi-stage-query/reference/#extern-to-export-to-a-destination) is an [experimental feature](https://druid.apache.org/docs/latest/development/experimental)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "display_client = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f0209-436a-47e2-a1f4-be6c43de0eac",
   "metadata": {},
   "source": [
    "## Load example data\n",
    "\n",
    "Once your Druid environment is up and running, ingest the sample data for this tutorial.\n",
    "\n",
    "Run the following cell to create a table called \"example-wiki-pivot-unpivot\". When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ee776-1eb9-46dc-989a-933bd7c7d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "REPLACE INTO \"example-wiki-export\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  \"isRobot\",\n",
    "  \"channel\",\n",
    "  \"added\",\n",
    "  \"user\",\n",
    "  \"deleted\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "display_client.run_task(sql)\n",
    "sql_client.wait_until_ready('example-wiki-export')\n",
    "display_client.table('example-wiki-export')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad757a-957e-465a-a284-855c14e3e8fd",
   "metadata": {},
   "source": [
    "## Exporting data\n",
    "\n",
    "[Exporting data](https://druid.apache.org/docs/latest/multi-stage-query/reference/#extern-to-export-to-a-destination) uses MSQ INSERT to an EXTERN table function to define the output location and format of the data.\n",
    "\n",
    "The SQL statement takes on the form:\n",
    "```\n",
    "INSERT INTO\n",
    "  EXTERN(<destination function>)\n",
    "AS CSV\n",
    "SELECT\n",
    "  ...\n",
    "```\n",
    "\n",
    "- Destination function: [S3 destination function](https://druid.apache.org/docs/latest/multi-stage-query/reference#s3), [local destination function](https://druid.apache.org/docs/latest/multi-stage-query/reference#local)\n",
    "- AS CSV is the format of the exported file(s), new formats are expected as this experimental function evolves\n",
    "- SELECT statement can be any MSQ query including transformations, joins and aggregations\n",
    "\n",
    "In this learning environment the `druid_export_storage_baseDir` property has been set to `/opt/shared/exports` in the docker compose \"environment\" file.\n",
    "The volume `/opt/shared` is accessible to all Druid processes as well as the Jupyter Labs container.\n",
    "\n",
    "Note: This tutorial demonstrates how to export the results of an asynchronous query to a local file system. In a cluster environment, local storage will not be a good choice; [export to S3](https://druid.apache.org/docs/latest/multi-stage-query/reference/#s3) instead.\n",
    "\n",
    "\n",
    "The output folder must exist and be empty. \n",
    "Run the following cell to remove the folder if it exists and then create under /opt/shared/exports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474e38b-07aa-4f7e-9275-2a2cd18ac0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/shared/exports/example-wiki-export\n",
    "!mkdir -p /opt/shared/exports/example-wiki-export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9b601-dfa6-48a3-a00b-e710a812d2e2",
   "metadata": {},
   "source": [
    "Run the following command to export of all data in the `example-wiki-export` datasource.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36cb33-1e8a-4284-966c-a3937c8a33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "    INSERT INTO \n",
    "       EXTERN ( local(exportPath => '/opt/shared/exports/example-wiki-export') )\n",
    "       AS CSV\n",
    "    SELECT\n",
    "      *\n",
    "    FROM \"example-wiki-export\" \n",
    "'''\n",
    "display_client.run_task(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9571de0-eac8-40f0-8970-f137aec9e767",
   "metadata": {},
   "source": [
    "Run the following command to list the contents of the exports folder.\n",
    "\n",
    "The filename contains:\n",
    "\n",
    "- query Id - identifies the specific query that generated the file(s)\n",
    "- worker task Id - identifies the MSQ worker that generated the file\n",
    "- a partition number - one partition for each file generated by each worker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cef199-430d-4094-b006-090d1013af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/shared/exports/example-wiki-export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580ec6e-3172-4829-97bc-53b71c72ec0e",
   "metadata": {},
   "source": [
    "You can see the content of the file with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95265824-7962-4dd6-a93f-1a6281633c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /opt/shared/exports/example-wiki-export/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe9b15-0724-419a-ad97-1a35971f1f47",
   "metadata": {},
   "source": [
    "## Exporting transformed data\n",
    "Export works for any SQL SELECT statement.\n",
    "\n",
    "Run following cell to export aggregate query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c044cdb-4631-4e3f-9552-4cedc0b598d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/shared/exports/example-wiki-export-agg\n",
    "!mkdir -p /opt/shared/exports/example-wiki-export-agg\n",
    "\n",
    "sql='''\n",
    "INSERT INTO \n",
    "       EXTERN ( local(exportPath => '/opt/shared/exports/example-wiki-export-agg') )\n",
    "       AS CSV\n",
    "\n",
    "SELECT \"user\" as \"user\",\n",
    "       \"channel\" as \"channel\",\n",
    "       SUM(\"added\"+\"deleted\") as \"total_changes\"\n",
    "FROM \"example-wiki-export\"\n",
    "GROUP BY 1,2\n",
    "'''\n",
    "display_client.run_task(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987b88a-3d10-41ac-825a-b716fd002fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /opt/shared/exports/example-wiki-export-agg/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbc0f6-11f9-4d1f-b1df-3f1af6ae2fdc",
   "metadata": {},
   "source": [
    "## Controlling the size of the output files\n",
    "You can control the size of the output files by using the `rowsPerPage` parameter.\n",
    "Since there are only 25k rows in this dataset, the following example uses 5000 rows per file to show how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88365e3b-f91f-40fe-a688-807bffa44b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/shared/exports/example-wiki-export-parts\n",
    "!mkdir -p /opt/shared/exports/example-wiki-export-parts\n",
    "sql='''\n",
    "INSERT INTO \n",
    "       EXTERN ( local(exportPath => '/opt/shared/exports/example-wiki-export-parts') )\n",
    "       AS CSV\n",
    "\n",
    "SELECT * \n",
    "FROM \"example-wiki-export\"\n",
    "'''\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"rowsPerPage\", 5000)\n",
    "display_client.run_task(req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f3d6b-9024-4db8-823d-8a349c568d6d",
   "metadata": {},
   "source": [
    "The following cell shows the 5 files that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be357f-9061-4b75-9d9c-1336728b9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/shared/exports/example-wiki-export-parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ee163-d10b-44d4-8741-e5edec101066",
   "metadata": {},
   "source": [
    "Run the next cell to count the number of rows in each file, you'll see that each one is somewhat evenly sized and close to the 5000 row target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba5d7a-fd49-45fb-bc0e-1ca61a143dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /opt/shared/exports/example-wiki-export-parts/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33496d-8ea7-4c09-80d4-879a5c2d1d04",
   "metadata": {},
   "source": [
    "The next cell shows the beginning of each file. Notice that each files has the column headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c00f5-4da8-4561-b9b4-d9818ba521c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 /opt/shared/exports/example-wiki-export-parts/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the table created for this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop(\"example-wiki-export\")\n",
    "druid.datasources.drop(\"example-wiki-export-parts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Druid can export the results of a query to external files. \n",
    "It parallelizes the process with \n",
    "\n",
    "Learn more about this experimental function in the [Apache Druid documentation](https://druid.apache.org/docs/latest/multi-stage-query/reference/#extern-to-export-to-a-destination)."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
