{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# (Result) by (action) using (feature)\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Adding dimensions and metrics to your data can enhance its analytic value. It's common, for example, to add product categorization, user demographics or additional location based metrics to Retail clickstream or POS data. In IoT scenarios, additional info like metric type (temperature, pressure, flow, etc) for a particular device is common, the device can be associated to a specific industrial process and grouped into components and subcomponents of the overall system being monitored are very useful in determining subsystem anomalies. \n",
    "\n",
    "Lookups and joins can be used at query time to enhance data in this fashion. But there is a performance penalty when using lookups and even more penalty with joins at query time. So in the interest of achieving fast analytic queries, joins can be applied at ingestion time.\n",
    "\n",
    "This tutorial demonstrates how to work with [feature](link to feature doc). In this tutorial you perform the following tasks:\n",
    "\n",
    "- Task 1\n",
    "- Task 2\n",
    "- Task 3\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid XX.0.0 or later.\n",
    "\n",
    "<!-- Profiles are:\n",
    "`druid-jupyter` - just Jupyter and Druid\n",
    "`all-services` - includes Jupyter, Druid, and Kafka\n",
    " -->\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `<PLACE PROFILE NAME HERE>` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b769122-c5a4-404e-9ef8-9c0ebd97695a",
   "metadata": {},
   "source": [
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
    "###Â Set up a connection to Apache Kafka\n",
    "\n",
    "<!-- Include these cells if your notebook uses Kafka. -->\n",
    "\n",
    "Run the next cell to set up the connection to Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffed9a-87a5-4b26-9f06-972fdbccd55a",
   "metadata": {},
   "source": [
    "### Set up a connection to the data generator\n",
    "\n",
    "<!-- Include these cells if your notebook uses the data generator. -->\n",
    "\n",
    "Run the next cell to set up the connection to the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807280f-b387-4ffb-8bc3-764c5b511458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "datagen_host = \"http://datagen:9999\"\n",
    "datagen_headers = {'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d6b39-6551-4b2a-bdfb-9606aa92c853",
   "metadata": {},
   "source": [
    "<!-- Include these cells if you need additional Python modules -->\n",
    "\n",
    "### Import additional modules\n",
    "\n",
    "Run the following cell to import additional Python modules that you will use to X, Y, Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c2524-0eba-4bc6-84ed-da3a25aa5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your modules here, remembering to align this with the prerequisites section\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Create a table using batch ingestion\n",
    "\n",
    "<!-- Use these cells if you are using batch ingestion for your notebook. -->\n",
    "\n",
    "Run the following cell to create a table using batch ingestion. Notice {the use of X as a timestamp | only required columns are ingested | WHERE / expressions / GROUP BY are front-loaded | partitions on X period and clusters by Y}.\n",
    "\n",
    "When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace example-dataset-topic with a unique table name for this notebook.\n",
    "\n",
    "# - Always prefix your table name with `example-`\n",
    "# - If using the standard example datasets, use the following standard values for `dataset`:\n",
    "\n",
    "#     wikipedia       wikipedia\n",
    "#     koalas          KoalasToTheMax one day\n",
    "#     koalanest       KoalasToTheMax one day (nested)\n",
    "#     nyctaxi3        NYC Taxi cabs (3 files)\n",
    "#     nyctaxi         NYC Taxi cabs (all files)\n",
    "#     flights         FlightCarrierOnTime (1 month)\n",
    "\n",
    "# Remember to apply good data modelling practice to your INSERT / REPLACE.\n",
    "\n",
    "table_name = 'example-dataset-topic'\n",
    "\n",
    "sql='''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "'''\n",
    "\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready(table_name)\n",
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ff0b2-ee98-40d6-b0b7-77274e34881d",
   "metadata": {},
   "source": [
    "## Create a table using streaming ingestion\n",
    "\n",
    "In this section, you use the data generator to generate a stream of messages into a Apache Kafka topic. Next, you'll set up an on-going ingestion into Druid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b732-c843-4483-896d-c7972ba9e4be",
   "metadata": {},
   "source": [
    "### Use the data generator to populate a Kafka topic\n",
    "\n",
    "Run the following cell to instruct the data generator to start producing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ef424-cd35-416c-9cd0-14f71d43d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more information on the available configurations and settings for the data generator, see the dedicated notebook in \"99-contributing\"\n",
    "\n",
    "# Replace example-dataset-topic with a unique table name for this notebook.\n",
    "\n",
    "# - Always prefix your table name with `example-`\n",
    "# - If using the standard example datasets, use the following standard values for `dataset`:\n",
    "\n",
    "#     social/socialposts            social\n",
    "#     clickstream/clickstream       clickstream\n",
    "\n",
    "# Remember to apply good data modelling practice to your data schema.\n",
    "\n",
    "datagen_topic = \"example-dataset-topic\"\n",
    "datagen_job = datagen_topic\n",
    "datagen_config = \"social/social_posts.json\"\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": datagen_job,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": datagen_topic  },\n",
    "    \"config_file\": datagen_config, \n",
    "    \"time\":\"10m\",\n",
    "    \"concurrency\":100\n",
    "}\n",
    "\n",
    "requests.post(f\"{datagen_host}/start\", json.dumps(datagen_request), headers=datagen_headers)\n",
    "requests.get(f\"{datagen_host}/status/{datagen_job}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fedf1f-566f-4501-be65-d20e216d2c59",
   "metadata": {},
   "source": [
    "### Use streaming ingestion to populate the table\n",
    "\n",
    "Ingest data from an Apache Kafka topic into Apache Druid by submitting an [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html) to the [streaming ingestion supervisor API](https://druid.apache.org/docs/latest/api-reference/supervisor-api).\n",
    "\n",
    "Run the next cell to set up the [`ioConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig), [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig), and [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). Notice {the use of X as a timestamp | only required columns are ingested | WHERE / expressions / GROUP BY are front-loaded | partitions on X period and clusters by Y}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8734c-e968-45de-a1bf-197d3bffb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "    \"topic\": datagen_topic,\n",
    "    \"inputFormat\": { \"type\": \"json\" },\n",
    "    \"useEarliestOffset\": \"true\" }\n",
    "\n",
    "tuningConfig = { \"type\": \"kafka\" }\n",
    "\n",
    "# Replace example-dataset-topic with a unique table name for this notebook.\n",
    "\n",
    "# - Always prefix your table name with `example-`\n",
    "# - If using the standard example datasets, use the following standard values for `dataset`:\n",
    "\n",
    "#     social/socialposts            social\n",
    "#     clickstream/clickstream       clickstream\n",
    "\n",
    "# Remember to apply good data modelling practice to your data schema.\n",
    "\n",
    "table_name = 'example-dataset-topic'\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "    \"granularitySpec\": { \"rollup\": \"false\", \"segmentGranularity\": \"hour\" },\n",
    "    \"dimensionsSpec\": { \"useSchemaDiscovery\" : \"true\"}\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "sql_client.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c9b88-837d-4c80-a28d-36184ba63355",
   "metadata": {},
   "source": [
    "#### Broadcast joins - small lookups joins\n",
    "\n",
    "SQL Based Ingestion can process joins efficiently during ingestion using either broadcast or sort merge joins. Broadcast is the default method, in which the right table of the join is broadcast in its entirety to all workers involved in the ingestion. The content of the lookup is kept within each worker's memory in order to process the join. You'll need to take care that the whole set of lookup tables joined in this fashion for a given ingestion will fit within the heap of each worker JVM.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f62dfe-7a7a-42b0-8db8-c36d6d35712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-kttm-enhanced-batch\" OVERWRITE ALL\n",
    "WITH\n",
    "kttm_data AS (\n",
    "  SELECT * \n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-v2/kttm-v2-2019-08-25.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"timestamp\" VARCHAR, \"agent_category\" VARCHAR, \"agent_type\" VARCHAR, \"browser\" VARCHAR, \"browser_version\" VARCHAR, \"city\" VARCHAR, \"continent\" VARCHAR, \"country\" VARCHAR, \"version\" VARCHAR, \"event_type\" VARCHAR, \"event_subtype\" VARCHAR, \"loaded_image\" VARCHAR, \"adblock_list\" VARCHAR, \"forwarded_for\" VARCHAR, \"language\" VARCHAR, \"number\" VARCHAR, \"os\" VARCHAR, \"path\" VARCHAR, \"platform\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"region\" VARCHAR, \"remote_address\" VARCHAR, \"screen\" VARCHAR, \"session\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR, \"window\" VARCHAR)\n",
    "),\n",
    "country_lookup AS (\n",
    "  SELECT * \n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/lookup/countries.tsv\"]}',\n",
    "      '{\"type\":\"tsv\",\"findColumnsFromHeader\":true}'\n",
    "    )\n",
    "  ) EXTEND (\"Country\" VARCHAR, \"Capital\" VARCHAR, \"ISO3\" VARCHAR, \"ISO2\" VARCHAR)\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  TIME_PARSE(kttm_data.\"timestamp\") AS __time,\n",
    "  kttm_data.\"session\",\n",
    "  kttm_data.\"agent_category\",\n",
    "  kttm_data.\"agent_type\",\n",
    "  kttm_data.\"browser\",\n",
    "  kttm_data.\"browser_version\",\n",
    "  kttm_data.\"language\",\n",
    "  kttm_data.\"os\",\n",
    "  kttm_data.\"city\",\n",
    "  kttm_data.\"country\",\n",
    "  country_lookup.\"Capital\" AS \"capital\",\n",
    "  country_lookup.\"ISO3\" AS \"iso3\",\n",
    "  kttm_data.\"forwarded_for\" AS \"ip_address\",\n",
    "  kttm_data.\"session_length\",\n",
    "  kttm_data.\"event_type\"\n",
    "FROM kttm_data\n",
    "LEFT JOIN country_lookup ON country_lookup.Country = kttm_data.country\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-kttm-enhanced-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4675c3-9fb2-4f66-b93e-57a6faf378fe",
   "metadata": {},
   "source": [
    "Data for both sources \"kttm_data\" and \"country_lookup\" are obtained from external sources:\n",
    "```sql\n",
    "WITH\n",
    "kttm_data AS\n",
    "(\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "               '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-v2/kttm-v2-2019-08-25.json.gz\"]}',\n",
    "               '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"timestamp\" VARCHAR, \"agent_category\" VARCHAR, ...)\n",
    "),\n",
    "country_lookup AS\n",
    "(\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/lookup/countries.tsv\"]}',\n",
    "      '{\"type\":\"tsv\",\"findColumnsFromHeader\":true}'\n",
    "    )\n",
    "  ) EXTEND (\"Country\" VARCHAR, \"Capital\" VARCHAR, \"ISO3\" VARCHAR, \"ISO2\" VARCHAR)\n",
    ")\n",
    "```\n",
    "\n",
    "Columns from both tables can be used in the SELECT expressions using the alias \"country_lookup\" to reference any joined column:\n",
    "```sql\n",
    "  kttm_data.\"country\",\n",
    "  country_lookup.\"Capital\" AS \"capital\",\n",
    "  country_lookup.\"ISO3\" AS \"iso3\"\n",
    "```\n",
    "\n",
    "The join is specified in the FROM clause:\n",
    "```sql\n",
    "FROM kttm_data\n",
    "LEFT JOIN country_lookup ON country_lookup.Country = kttm_data.country\n",
    "```\n",
    "LEFT JOIN insured that all the rows from kttm_data source are ingested. An INNER JOIN would exclude rows from \"kttm_data\" if the value for \"kttm_data.country\" is not present in \"country_lookup.Country\". \n",
    "Since no context parameters were set, the join is processed as a broadcast join. The first table in the FROM clause is the distributed table and all other joined tables will be shipped to the workers to execute the join.\n",
    "\n",
    "Take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a0d90-2950-41ce-ac5c-20c73f61a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT\n",
    "  \"iso3\" AS \"country_code\", \n",
    "  \"capital\",\n",
    "  count( DISTINCT \"ip_address\" ) distinct_users, \n",
    "  MIN(\"session_length\")/1000 fastest_session_ms,\n",
    "  MAX(\"session_length\")/1000 slowest_session_ms\n",
    "FROM \"example-kttm-enhanced-batch\"\n",
    "WHERE \"event_type\"='LayerClear'\n",
    "GROUP BY 1,2\n",
    "ORDER BY 3 DESC\n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873900b-caf9-4dae-a674-e0093d512152",
   "metadata": {},
   "source": [
    "#### Shuffle joins - \"Large lookup to fact\" or \"fact to fact\" joins\n",
    "\n",
    "See [Shuffle joins in SQL Based Ingestion](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sort-merge).\n",
    "This is the ability to join large tables to other large tables without fully loading either one into memory. Both sources involved in the join are scanned in parallel across all workers, the intermediate data for both sources is then redistributed among the workers based on the join column(s) such that rows from both sources with the same values end up in the same worker.\n",
    "\n",
    "![](assets/shuffle-join.png)\n",
    "\n",
    "In order to use shuffle join the query context must include:\n",
    "```\n",
    "{\n",
    "   \"sqlJoinAlgorithm\":\"sortMerge\"\n",
    "}\n",
    "```\n",
    "\n",
    "Given that this example is meant to run on the local docker compose deployment, two very large tables is not possible, try it out with small sources and just pretend they are big. We'll use the \"wikipedia\" sample data and join it with \"example-wiki-users-batch\" profile data. But first, create the users table because at the time of this writing there wasn't a matching \"user\" source handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a4721-b105-48d6-92aa-2c1fedc57706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we don't have a source for example-wiki-users-batch let's\n",
    "# create one using in-database transformation that\n",
    "# generates user profiles from the wikipedia data by\n",
    "# - grouping on \"user\"\n",
    "# - injecting variability into the \"group\" column using modulus of the __time of the event\n",
    "# - \"edits\" represent the number of edits done by the user, so just count the number of events\n",
    "# - calculate the registration time using the minimum __time from events and adjusting it some variable years back in time\n",
    "# - determine the preferred language of the user based on their earliest channel edit\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"example-wiki-users-batch\" OVERWRITE ALL\n",
    "SELECT \n",
    "  \"user\", \n",
    "  EARLIEST(\n",
    "    CASE \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),5) > 3 THEN 'Reviewers' \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),17) > 13 THEN 'Patrollers' \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),23) > 21 THEN 'Bots'\n",
    "      ELSE 'Autoconfirmed'\n",
    "    END,\n",
    "    1024\n",
    "  ) AS \"group\",\n",
    "  count(*) \"edits\",\n",
    "  TIME_SHIFT(MIN(__time), 'P1Y', -1 * MOD(MIN(EXTRACT (MICROSECOND FROM __time)),20) ) AS \"registered_at_ms\",\n",
    "  EARLIEST(SUBSTRING(\"channel\", 2, 2), 1024) AS \"language\"\n",
    "FROM \"example-wikipedia-batch\"\n",
    "GROUP BY 1\n",
    "PARTITIONED BY ALL\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)              # init request object\n",
    "request.add_context( 'finalizeAggregations', True) # EARLIEST functions will store a partial aggregation otherwise\n",
    "request.add_context( 'maxNumTasks', 2)             # can't go any higher in test env\n",
    "\n",
    "druid.display.run_task(request)\n",
    "druid.sql.wait_until_ready('example-wiki-users-batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470cb8a-fa8d-47e2-8e1a-2b1d9cab9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "The next cell runs an ingestion using the sortMerge join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376978df-5598-438e-ab78-c57fe4eff02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-wiki-merge-batch\" OVERWRITE ALL\n",
    "WITH \"wikidata\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT \n",
    "  TIME_PARSE(d.\"timestamp\") as \"__time\",\n",
    "  d.\"isRobot\", \n",
    "  d.\"channel\" , \n",
    "  d.\"timestamp\" , \n",
    "  d.\"flags\" , \n",
    "  d.\"isUnpatrolled\" , \n",
    "  d.\"page\" , \n",
    "  d.\"diffUrl\" , \n",
    "  d.\"added\" , \n",
    "  d.\"comment\" , \n",
    "  d.\"commentLength\" , \n",
    "  d.\"isNew\" , \n",
    "  d.\"isMinor\" , \n",
    "  d.\"delta\" , \n",
    "  d.\"isAnonymous\" , \n",
    "  d.\"user\" , \n",
    "  d.\"deltaBucket\" , \n",
    "  d.\"deleted\" , \n",
    "  d.\"namespace\" , \n",
    "  d.\"cityName\" , \n",
    "  d.\"countryName\" , \n",
    "  d.\"regionIsoCode\" , \n",
    "  d.\"metroCode\" , \n",
    "  d.\"countryIsoCode\" , \n",
    "  d.\"regionName\", \n",
    "  u.\"group\" AS \"user_group\",\n",
    "  u.\"edits\" AS \"user_edits\",\n",
    "  u.\"registered_at_ms\" AS \"user_registration_epoch\",\n",
    "  u.\"language\" AS \"user_language\"\n",
    "FROM \"wikidata\" AS d \n",
    "  LEFT JOIN \"example-wiki-users-batch\" AS u ON u.\"user\"=d.\"user\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)                 # init request object\n",
    "request.add_context( 'sqlJoinAlgorithm', 'sortMerge') # use sortMerge to join the sources\n",
    "request.add_context( 'maxNumTasks', 2)                # use 2 tasks to run the ingestion\n",
    "\n",
    "druid.display.run_task(request)\n",
    "druid.sql.wait_until_ready('example-wiki-merge-batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed859f33-716c-469e-8a82-99e64e945bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the next cell to query the newly joined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5a005-285d-4368-b6ee-52975777c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT \"user_group\",\n",
    "  count( DISTINCT \"user\") \"distinct_users\",\n",
    "  sum(\"user_edits\") \"total_activity\"\n",
    "FROM \"example-wiki-merge-batch\"\n",
    "GROUP BY 1\n",
    "ORDER BY 1, 3 DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the XXX used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for batch ingested tables\n",
    "\n",
    "print(f\"Drop table: [{druid.datasources.drop(table_name)}]\")\n",
    "\n",
    "# Use this when doing streaming with the data generator\n",
    "\n",
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop table: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You learned this\n",
    "* Remember this\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try this out on your own data\n",
    "* Solve for problem X that is't covered here\n",
    "* Read docs pages\n",
    "* Watch or read something cool from the community\n",
    "* Do some exploratory stuff on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d3362-b1a4-47a4-a782-9773c216b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some useful code elements that you can re-use.\n",
    "\n",
    "# When just wanting to display some SQL results\n",
    "sql = f'''SELECT * FROM \"{table_name}\" LIMIT 5'''\n",
    "display.sql(sql)\n",
    "\n",
    "# When ingesting data and wanting to describe the schema\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready('{table_name}')\n",
    "display.table('{table_name}')\n",
    "\n",
    "# When you want to show the native version of a SQL statement\n",
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))\n",
    "\n",
    "# When you want a simple plot\n",
    "df = pd.DataFrame(sql_client.sql(sql))\n",
    "df.plot(x='x-axis', y='y-axis', marker='o')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.show()\n",
    "\n",
    "# When you want to add some query context parameters\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateTopN\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "# When you want to compare two different sets of results\n",
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3\n",
    "\n",
    "# When you want to see some messages from a Kafka topic\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=kafka_host)\n",
    "consumer.subscribe(topics=datagen_topic)\n",
    "count = 0\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "consumer.unsubscribe()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
