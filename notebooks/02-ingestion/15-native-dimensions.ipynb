{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Define schemas for incoming stream data\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "During streaming ingestion, the schema for events written into a table from a stream are set in the `dimensionsSpec`. This tutorial demonstrates various ways to work with the [dimensionsSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimensionsspec) against an example stream of events.\n",
    "\n",
    "In this tutorial, you perform the following tasks:\n",
    "\n",
    "- Set up a streaming ingestion from Apache Kafka.\n",
    "- Start an ingestion that consumes specific dimensions and writes them into a table.\n",
    "- Amend the ingestion to consume all but specific dimensions.\n",
    "- Run an ingestion using automatic schema discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c28c8b-1ae7-4b18-8c76-844375ab29cc",
   "metadata": {},
   "source": [
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "    \n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
    "Run the next cell to set up the connection to Apache Kafka and Data Generator, and import helper functions for use later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kafka\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "datagenUrl = \"http://datagen:9999\"\n",
    "\n",
    "generalHeaders = {'Content-Type': 'application/json'}\n",
    "\n",
    "if (os.environ['KAFKA_HOST'] == None):\n",
    "    kafka_host=f\"kafka:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b7439-ad21-4808-96b1-8e3c992fa51e",
   "metadata": {},
   "source": [
    "### Start a data stream\n",
    "\n",
    "Run the next cell to use the learn-druid Data Generator to create a stream that we can consume from.\n",
    "\n",
    "This creates clickstream sample data for an hour and publishes it to a Kafka topic for Druid to consume from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec019-7145-4005-bb85-ea25eda7bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name=\"example-social-dimensions\"\n",
    "topic_name = job_name\n",
    "\n",
    "target = {\n",
    "    \"type\":\"kafka\",\n",
    "    \"endpoint\": kafka_host,\n",
    "    \"topic\": topic_name\n",
    "}\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": topic_name,\n",
    "    \"target\": target,\n",
    "    \"config_file\": \"social/social_posts.json\",\n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":10,\n",
    "    \"time_type\": \"REAL\"\n",
    "}\n",
    "\n",
    "requests.post(f\"{datagenUrl}/start\", json.dumps(datagen_request), headers=generalHeaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dde5e8-237e-4531-84c2-8647d92ceaea",
   "metadata": {},
   "source": [
    "### Set up ingestion specification basics\n",
    "\n",
    "Run the following cell to create an [ioConfig](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig) object that sets the connection to the topic from Druid along with a very simple [tuningConfig](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig) object for the tuning configuration for the ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb89079-7e2a-404b-be85-d9fc7c97d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"consumerProperties\": {\n",
    "    \"bootstrap.servers\": \"kafka:9092\"\n",
    "  },\n",
    "  \"topic\": topic_name,\n",
    "  \"inputFormat\": {\n",
    "    \"type\": \"json\"\n",
    "  },\n",
    "  \"useEarliestOffset\": \"false\"\n",
    "}\n",
    "\n",
    "tuningConfig = { \"type\": \"kafka\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee01e6-d620-4ca7-9f2b-d49ab839ca5a",
   "metadata": {},
   "source": [
    "The third part of the ingestion specification defines the [dataSchema](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). In the cells that follow, you will define all three parts:\n",
    "\n",
    "* [timestampSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec) uses the `time` column from the generated data as the primary timestamp.\n",
    "* [granularitySpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#granularityspec) uses the primary timestamp to write data into daily partitions.\n",
    "* [dimensionsSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimensionsspec) defines what data to create inside the target table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284bc813-dd75-49aa-bac2-10d1016fff46",
   "metadata": {},
   "source": [
    "## Configure the timestamp and partitioning scheme\n",
    "\n",
    "Run the next cell to see a sample of the raw data being emitted from the Data Generator.\n",
    "\n",
    "This cell uses a simple consumer to subscribe to the topic and show the first five rows that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128c2a4-588b-4bf6-a0f9-2f002f0ecdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    " bootstrap_servers=kafka_host\n",
    ")\n",
    "\n",
    "consumer.subscribe(topics=topic_name)\n",
    "count = 0\n",
    "\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "\n",
    "consumer.unsubscribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6693b19-be98-4a8b-b3a5-737823175f1d",
   "metadata": {},
   "source": [
    "Each event includes a timestamp in the `time` field in ISO standard format.\n",
    "\n",
    "Run the following cell to set the primary timestamp to this field with the [format](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec) set as \"iso\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6b53a-d1f8-4f9a-9f54-14ccdf423b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_timestampSpec = {\n",
    "    \"column\": \"time\",\n",
    "    \"format\": \"iso\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd2912-553f-466c-a3ee-f5b793aee22e",
   "metadata": {},
   "source": [
    "Run the next cell to set the primary partitioning for your table to `HOUR`.\n",
    "\n",
    "Read more about this important design consideration in the official documentation on [partitioning](https://druid.apache.org/docs/latest/ingestion/partitioning) and [segment size optimization](https://druid.apache.org/docs/latest/operations/segment-optimization).\n",
    "\n",
    "Notice that you also disable ingestion-time aggregation ([rollup](https://druid.apache.org/docs/latest/ingestion/rollup)) inside the `granularitySpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804095a-1b9f-48dd-9c13-11c1c083e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_granularitySpec = {\n",
    "    \"rollup\": \"false\",\n",
    "    \"segmentGranularity\": \"hour\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79aee4-9dcf-4963-ab06-9086bc0604ff",
   "metadata": {},
   "source": [
    "You have now created the first two parts of the `dataSchema` that deal with treatment and use of a primary timestamp.\n",
    "\n",
    "Reviewing the sample data, we can now turn our attention to the options for the final part of the `dataSchema`: the `dimensionsSpec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02beae03-25e9-4ce0-a785-ccfb13ec36cb",
   "metadata": {},
   "source": [
    "## Explicitly set dimensions\n",
    "\n",
    "Run the next cell to create a `dimensionsSpec` object that uses the \"explicit\" method for ingesting events.\n",
    "\n",
    "Notice that it is made up of [dimension objects](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimension-objects) inside a `dimensions` list - the \"edited\" field has been left out intentionally.\n",
    "\n",
    "There are two flavors of dimension object:\n",
    "\n",
    "* Dimensions that ingested using all defaults, bringing in data as a string with a bitmap index.\n",
    "* Dimensions that have specific types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987c712-8670-4fbd-b3c6-d072efb439a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_dimensionsSpec = {\n",
    "    \"dimensions\": [\n",
    "        \"username\",\n",
    "        \"post_title\",\n",
    "        {\n",
    "            \"name\" : \"views\",\n",
    "            \"type\" : \"long\" },\n",
    "        {\n",
    "            \"name\" : \"upvotes\",\n",
    "            \"type\" : \"long\" },\n",
    "        {\n",
    "            \"name\" : \"comments\",\n",
    "            \"type\" : \"long\" }\n",
    "        ]\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e24a56-3d9e-4589-8d0c-113a5b5ec8e9",
   "metadata": {},
   "source": [
    "Run the next cell to create the final `dataSchema`. Notice that the table name is also defined here.\n",
    "\n",
    "Beneath this it is combined with the `ioConfig` and `tuningConfig` to create a native [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69af4ad-00e2-4d9b-b7fc-5725dfe9e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = topic_name\n",
    "\n",
    "dataSchema = {\n",
    "      \"dataSource\": table_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50d1bb-1bc9-4ef1-a1a7-637cc3bb4616",
   "metadata": {},
   "source": [
    "Run the next cell to start ingestion raw data from Kafka to Druid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5125-1096-462a-a637-cd8f438a2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc897578-48d7-4b8c-9ece-dab4390e2336",
   "metadata": {},
   "source": [
    "Run the following cell to wait until the ingestion has started and the new table is ready for query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830e767-ad9a-47c0-80e2-4f19cd4a20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01430f85-eb06-4999-92f2-b789b3a4f611",
   "metadata": {},
   "source": [
    "Run the following cell to get details about the table you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17dd96-cf4b-44ab-bed4-cfa43fd67f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654936f-8288-4fd5-a4ad-1f94267e43ae",
   "metadata": {},
   "source": [
    "The `type` tells you how Druid will interpret the data in SQL. Notice that the `dimensionsSpec` has caused Druid to apply a type of BIGINT to `views`, `upvotes`, and `comments`.\n",
    "\n",
    "Learn more about data types in the dedicated [notebook on data types](../02-ingestion/04-table-datatypes.ipynb).\n",
    "\n",
    "Run the next cell to stop ingestion and drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118c6d0-46a0-40e2-ad38-a740205d2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/suspend\",\"\")}]')\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "        \n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca324d3-26c5-4033-a102-4f73a80401d1",
   "metadata": {},
   "source": [
    "## Explicitly exclude dimensions\n",
    "\n",
    "Run the next cell to create a `dimensionsSpec` object that uses the \"exclusion\" method for ingesting events.\n",
    "\n",
    "Notice that it is made up of the names of dimensions to exclude from the incoming data inside `dimensionExclusions` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727467c-e6af-4879-80e1-c2cac368d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_dimensionsSpec = {\n",
    "    \"dimensionExclusions\": [\n",
    "        \"username\",\n",
    "        \"edited\"\n",
    "        ]\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d7546-8e58-473a-b482-37ed8176c88f",
   "metadata": {},
   "source": [
    "Now incorporate this adaptation into the overall ingestion specification by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21542771-2f28-4ca3-8bb6-f051f66b5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = {\n",
    "      \"dataSource\": table_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19f727-67ed-4090-b83a-5fefb2d7bd89",
   "metadata": {},
   "source": [
    "Submit the revised specification for this table to Druid by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877e620-0558-4e58-b799-5ddc5e5862af",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e66285-c411-442e-9bdf-0b53e3322e57",
   "metadata": {},
   "source": [
    "Run the next cell to view the schema for the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb3e45-e89c-4e62-9bd7-61d2c2d57013",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b39c2-af0f-4430-9d51-aec6b5e4789d",
   "metadata": {},
   "source": [
    "Notice that `views`, `upvotes`, and `comments` have a type of VARCHAR.\n",
    "\n",
    "As before, stop ingestion and drop the table by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea728193-bfb0-46e4-9984-25549297efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/suspend\",\"\")}]')\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "        \n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb153971-e522-4769-adbe-bd21d95014e4",
   "metadata": {},
   "source": [
    "## Use automatic schema discovery\n",
    "\n",
    "Now set up your `dimensionsSpec` to instruct Druid to discover dimensions and determine a data type automatically by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efe405-2758-47cd-a532-1c820a3f4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_dimensionsSpec = {\n",
    "    \"useSchemaDiscovery\" : \"true\" }\n",
    "\n",
    "dataSchema = {\n",
    "      \"dataSource\": table_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20470873-bf5b-48ee-8842-22ccdcd084af",
   "metadata": {},
   "source": [
    "Submit the revised specification for this table to Druid by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e52d35-6f29-4747-b706-4a16672160da",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea350ef-5c77-4cc4-ae38-7a8b61e5679f",
   "metadata": {},
   "source": [
    "Review the schema for the table by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b18176-9e11-415c-ad97-5ce12db71713",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8f87b-0b06-4061-8bd2-1f5e7a02f480",
   "metadata": {},
   "source": [
    "Notice that `views`, `upvotes`, and `comments` have been detected as a BIGINT.\n",
    "\n",
    "Stop ingestion and drop the table by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4daaa9-3bc5-40f8-a4d5-869a1e73f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/suspend\",\"\")}]')\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "        \n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to stop the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stop streaming generator: [{requests.post(f'{datagenUrl}/stop/{job_name}','')}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* The schema of incoming data is defined in the `dimensionsSpec` and is realized in the target table.\n",
    "* Dimensions can be explicitly included and typed, explicitly excluded, or automatically detected and typed.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Review the documentation on the [`dimensionsSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimensionsspec).\n",
    "* Review the documentation on [partitioning](https://druid.apache.org/docs/latest/ingestion/partitioning) and [segment size optimization](https://druid.apache.org/docs/latest/operations/segment-optimization).\n",
    "* Run through the dedicated [notebook on data types](../02-ingestion/04-table-datatypes.ipynb).\n",
    "* Learn about [changing schemas](https://druid.apache.org/docs/latest/data-management/schema-changes) in Druid.\n",
    "* Experiment with combining batch and streaming data in the same table."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
