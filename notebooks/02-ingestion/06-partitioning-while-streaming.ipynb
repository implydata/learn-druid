{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Optimizing performance with streaming ingestion\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "In the [05-partitioning-data.ipynb](05-partitioning-data.ipynb) notebook you learned about Apache Druid's data sharding strategy and how it can be used to improve system health and query performance. That was all about batch ingestion, but what happens with streaming ingestion?\n",
    "\n",
    "Streaming ingestion is inherently different because it is optimized for scalable throughput. Streaming ingestion scales up by adding more tasks. Each task ingests portions of the data from a stream and generates segment files idependently from other tasks. With more parallel tasks, this causes more fragmentation. We will review how compaction tasks are used to merge segments and how compaction can apply a secondary partitioning strategy in order to optimize the data after ingestion.\n",
    "\n",
    "In this notebook you will setup a streaming ingestion job and understand how its settings translate into segment creation. You'll learn about fragmentation of the data and how to optimize it after initial ingestion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works was tested with Druid 27.0.0.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "<!-- Profiles are:\n",
    "`druid-jupyter` - just Jupyter and Druid\n",
    "`all-services` - includes Jupyter, Druid, and Kafka\n",
    " -->\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [the project on github](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "# shortcuts for APIs\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "# client for Data Generator API\n",
    "datagen = druidapi.rest.DruidRestClient(\"http://datagen:9999\")\n",
    "\n",
    "# REST client for Druid API\n",
    "rest_client = druid.rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebce99-ce8e-4da0-aafc-f8954e3aa1e9",
   "metadata": {},
   "source": [
    "## Setup the Topic and produce data\n",
    "To test parallelism we'll need a topic with multiple partitions. Apache Druid maps one or more Kafka partitions to each task. Each task consumes from its assigned partitions to serve queries and create segments.\n",
    "A topic with 2 partitions means we can use a maximum of 2 tasks to ingest from it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd421f-9b5c-4681-a20e-5233fe90802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "topic_name='social'\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=kafka_host, \n",
    "    client_id='admin'\n",
    ")\n",
    "\n",
    "topic_list = []\n",
    "topic_list.append(NewTopic(name=topic_name, num_partitions=4, replication_factor=1))\n",
    "admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8a644-2c54-4510-b0b8-56e79230ef16",
   "metadata": {},
   "source": [
    "### Feed the Topic\n",
    "Add an hour's worth of social posts to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269611e2-8c40-4a9b-93e6-bccf4f820dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "#simulate events with timestamps starting at the top of the hour\n",
    "gen_now = datetime.now().replace(microsecond=0, second=0, minute=0)\n",
    "gen_start_time = gen_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# generate 1 million events\n",
    "total_row_count=1000000\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "datagen_request = {\n",
    "    \"name\": \"social_stream\",\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": topic_name  },\n",
    "    \"config_file\": \"social/social_posts.json\", \n",
    "    \"total_events\":total_row_count,\n",
    "    \"concurrency\":500,\n",
    "    \"time_type\": gen_start_time\n",
    "}\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348baa4-590a-4c7b-a404-4c7f7d2e55ee",
   "metadata": {},
   "source": [
    "The next cell monitors the data generation until it completes publishing messages into the Kafka topic.\n",
    "It will take a few minutes to generate 1 million messages. you can see the progress in the output field `total_records`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311628c-dbd1-4809-a5b5-c24e7c545897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wait for the messages to be fully published \n",
    "done = False\n",
    "while not done:\n",
    "    result = datagen.get_json(\"/status/social_stream\",'')\n",
    "    clear_output(wait=True)\n",
    "    print(json.dumps(result, indent=2))\n",
    "    if result[\"status\"] == 'COMPLETE':\n",
    "        done = True\n",
    "    else:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eab555-a31b-46ce-a28e-2bd7397ade8c",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "These functions will help us measure multiple attempts at ingesting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2fc28-2c01-4614-bb6a-a9a0db50d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor ingestion by counting the rows ingested until the expected number of rows have been loaded\n",
    "def monitor_ingestion( target_table:str, target_rows:int):\n",
    "    row_count=0\n",
    "    while row_count<target_rows:\n",
    "        res = sql_client.sql(f'SELECT count(1) as \"count\" FROM {target_table}')\n",
    "        clear_output(wait=True)\n",
    "        print(json.dumps(res, indent=2))\n",
    "        row_count = res[0]['count']\n",
    "        time.sleep(1)\n",
    "        \n",
    "# suspend the streaming ingestion job and wait for tasks to publish their segments\n",
    "def stop_streaming_job( target_table: str, reset_offsets: bool = False):\n",
    "    print(f'Pause streaming ingestion: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/suspend\",\"\", require_ok=False)}]')\n",
    "    \n",
    "\n",
    "    tasks = druid.tasks.tasks(state='running', table=target_table)\n",
    "    tasks_done = 0\n",
    "    while tasks_done<len(tasks):\n",
    "        tasks_done = 0\n",
    "        clear_output( wait=True)\n",
    "        print(f'Waiting for running tasks to publish their segments ...')\n",
    "        for task in tasks:\n",
    "            status = druid.tasks.task_status(task['id'])\n",
    "            print(f\"Task [{task['id']}] Status:{status['status']['statusCode']} RunnerStatus:{status['status']['runnerStatusCode']}\")\n",
    "            if (status['status']['statusCode']!='RUNNING'): \n",
    "                tasks_done += 1 \n",
    "        time.sleep(1)\n",
    "            \n",
    "    if reset_offsets:\n",
    "        print(f'Reset offsets for re-runnability: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/reset\",\"\", require_ok=False)}]')\n",
    "    print(f'Terminate streaming ingestion: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/terminate\",\"\", require_ok=False)}]')\n",
    "\n",
    "# Remove table data and metadata from Druid\n",
    "def drop_table( target_table: str):\n",
    "    # mark segments as unused \n",
    "    druid.datasources.drop(target_table)\n",
    "    # remove segment metadata and data for unused segments\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    kill_task = {\n",
    "      \"type\": \"kill\",\n",
    "      \"dataSource\": target_table,\n",
    "      \"interval\" : \"2000-09-12/2999-09-13\"\n",
    "    }\n",
    "    print(druid.rest.post(f\"/druid/indexer/v1/task\", json.dumps(kill_task),require_ok=False, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ae3c2-4c78-4054-af71-9fe8998ab39d",
   "metadata": {},
   "source": [
    "### Ingest the data with a single task\n",
    "Next, we'll try ingesting the same data with different number of tasks.\n",
    "The docker compose environment file is configured with `druid_worker_capacity=4` meaning we can have up to 4 tasks running concurrently. This config oversubscribes the CPUs available to docker, so it is not intended to show scale up, but rather the effects on segment generation when using more tasks.\n",
    "\n",
    "The first test is with 1 task and we'll run with 2 tasks and 4 tasks to see which segments are produced in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f0403-3154-47de-add1-8c613c56b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = 'social_media_1task'\n",
    "kafka_ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": { \n",
    "        \"ioConfig\": { \"type\": \"kafka\", \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "            \"topic\": topic_name,  \"taskCount\": 1, \"useEarliestOffset\": True,\n",
    "            \"inputFormat\": { \"type\": \"json\"} \n",
    "        },\n",
    "        \"tuningConfig\": { \"type\": \"kafka\"},\n",
    "        \"dataSchema\": {\n",
    "            \"dataSource\": target_table,\n",
    "            \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "            \"dimensionsSpec\": {\n",
    "                \"dimensions\": [\n",
    "                    \"username\",\n",
    "                    \"post_title\",\n",
    "                    { \"type\": \"long\", \"name\": \"views\"},\n",
    "                    { \"type\": \"long\", \"name\": \"upvotes\"},\n",
    "                    { \"type\": \"long\", \"name\": \"comments\"},\n",
    "                    \"edited\"\n",
    "                ]\n",
    "            },\n",
    "            \"granularitySpec\": { \"queryGranularity\": \"none\",  \"rollup\": False, \"segmentGranularity\": \"hour\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "supervisor = rest_client.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)\n",
    "print(f'Start supervisor response: [{supervisor.status_code}]')\n",
    "print(f'Waiting for tasks to start streaming...')\n",
    "# wait for table creation and ingestion start\n",
    "druid.sql.wait_until_ready(target_table, verify_load_status=False) \n",
    "start_time = datetime.now()\n",
    "monitor_ingestion( target_table, total_row_count)  # wait for the 1000000 rows to be loaded\n",
    "end_time = datetime.now()\n",
    "print(f'Total load time = {(end_time-start_time).total_seconds()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf045ea-af43-4bfb-891a-de8615979c78",
   "metadata": {},
   "source": [
    "My result for this run was: \n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"count\": 1000000\n",
    "  }\n",
    "]\n",
    "Total load time = 8.326123\n",
    "```\n",
    "\n",
    "Let's stop that ingestion to free up the worker slots and start the next one. \n",
    "Stopping the ingestion involves waiting for the currently running tasks to finish building and publishing their segments, this could take up to a minute when the coordinator picks up the new segment and hands it off to the historical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724f65a-46e9-4119-82de-4bda18dab198",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_streaming_job('social_media_1task', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11234789-8225-4733-a24c-7d318fd4d82e",
   "metadata": {},
   "source": [
    "### Ingest the data with two and four tasks\n",
    "The next task ingests the same data into a different table so we can compare the results.\n",
    "This one runs with 2 tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = 'social_media_2task'\n",
    "kafka_ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": { \n",
    "        \"ioConfig\": { \"type\": \"kafka\", \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "            \"topic\": topic_name,  \"taskCount\": 2, \"useEarliestOffset\": True,\n",
    "            \"inputFormat\": { \"type\": \"json\"} \n",
    "        },\n",
    "        \"tuningConfig\": { \"type\": \"kafka\"},\n",
    "        \"dataSchema\": {\n",
    "            \"dataSource\": target_table,\n",
    "            \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "            \"dimensionsSpec\": {\n",
    "                \"dimensions\": [\n",
    "                    \"username\",\n",
    "                    \"post_title\",\n",
    "                    { \"type\": \"long\", \"name\": \"views\"},\n",
    "                    { \"type\": \"long\", \"name\": \"upvotes\"},\n",
    "                    { \"type\": \"long\", \"name\": \"comments\"},\n",
    "                    \"edited\"\n",
    "                ]\n",
    "            },\n",
    "            \"granularitySpec\": { \"queryGranularity\": \"none\",  \"rollup\": False, \"segmentGranularity\": \"hour\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "supervisor = rest_client.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)\n",
    "print(f'Start supervisor response: [{supervisor.status_code}]')\n",
    "print(f'Waiting for tasks to start streaming...')\n",
    "# wait for table creation and ingestion start\n",
    "druid.sql.wait_until_ready(target_table, verify_load_status=False) \n",
    "start_time = datetime.now()\n",
    "monitor_ingestion( target_table, total_row_count)  # wait for the 1000000 rows to be loaded\n",
    "end_time = datetime.now()\n",
    "print(f'Total load time = {(end_time-start_time).total_seconds()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272f497-4307-48ea-828f-ca25179559ac",
   "metadata": {},
   "source": [
    "My results:\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"count\": 1000000\n",
    "  }\n",
    "]\n",
    "Total load time = 6.341758\n",
    "```\n",
    "\n",
    "I see some improvement which means that some of the work is being parallelized, but it is likely that I/O is is the long pole in the tent as I only have one drive. This would be different in a real cluster, but hey, 120,000+ msgs/second with 1 and 157,000+ with 2 isn't bad for my local setup.\n",
    "\n",
    "Let's stop this ingestion and try with 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7523c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_streaming_job('social_media_2task', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad07fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = 'social_media_4task'\n",
    "kafka_ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": { \n",
    "        \"ioConfig\": { \"type\": \"kafka\", \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "            \"topic\": topic_name,  \"taskCount\": 4, \"useEarliestOffset\": True,\n",
    "            \"inputFormat\": { \"type\": \"json\"} \n",
    "        },\n",
    "        \"tuningConfig\": { \"type\": \"kafka\"},\n",
    "        \"dataSchema\": {\n",
    "            \"dataSource\": target_table,\n",
    "            \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "            \"dimensionsSpec\": {\n",
    "                \"dimensions\": [\n",
    "                    \"username\",\n",
    "                    \"post_title\",\n",
    "                    { \"type\": \"long\", \"name\": \"views\"},\n",
    "                    { \"type\": \"long\", \"name\": \"upvotes\"},\n",
    "                    { \"type\": \"long\", \"name\": \"comments\"},\n",
    "                    \"edited\"\n",
    "                ]\n",
    "            },\n",
    "            \"granularitySpec\": { \"queryGranularity\": \"none\",  \"rollup\": False, \"segmentGranularity\": \"hour\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "supervisor = rest_client.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)\n",
    "print(f'Start supervisor response: [{supervisor.status_code}]')\n",
    "print(f'Waiting for tasks to start streaming...')\n",
    "# wait for table creation and ingestion start\n",
    "druid.sql.wait_until_ready(target_table, verify_load_status=False) \n",
    "start_time = datetime.now()\n",
    "monitor_ingestion( target_table, total_row_count)  # wait for the 1000000 rows to be loaded\n",
    "end_time = datetime.now()\n",
    "print(f'Total load time = {(end_time-start_time).total_seconds()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda65815",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_streaming_job('social_media_4task', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9488652-df3e-44d9-a671-372df333d01a",
   "metadata": {},
   "source": [
    "My result with 4 tasks:\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"count\": 1000000\n",
    "  }\n",
    "]\n",
    "Total load time = 5.541399\n",
    "```\n",
    "While it got a little faster, the returns are clearly diminishing and remember that testing performance with this laptop setup doesn't make much sense because all processes are sharing my laptop resources, so parallelism is limited and resource isolation is non-existent. As an example of this variability, in another run, this ingestion turned out to be 1 second slower than the 2 task run.\n",
    "\n",
    "We are focusing on the effects that the number of tasks have on segment production so let's [look at the datasources](http://localhost:8888/unified-console.html#segments) in the Druid console. Notice that the number of segments produced is the number of tasks that we running to ingest. \n",
    "\n",
    "![](assets/datasources-streaming-diif-tasks.png)\n",
    "\n",
    "In general, streaming tasks will generate at least one segment file per task duration period. They could generate more. If the __time column of the data received spans multiple segment granularity time chunks, there will be at least one segment output for each time chunk touched by the data. There could be more than one segment per time chunk if the number of rows received exceeds the `maxRowsPerSegment`. So there are a few sources of data fragmentation when running streaming ingestion that are inherent to its scalable high troughput design. \n",
    "\n",
    "## Optimize segments through compaction\n",
    "\n",
    "In a real cluster, each streaming task runs on independent resources, increasing the number of tasks increases overall throughput on the ingestion. But keeping the task count to the minimum needed is better in terms of the number of segments they'll generate. So there's a balance between the throughput that you need in order to reduce or eliminate lag and the number of segments generated.\n",
    "\n",
    "Notice that the `Shard Type` column shows \"Numbered\". This means that the segments partitions within a segment granularity (one hour in our case) are not split in any particular way, each task consumed from its assigned Kafka partitions and built one or more segments from the data it received.\n",
    "\n",
    "The [partitioning-data](05-partitioning-data.ipynb) notebook talks about ideal segment sizes and secondary partitioning (also called clustering) which reorganizes the data into segments that are more efficient at query time. Streaming ingestion does not create ideal segments. In order to optimize segments that have been ingested through streaming ingestion, it is a best practice to use [auto compaction](https://druid.apache.org/docs/latest/design/coordinator#automatic-compaction). \n",
    "\n",
    "For now, let's do a [manual compaction](https://druid.apache.org/docs/latest/data-management/compaction#setting-up-manual-compaction) to demonstrate its effects and then we'll review the setup for auto compaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d66c4e-ab16-40d9-80fe-4cab0fd761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = \"social_media_4task\"\n",
    "gen_start_time = gen_now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "gen_end_time = (gen_now + timedelta(hours=1)).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "interval = gen_start_time + '/' + gen_end_time\n",
    "compaction_task = {\n",
    "  \"type\": \"compact\",\n",
    "  \"dataSource\": target_table,\n",
    "  \"ioConfig\": {\n",
    "    \"type\": \"compact\",\n",
    "    \"inputSpec\": {\n",
    "      \"type\": \"interval\",\n",
    "      \"interval\": interval\n",
    "    }\n",
    "  },\n",
    "  \"granularitySpec\": {\n",
    "    \"segmentGranularity\": \"hour\"\n",
    "  }\n",
    "}\n",
    "compaction_response = rest_client.post(\"/druid/indexer/v1/task\", json.dumps(compaction_task), headers=headers)\n",
    "print(f'Start compaction response: [{compaction_response}]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd5e6d-2f5b-425e-b4ff-f80727245580",
   "metadata": {},
   "source": [
    "[Look at the segments](http://localhost:8888/unified-console.html#segments/datasource=social_media_4task) for `social_media_4task` now (it might take a few seconds to show the change). You'll see that the 4 segments have been compacted into one. \n",
    "\n",
    "Compaction can also apply secondary partitioning to reorganize the data within each time chunk for better segment pruning at query time. \n",
    "\n",
    "Since the table only has 1 million records and the default `targetRowsPerSegment` during compaction is 5 million, you'll need to lower that value to see the effects of secondary partitioning with this table.\n",
    "\n",
    "Here's the compaction task, notice that the `partitionsSpec` in `tuningConfig` specifies `range` partitioning with `username` as the partitioning column. It also uses targetRowsPerSegment at 250k so that we'll see the partitioning. This is just for illustration, keeping it at 5 million or so is a good initial target in a real scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e600023",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_task = {\n",
    "  \"type\": \"compact\",\n",
    "  \"dataSource\": target_table,\n",
    "  \"ioConfig\": {\n",
    "    \"type\": \"compact\",\n",
    "    \"inputSpec\": {\n",
    "      \"type\": \"interval\",\n",
    "      \"interval\": interval\n",
    "    }\n",
    "  }, \n",
    "  \"tuningConfig\": {\n",
    "    \"type\":\"compaction\",\n",
    "    \"forceGuaranteedRollup\": True,\n",
    "    \"partitionsSpec\": {\n",
    "      \"type\": \"range\",\n",
    "      \"partitionDimensions\": [\n",
    "        \"username\"\n",
    "      ],\n",
    "      \"targetRowsPerSegment\": 250000\n",
    "    }\n",
    "  } \n",
    "}\n",
    "\n",
    "\n",
    "compaction_response = rest_client.post(\"/druid/indexer/v1/task\", json.dumps(compaction_task), headers=headers)\n",
    "print(f'Start compaction response: [{compaction_response}]')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d50f3c20-e3f6-4f2f-aae8-670327d80eea",
   "metadata": {},
   "source": [
    "[Look at the segments](http://localhost:8888/unified-console.html#segments/datasource=social_media_4task) again. You'll see that it is back to 4 segments but this time they have more information in the Shard Spec column. The compaction process sorted the data by `username` and split it up into ranges of the column's values attempting to keep each segment file as close to the 250k we requested. Given the low number of user names in the data and their slight skew, we ended up with uneven segment sizes. Try to use columns with enough cardinality to avoid this issue or add other columns to the partitioning spec that will help add cardinality. The columns used for secondary partitioning should be the most common filter criteria in your queries; other than time filters that is. This will help improve query performance because the Broker will prune the segments needed and only submit the query to the Historicals with the relevant segments. \n",
    "\n",
    "![](assets/segments-range-partitioned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the tables and topics created by this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39393914-fc2b-41bc-81a3-b5e8611114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop tables\n",
    "\n",
    "drop_table(\"social_media_1task\")\n",
    "drop_table(\"social_media_2task\")\n",
    "drop_table(\"social_media_4task\")\n",
    "\n",
    "# remove topic from Kafka\n",
    "try:\n",
    "    admin_client.delete_topics(topics=[topic_name])\n",
    "    print(\"Topic Deleted Successfully\")\n",
    "except  Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47675083-16c1-492c-90c4-424886b6ebfe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You learned that\n",
    "  * Streaming increases throughput by using more tasks\n",
    "  * More tasks means more segments being produced\n",
    "  * Find the right balance for your use case\n",
    "  * Segments are not optimally organized for query after streaming\n",
    "  * Compaction should follow every streaming ingestion\n",
    "  * Compaction has the same partitioning and clustering capabilities as batch ingestion.\n",
    "  \n",
    "## Learn more\n",
    "\n",
    "* If you haven't already, review the [05-partitioning-data.ipynb](05-partitioning-data.ipynb) notebook which takes a look at how to address partitioning and clustering for batch ingestion.\n",
    "* Read about:\n",
    "  * [Streaming Ingestion]()\n",
    "  * [Compaction and Auto Compaction]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b985e-f6a8-4dcc-81c0-e81234bfb94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
