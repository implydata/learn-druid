{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73dbb71f-0253-4940-a7b6-0b808839a82d",
   "metadata": {},
   "source": [
    "# Batch ingestion\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "  \n",
    "In this notebook we are focusing on [SQL based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sql-reference). \n",
    "\n",
    "Batch ingestion is the process that reads raw data from files or other external batch sources and transforms them into time partitioned and fully indexed [Druid segment files](https://druid.apache.org/docs/latest/design/segments). \n",
    "\n",
    "This notebook focuses on the basics of batch ingestion in Druid, you will:\n",
    "\n",
    "- Ingest data from one or more external files\n",
    "- Learn to use the context parameters to control the parallelism of ingestion\n",
    "- Filter data at during ingestion\n",
    "- Apply transformations during ingestion\n",
    "- Ingested nested JSON columns\n",
    "- Enhance the data during ingestion using joins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Learn Druid Project page](https://github.com/implydata/learn-druid/#readme).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "if (os.environ['DRUID_HOST'] == None):\n",
    "    druid_host=f\"http://router:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4a2e1-2fbb-420b-a052-5e80885e79c1",
   "metadata": {},
   "source": [
    "## SQL based ingestion\n",
    "\n",
    "Run the following cell to load data from an external file into the \"example-wikipedia-batch\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47fcff-9055-4058-852f-6f5c61d07965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-wikipedia-batch\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-wikipedia-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776cd84-9f38-40a3-9e47-5e821d15c901",
   "metadata": {},
   "source": [
    "REPLACE or INSERT at the beginning of the statement tells Druid to execute an ingestion task. INSERT is used when appending data, REPLACE when replacing data. Both methods work when adding data to a new or empty Druid datasource. The OVERWRITE ALL clause means that the whole datasource will be replaced with the result of this ingestion. \n",
    "\n",
    "```\n",
    "REPLACE INTO \"example-wikipedia-batch\" OVERWRITE ALL\n",
    "```\n",
    "\n",
    "The WITH clause is used to declare one or more input sources, this could also be placed directly in the FROM clause of the final SELECT, but this is easier to read:\n",
    "\n",
    "```\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT ...\n",
    "      FROM TABLE (EXTERN ( ... ) )\n",
    ") EXTEND (...)\n",
    "```\n",
    "\n",
    "EXTERN supports many batch [input sources](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) and [formats](https://druid.apache.org/docs/latest/ingestion/data-formats.html). In this case the SQL statement uses input source type \"http\" to access a set or \"uris\" that each contain a data file in the \"json\" data format. Note that compressed files are allowed and will automatically be decompressed.\n",
    "```\n",
    "FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "```\n",
    "The [EXTEND clause describes the input schema](https://druid.apache.org/docs/latest/multi-stage-query/reference#extern-function) using SQL data types:\n",
    "```\n",
    "EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR,  ...)\n",
    "```\n",
    "\n",
    "The final SELECT statement defines the transformations and schema of the resulting Druid table. A \"__time\" column is usually parsed from the source, this expression will be mapped to Druid's primary time partitioning of segments. In this case we specified the \"__time\" column and ingested the rest of the columns as defined in the EXTEND clause using \"*\".\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "```\n",
    "\n",
    "The final portion of this ingestion is the PARTITIONED BY DAY clause which tells Driud to create a separate set of segments for each day. A PARTITION BY clause must be included in all INSERT/REPLACE statements.\n",
    "\n",
    "##### Wait for Segment Availibility:\n",
    "The `sql_wait_until_ready` function is used to pause until all the ingested data is available in the Historical cacheing layer before executing any queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f811a-7795-4351-bc6b-3abb34da0116",
   "metadata": {},
   "source": [
    "#### Query the data\n",
    "Let's take a look at the data that was loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2584e65-c952-47f1-a885-2e5d3a4fef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT channel,  count(*) num_events\n",
    "FROM \"example-wikipedia-batch\" \n",
    "WHERE __time BETWEEN '2016-06-27' AND '2016-06-28'\n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "druid.display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43d5f3-0350-40e6-b5a6-737c610d7562",
   "metadata": {},
   "source": [
    "## Ingesting from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340d912-2441-48ef-b6ce-e182f57734d4",
   "metadata": {},
   "source": [
    "[Druid Input Sources](https://druid.apache.org/docs/latest/ingestion/native-batch.html#splittable-input-sources) allow you to specify multiple files as input to an ingestion job.\n",
    "\n",
    "In the following example we are using the same file three times simulating multiple source files with the same schema. Normally this would be a list of different files to load: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91d16b-1880-4a4c-8e8a-650075e26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-wikipedia-3-batch\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-wikipedia-3-batch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce84c5-2c1c-4c6a-b9f4-7a22e2216ec5",
   "metadata": {},
   "source": [
    "Let's look at the data now. The quantities are 3 times larger than before because we loaded the same file three times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278c0d2-9fd5-4a7b-bb65-a6db7cf98971",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT channel, count(*) num_events\n",
    "FROM \"example-wikipedia-3-batch\" \n",
    "WHERE __time BETWEEN '2016-06-27' AND '2016-06-28'\n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751316a2-bd0c-43eb-a74f-fed15782f54d",
   "metadata": {},
   "source": [
    "## Context parameters\n",
    "Certain aspects of the ingestion can be controlled using [context parameter](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters)s. This section discussed two of the commonly used parameters:\n",
    "\n",
    "##### maxNumTasks\n",
    "The Multi-stage Query Framework uses parallel workers to execute each stage of the ingestion process. Each stage creates output partitions that organize the data in preparation for the next stage. \n",
    "\n",
    "The input stage parallelism is limited by the input sources, as each file is processed by one of the workers. While multiple input files are split evenly among parallel worker tasks. As such a single large file cannot be parallelized at this stage. You can split very large files into smaller ones to improve parallelism for the input stage.\n",
    "\n",
    "After the initial input stage, the level of parallelism of the job will remain consistent and is controlled by the [context parameter](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters) `maxNumTasks`\n",
    "\n",
    "If you are running Druid on your laptop, the default configuration only provides 4 worker slots on the Middle Manager, so you can only run with `maxNumTasks=4` resulting in one controller and one worker. If you are using this notebook against a larger Druid cluster, feel free to experiment with higher values. Note that, if `maxNumTasks` exceeds the available worker slots, the job will fail with a timeout error because it waits for all the worker tasks to be active.\n",
    "\n",
    "##### rowsPerSegment\n",
    "`rowsPerSegment` defaults to 3,000,000. You can adjust it to produce larger or smaller segments. \n",
    "\n",
    "This example shows how to set context parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55313e69-e63e-47d9-917c-2da87926ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-wikipedia-4-batch\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)         # init request object\n",
    "request.add_context( 'rowsPerSegment', 20000) # setting it low to produce many segments\n",
    "request.add_context( 'maxNumTasks', 4)        # can't go any higher in learning environment\n",
    "\n",
    "druid.display.run_task(request)\n",
    "druid.sql.wait_until_ready('example-wikipedia-4-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87e942-000a-49d3-b915-4319519c4ed1",
   "metadata": {},
   "source": [
    "With a `rowsPerSegment` of only 20,000, the same ingestion as before produces more segments. Open the [Druid console in the Data Sources view](http://localhost:8888/unified-console.html#datasources) to see the difference in segments between \"example-wikipedia-3-batch\" and \"example-wikipedia-4-batch\".\n",
    "\n",
    "Note that 20,000 is a very low value used to illustrate setting parameters. Normally this value is in the millions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f95a00-cc40-478f-aad9-cb8243f6c363",
   "metadata": {},
   "source": [
    "## Filter data during ingestion\n",
    "\n",
    "In situations where you need data cleansing or your only interested in a subset of the data, the ingestion job can filter the data by simply adding a WHERE clause.\n",
    "\n",
    "The example excludes all robotic wikipedia updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1fcaa-1e78-475b-b2ef-131aa88ead51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-wikipedia-only-human\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "\n",
    "WHERE \"isRobot\"='false'\n",
    "\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-wikipedia-only-human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c825d39-0bbd-4cd3-ae8a-760146d9fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT isRobot, channel, count(*) num_events\n",
    "FROM \"example-wikipedia-only-human\" \n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561d663-9cfe-4a62-80cc-dcbb0ce199ff",
   "metadata": {},
   "source": [
    "## Transform data during ingestion\n",
    "\n",
    "The SQL language provides a rich [set of functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html) that can be applied to input columns to transform the data as it is being ingested. All scalar SQL functions are available for normal ingestion. Rollup ingestion is discussed in the [Rollup Notebook](05-rollup.ipynb) which includes the use of aggregate functions at ingestion time as well.\n",
    "\n",
    "Here are some examples of such transformations:\n",
    "\n",
    "##### Time manipulation\n",
    "There are many [time parsing and manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#date-and-time-functions) available in Apache Druid. It is common to do some time cleansing/transformation at ingestion. Here are some examples of time manipulation functions:\n",
    "```\n",
    "  TIME_PARSE( \"timestamp\") AS \"__time\",   \n",
    "  TIME_FLOOR( TIME_PARSE( \"timestamp\"), 'P1W') AS \"week_start\",\n",
    "  TIMESTAMPDIFF( DAY,\n",
    "                 TIME_FLOOR( TIME_PARSE( \"timestamp\"), 'P1W'),\n",
    "                 TIME_PARSE( \"timestamp\")\n",
    "               ) AS \"days_since_week_start\"\n",
    "   \n",
    "```\n",
    "\n",
    "##### Use CASE statements to transform data\n",
    "CASE statements can be used to resolve complex logic and prepare columns for certain query patterns. \n",
    "Examples:\n",
    "```\n",
    "  CASE\n",
    "     WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0\n",
    "     ELSE 1\n",
    "  END AS adblock_count,\n",
    "\n",
    "  CASE\n",
    "     WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1\n",
    "     ELSE 0\n",
    "  END AS easylist_count\n",
    "```\n",
    "The two case statements above are examples of converting a categorical column like `adblock_list` into a numerical column that can be used as a meaningful metric when aggregated across different dimensions to get the count of events that were affected by an ad blocker.\n",
    "\n",
    "##### String manipulation\n",
    "Apache Druid has [string manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#string-functions) that can be very useful for transformation during ingestion. Some examples:\n",
    "```\n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version,\n",
    "  STRING_TO_ARRAY(\"app_version\",'\\.') AS version_array,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version\n",
    "```\n",
    "The above makes use of regex-based extraction, string replacement, string to array conversion and access to array elements as examples of the string transformation functions available.\n",
    "\n",
    "##### Data Flattening functions\n",
    "If you need to extract fields from nested structures in the input data, JSON_VALUE function can be used to retrieve them and cast them to the desired data type:\n",
    "```\n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "```\n",
    "\n",
    "Here's a SQL based ingestion statement that uses all of these examples and a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbddfb3-a482-477a-94a7-18a5539590ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-kttm-transform-batch\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND (\"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \"event\" TYPE('COMPLEX<json>'), \"agent\" TYPE('COMPLEX<json>'), \"client_ip\" VARCHAR, \"geo_ip\" TYPE('COMPLEX<json>'), \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \"screen\" VARCHAR, \"window\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  session, \n",
    "  number,\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  TIMESTAMPDIFF(DAY, TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W'), TIME_PARSE(\"timestamp\")) AS days_since_week_start,\n",
    "  TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W') AS week_start,\n",
    "  TIME_CEIL(TIME_PARSE(\"timestamp\"), 'P1W') AS week_end,\n",
    "  TIME_SHIFT(TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1D'),'P1D', -1) AS start_of_yesterday,\n",
    "  \n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "  \n",
    "  CASE WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0 ELSE 1 END AS adblock_count,\n",
    "  CASE WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1 ELSE 0 END AS easylist_count,\n",
    "  \n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),2) AS minor_version,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version,\n",
    "  session_length\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-kttm-transform-batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e787fe-65f5-4110-bc69-ce583285ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what time of day shows the highest user activity\n",
    "druid.display.sql(\"\"\"\n",
    "SELECT EXTRACT( HOUR FROM \"__time\") time_hour, city, count(distinct \"session\") session_count\n",
    "FROM \"example-kttm-transform-batch\" \n",
    "WHERE \"city\" IS NOT NULL AND \"city\" <> ''\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d47d9-ceaa-4623-9e8e-215b3ef391a4",
   "metadata": {},
   "source": [
    "## Nested columns\n",
    "\n",
    "Apache Druid supports ingestion of [nested columns](https://druid.apache.org/docs/latest/querying/nested-columns.html). These are columns that contain nested structures with their own set of fields which in turn are either have literal values or nested structures as well. Druid can automatically parse nested columns and index all internal fields into columnar form. This makes all fields in the JSON objects available for fast filtering and aggregation just as if they were top level columns. The schema of the nested columns is automatically discovered and access to the columns is through familiar JSON paths by using the JSON_VALUE function.\n",
    "\n",
    "This example loads the Koalas to the Max sample dataset that includes multiple nested columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89b2f6-cbff-437e-9468-278651947039",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-kttm-nested-batch\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND ( \"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \n",
    "               \"event\" TYPE('COMPLEX<json>'), \n",
    "               \"agent\" TYPE('COMPLEX<json>'), \n",
    "               \"client_ip\" VARCHAR, \n",
    "               \"geo_ip\" TYPE('COMPLEX<json>'), \n",
    "               \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \n",
    "               \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \n",
    "               \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \n",
    "               \"screen\" VARCHAR, \"window\" VARCHAR, \n",
    "               \"session_length\" BIGINT, \"timezone\" VARCHAR, \n",
    "               \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\", *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-kttm-nested-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57016cb-ff17-420d-882b-174ec6fd5ab9",
   "metadata": {},
   "source": [
    "As you can see, ingesting nested columns is straight forward. All you need to do is declare them as \"TYPE('COMPLEX<json>')\", include the input field in the main SELECT clause ( * = all columns ) and you're done!\n",
    "Take a look at the query example below where we access these nested fields as dimensions we can group by, metrics we can aggregate and filters we can apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2f86d-f074-4d33-98a0-c6e5f44cc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT\n",
    "  JSON_VALUE(\"agent\", '$.browser') AS \"browser\",\n",
    "  SUM( JSON_VALUE(\"event\", '$.layer' RETURNING BIGINT) ) AS \"sum_layers\",\n",
    "  COUNT( DISTINCT JSON_VALUE(\"geo_ip\", '$.city') ) AS \"unique_cities\"\n",
    "\n",
    "FROM \"example-kttm-nested-batch\"\n",
    "\n",
    "WHERE JSON_VALUE(\"geo_ip\", '$.continent') = 'South America'\n",
    "\n",
    "GROUP BY 1 \n",
    "ORDER BY 3 DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb243a-276b-4227-a485-f398ba078f09",
   "metadata": {},
   "source": [
    "Since nested columns could have different fields from row to row or as their schema changes over time, you can inspect the fields that have been discovered during ingestion using the JSON_PATHS function on nested columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ef331-b119-4182-aed1-e8fa2e777650",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT 'agent' as nested_column, STRING_AGG( DISTINCT JSON_PATHS(\"agent\"), ', ') paths FROM \"example-kttm-nested-batch\"\n",
    "UNION ALL\n",
    "SELECT 'event', STRING_AGG( DISTINCT JSON_PATHS(\"event\"), ', ') paths FROM \"example-kttm-nested-batch\"\n",
    "UNION ALL\n",
    "SELECT 'geo_ip', STRING_AGG( DISTINCT JSON_PATHS(\"geo_ip\"), ', ') paths FROM \"example-kttm-nested-batch\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e330808-696b-4278-be8e-269924cc8089",
   "metadata": {},
   "source": [
    "<a id='system_fields'></a>\n",
    "## System Fields for Batch Ingestion\n",
    "\n",
    "When doing ingestion of multiple files, it is generally helpful to know the specific source of the data. This feature allows you to do just that. It provides system fields that identifty the input source and which can be added to the ingestion job.\n",
    "\n",
    "Each Input Source has slightly different input fields. In the example below we use HTTP [checkout in the docs to see the fields that are available](https://druid.apache.org/docs/latest/ingestion/input-sources#http-input-source). \n",
    "\n",
    "To enable this functionality, add the new property \"systemFields\" the Input Source field in the EXTERN clause:\n",
    "```\n",
    "FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\n",
    "         \"type\":\"http\",\n",
    "         \"systemFields\":[\"__file_uri\",\"__file_path\"],   <<<<<< list of system fields to capture\n",
    "         \"uris\":[<list of file URIs]}',\n",
    "        }'\n",
    "...\n",
    "    )\n",
    "```\n",
    "\n",
    "and in the EXTEND clause add the fields so they are accesible to SELECT :\n",
    "```\n",
    "EXTEND (\"__file_uri\" VARCHAR,\"__file_path\" VARCHAR, ...)\n",
    "\n",
    "```\n",
    "\n",
    "This example ingests three files (20 million rows each) into a roll-up table to demonstrate the use of system fields. It takes a while, so be patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265d6a4-4d90-4b5f-9ae5-c417ffddb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-taxi-trips-rollup\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"systemFields\":[\"__file_uri\",\"__file_path\"],\"uris\":[\"https://static.imply.io/example-data/trips/trips_xaa.csv.gz\",\"https://static.imply.io/example-data/trips/trips_xab.csv.gz\",\"https://static.imply.io/example-data/trips/trips_xac.csv.gz\"]}',\n",
    "      '{\"type\":\"csv\",\"findColumnsFromHeader\":false,\"columns\":[\"trip_id\",\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\"store_and_fwd_flag\",\"rate_code_id\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"passenger_count\",\"trip_distance\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"ehail_fee\",\"improvement_surcharge\",\"total_amount\",\"payment_type\",\"trip_type\",\"pickup\",\"dropoff\",\"cab_type\",\"precipitation\",\"snow_depth\",\"snowfall\",\"max_temperature\",\"min_temperature\",\"average_wind_speed\",\"pickup_nyct2010_gid\",\"pickup_ctlabel\",\"pickup_borocode\",\"pickup_boroname\",\"pickup_ct2010\",\"pickup_boroct2010\",\"pickup_cdeligibil\",\"pickup_ntacode\",\"pickup_ntaname\",\"pickup_puma\",\"dropoff_nyct2010_gid\",\"dropoff_ctlabel\",\"dropoff_borocode\",\"dropoff_boroname\",\"dropoff_ct2010\",\"dropoff_boroct2010\",\"dropoff_cdeligibil\",\"dropoff_ntacode\",\"dropoff_ntaname\",\"dropoff_puma\"]}'\n",
    "    )\n",
    "  ) EXTEND (\"__file_uri\" VARCHAR,\"__file_path\" VARCHAR, \"trip_id\" BIGINT, \"vendor_id\" BIGINT, \"pickup_datetime\" VARCHAR, \"dropoff_datetime\" VARCHAR, \"store_and_fwd_flag\" VARCHAR, \"rate_code_id\" BIGINT, \"pickup_longitude\" DOUBLE, \"pickup_latitude\" DOUBLE, \"dropoff_longitude\" DOUBLE, \"dropoff_latitude\" DOUBLE, \"passenger_count\" BIGINT, \"trip_distance\" DOUBLE, \"fare_amount\" DOUBLE, \"extra\" DOUBLE, \"mta_tax\" DOUBLE, \"tip_amount\" DOUBLE, \"tolls_amount\" DOUBLE, \"ehail_fee\" VARCHAR, \"improvement_surcharge\" VARCHAR, \"total_amount\" DOUBLE, \"payment_type\" BIGINT, \"trip_type\" VARCHAR, \"pickup\" VARCHAR, \"dropoff\" VARCHAR, \"cab_type\" VARCHAR, \"precipitation\" DOUBLE, \"snow_depth\" BIGINT, \"snowfall\" BIGINT, \"max_temperature\" BIGINT, \"min_temperature\" BIGINT, \"average_wind_speed\" DOUBLE, \"pickup_nyct2010_gid\" BIGINT, \"pickup_ctlabel\" BIGINT, \"pickup_borocode\" BIGINT, \"pickup_boroname\" VARCHAR, \"pickup_ct2010\" BIGINT, \"pickup_boroct2010\" BIGINT, \"pickup_cdeligibil\" VARCHAR, \"pickup_ntacode\" VARCHAR, \"pickup_ntaname\" VARCHAR, \"pickup_puma\" BIGINT, \"dropoff_nyct2010_gid\" BIGINT, \"dropoff_ctlabel\" BIGINT, \"dropoff_borocode\" BIGINT, \"dropoff_boroname\" VARCHAR, \"dropoff_ct2010\" BIGINT, \"dropoff_boroct2010\" BIGINT, \"dropoff_cdeligibil\" VARCHAR, \"dropoff_ntacode\" VARCHAR, \"dropoff_ntaname\" VARCHAR, \"dropoff_puma\" BIGINT)\n",
    ")\n",
    "SELECT\n",
    "  TIME_FLOOR( TIME_PARSE(TRIM(\"pickup_datetime\")), 'P1M') AS \"__time\",\n",
    "  \"__file_uri\",\n",
    "  \"__file_path\",\n",
    "  count(*) as \"row_count\"\n",
    "FROM \"ext\"\n",
    "GROUP BY 1,2,3\n",
    "PARTITIONED BY ALL\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-taxi-trips-rollup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271791d0-0a27-47d3-b9ed-6f445c302412",
   "metadata": {},
   "source": [
    "Query the system fields that were ingested to see information about how each file was ingested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1488573-b82e-4b04-ae69-91d9c169a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT \"__file_uri\", \"__file_path\", \n",
    "    SUM(\"row_count\") \"total_file_rows\" \n",
    "FROM \"example-taxi-trips-rollup\"\n",
    "GROUP BY 1,2\n",
    "'''\n",
    "\n",
    "druid.display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06e516-55ea-456f-8665-75884f6d3f27",
   "metadata": {},
   "source": [
    "While the above examples are rather simple, this is a powerful tool to enhance data when the files are organized in folder structures where the path contains infomation about the data. It is common to see this kind of file system organization in cloud storage where that data has already been partitioned by time or other dimensions. Take this list of files as an example:\n",
    "```\n",
    "/data/activity_log/customer=501/hour=2024-01-01T10:00:00/datafile1.csv\n",
    "/data/activity_log/customer=501/hour=2024-01-01T10:00:00/datafile2.csv\n",
    "/data/activity_log/customer=376/hour=2024-01-01T11:00:00/datafile1.csv\n",
    "...\n",
    "```\n",
    "With this example, the __file_uri or __file_path columns can be parsed at ingestion to create other fields using functions like REGEXP_EXTRACT to extract `customer` and `hour` in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a76a4b-d68a-43b3-87eb-239d9d72c042",
   "metadata": {},
   "source": [
    "## Enhancing data at ingestion\n",
    "\n",
    "Adding dimensions and metrics to your data can enhance its analytic value. It's common, for example, to add product categorization, user demographics or additional location based metrics to Retail clickstream or POS data. In IoT scenarios, additional info like metric type (temperature, pressure, flow, etc) for a particular device is common, the device can be associated to a specific industrial process and grouped into components and subcomponents of the overall system being monitored are very useful in determining subsystem anomalies. \n",
    "\n",
    "Lookups and joins can be used at query time to enhance data in this fashion. But there is a performance penalty when using lookups and even more penalty with joins at query time. So in the interest of achieving fast analytic queries, joins can be applied at ingestion time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea89ec-e119-4478-b56e-13f056886dee",
   "metadata": {},
   "source": [
    "#### Broadcast joins - small lookups joins\n",
    "\n",
    "SQL Based Ingestion can process joins efficiently during ingestion using either broadcast or sort merge joins. Broadcast is the default method, in which the right table of the join is broadcast in its entirety to all workers involved in the ingestion. The content of the lookup is kept within each worker's memory in order to process the join. You'll need to take care that the whole set of lookup tables joined in this fashion for a given ingestion will fit within the heap of each worker JVM.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd142c44-9f9e-42c8-be62-740942e2c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-kttm-enhanced-batch\" OVERWRITE ALL\n",
    "WITH\n",
    "kttm_data AS (\n",
    "  SELECT * \n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-v2/kttm-v2-2019-08-25.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"timestamp\" VARCHAR, \"agent_category\" VARCHAR, \"agent_type\" VARCHAR, \"browser\" VARCHAR, \"browser_version\" VARCHAR, \"city\" VARCHAR, \"continent\" VARCHAR, \"country\" VARCHAR, \"version\" VARCHAR, \"event_type\" VARCHAR, \"event_subtype\" VARCHAR, \"loaded_image\" VARCHAR, \"adblock_list\" VARCHAR, \"forwarded_for\" VARCHAR, \"language\" VARCHAR, \"number\" VARCHAR, \"os\" VARCHAR, \"path\" VARCHAR, \"platform\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"region\" VARCHAR, \"remote_address\" VARCHAR, \"screen\" VARCHAR, \"session\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR, \"window\" VARCHAR)\n",
    "),\n",
    "country_lookup AS (\n",
    "  SELECT * \n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/lookup/countries.tsv\"]}',\n",
    "      '{\"type\":\"tsv\",\"findColumnsFromHeader\":true}'\n",
    "    )\n",
    "  ) EXTEND (\"Country\" VARCHAR, \"Capital\" VARCHAR, \"ISO3\" VARCHAR, \"ISO2\" VARCHAR)\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  TIME_PARSE(kttm_data.\"timestamp\") AS __time,\n",
    "  kttm_data.\"session\",\n",
    "  kttm_data.\"agent_category\",\n",
    "  kttm_data.\"agent_type\",\n",
    "  kttm_data.\"browser\",\n",
    "  kttm_data.\"browser_version\",\n",
    "  kttm_data.\"language\",\n",
    "  kttm_data.\"os\",\n",
    "  kttm_data.\"city\",\n",
    "  kttm_data.\"country\",\n",
    "  country_lookup.\"Capital\" AS \"capital\",\n",
    "  country_lookup.\"ISO3\" AS \"iso3\",\n",
    "  kttm_data.\"forwarded_for\" AS \"ip_address\",\n",
    "  kttm_data.\"session_length\",\n",
    "  kttm_data.\"event_type\"\n",
    "FROM kttm_data\n",
    "LEFT JOIN country_lookup ON country_lookup.Country = kttm_data.country\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready('example-kttm-enhanced-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6e5b2-f3ce-41a5-85cb-242a15edac7e",
   "metadata": {},
   "source": [
    "Data for both sources \"kttm_data\" and \"country_lookup\" are obtained from external sources:\n",
    "```\n",
    "WITH\n",
    "kttm_data AS\n",
    "(\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "               '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-v2/kttm-v2-2019-08-25.json.gz\"]}',\n",
    "               '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"timestamp\" VARCHAR, \"agent_category\" VARCHAR, ...)\n",
    "),\n",
    "country_lookup AS\n",
    "(\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/lookup/countries.tsv\"]}',\n",
    "      '{\"type\":\"tsv\",\"findColumnsFromHeader\":true}'\n",
    "    )\n",
    "  ) EXTEND (\"Country\" VARCHAR, \"Capital\" VARCHAR, \"ISO3\" VARCHAR, \"ISO2\" VARCHAR)\n",
    ")\n",
    "```\n",
    "\n",
    "Columns from both tables can be used in the SELECT expressions using the alias \"country_lookup\" to reference any joined column:\n",
    "```\n",
    "  kttm_data.\"country\",\n",
    "  country_lookup.\"Capital\" AS \"capital\",\n",
    "  country_lookup.\"ISO3\" AS \"iso3\"\n",
    "```\n",
    "\n",
    "The join is specified in the FROM clause:\n",
    "```\n",
    "FROM kttm_data\n",
    "LEFT JOIN country_lookup ON country_lookup.Country = kttm_data.country\n",
    "```\n",
    "LEFT JOIN insured that all the rows from kttm_data source are ingested. An INNER JOIN would exclude rows from \"kttm_data\" if the value for \"kttm_data.country\" is not present in \"country_lookup.Country\". \n",
    "Since no context parameters were set, the join is processed as a broadcast join. The first table in the FROM clause is the distributed table and all other joined tables will be shipped to the workers to execute the join.\n",
    "\n",
    "Take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb13e7-9071-46b6-8088-810c72257073",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT\n",
    "  \"iso3\" AS \"country_code\", \n",
    "  \"capital\",\n",
    "  count( DISTINCT \"ip_address\" ) distinct_users, \n",
    "  MIN(\"session_length\")/1000 fastest_session_ms,\n",
    "  MAX(\"session_length\")/1000 slowest_session_ms\n",
    "FROM \"example-kttm-enhanced-batch\"\n",
    "WHERE \"event_type\"='LayerClear'\n",
    "GROUP BY 1,2\n",
    "ORDER BY 3 DESC\n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa697e8-bec5-4d77-a424-5ee37a86b859",
   "metadata": {},
   "source": [
    "#### Shuffle joins - \"Large lookup to fact\" or \"fact to fact\" joins\n",
    "\n",
    "See [Shuffle joins in SQL Based Ingestion](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sort-merge).\n",
    "This is the ability to join large tables to other large tables without fully loading either one into memory. Both sources involved in the join are scanned in parallel across all workers, the intermediate data for both sources is then redistributed among the workers based on the join column(s) such that rows from both sources with the same values end up in the same worker.\n",
    "\n",
    "![](assets/shuffle-join.png)\n",
    "\n",
    "In order to use shuffle join the query context must include:\n",
    "```\n",
    "{\n",
    "   \"sqlJoinAlgorithm\":\"sortMerge\"\n",
    "}\n",
    "```\n",
    "\n",
    "Given that this example is meant to run on the local docker compose deployment, two very large tables is not possible, try it out with small sources and just pretend they are big. We'll use the \"wikipedia\" sample data and join it with \"example-wiki-users-batch\" profile data. But first, create the users table because at the time of this writing there wasn't a matching \"user\" source handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995aeb98-b5c3-44e3-a4e5-2e13eb3a248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we don't have a source for example-wiki-users-batch let's\n",
    "# create one using in-database transformation that\n",
    "# generates user profiles from the wikipedia data by\n",
    "# - grouping on \"user\"\n",
    "# - injecting variability into the \"group\" column using modulus of the __time of the event\n",
    "# - \"edits\" represent the number of edits done by the user, so just count the number of events\n",
    "# - calculate the registration time using the minimum __time from events and adjusting it some variable years back in time\n",
    "# - determine the preferred language of the user based on their earliest channel edit\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"example-wiki-users-batch\" OVERWRITE ALL\n",
    "SELECT \n",
    "  \"user\", \n",
    "  EARLIEST(\n",
    "    CASE \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),5) > 3 THEN 'Reviewers' \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),17) > 13 THEN 'Patrollers' \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),23) > 21 THEN 'Bots'\n",
    "      ELSE 'Autoconfirmed'\n",
    "    END,\n",
    "    1024\n",
    "  ) AS \"group\",\n",
    "  count(*) \"edits\",\n",
    "  TIME_SHIFT(MIN(__time), 'P1Y', -1 * MOD(MIN(EXTRACT (MICROSECOND FROM __time)),20) ) AS \"registered_at_ms\",\n",
    "  EARLIEST(SUBSTRING(\"channel\", 2, 2), 1024) AS \"language\"\n",
    "FROM \"example-wikipedia-batch\"\n",
    "GROUP BY 1\n",
    "PARTITIONED BY ALL\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)              # init request object\n",
    "request.add_context( 'finalizeAggregations', True) # EARLIEST functions will store a partial aggregation otherwise\n",
    "request.add_context( 'maxNumTasks', 2)             # can't go any higher in test env\n",
    "\n",
    "druid.display.run_task(request)\n",
    "druid.sql.wait_until_ready('example-wiki-users-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ff8bb-2a1d-4445-be59-9d2460397875",
   "metadata": {},
   "source": [
    "The next cell runs an ingestion using the sortMerge join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcc958-357c-4b64-b19f-bcbf0fc6941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"example-wiki-merge-batch\" OVERWRITE ALL\n",
    "WITH \"wikidata\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT \n",
    "  TIME_PARSE(d.\"timestamp\") as \"__time\",\n",
    "  d.\"isRobot\", \n",
    "  d.\"channel\" , \n",
    "  d.\"timestamp\" , \n",
    "  d.\"flags\" , \n",
    "  d.\"isUnpatrolled\" , \n",
    "  d.\"page\" , \n",
    "  d.\"diffUrl\" , \n",
    "  d.\"added\" , \n",
    "  d.\"comment\" , \n",
    "  d.\"commentLength\" , \n",
    "  d.\"isNew\" , \n",
    "  d.\"isMinor\" , \n",
    "  d.\"delta\" , \n",
    "  d.\"isAnonymous\" , \n",
    "  d.\"user\" , \n",
    "  d.\"deltaBucket\" , \n",
    "  d.\"deleted\" , \n",
    "  d.\"namespace\" , \n",
    "  d.\"cityName\" , \n",
    "  d.\"countryName\" , \n",
    "  d.\"regionIsoCode\" , \n",
    "  d.\"metroCode\" , \n",
    "  d.\"countryIsoCode\" , \n",
    "  d.\"regionName\", \n",
    "  u.\"group\" AS \"user_group\",\n",
    "  u.\"edits\" AS \"user_edits\",\n",
    "  u.\"registered_at_ms\" AS \"user_registration_epoch\",\n",
    "  u.\"language\" AS \"user_language\"\n",
    "FROM \"wikidata\" AS d \n",
    "  LEFT JOIN \"example-wiki-users-batch\" AS u ON u.\"user\"=d.\"user\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)                 # init request object\n",
    "request.add_context( 'sqlJoinAlgorithm', 'sortMerge') # use sortMerge to join the sources\n",
    "request.add_context( 'maxNumTasks', 2)                # use 2 tasks to run the ingestion\n",
    "\n",
    "druid.display.run_task(request)\n",
    "druid.sql.wait_until_ready('example-wiki-merge-batch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307374a4-635a-47bd-baf6-054373b3aa0c",
   "metadata": {},
   "source": [
    "Run the next cell to query the newly joined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbc94d-c835-4dd5-a8e6-c499d35e2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT \"user_group\",\n",
    "  count( DISTINCT \"user\") \"distinct_users\",\n",
    "  sum(\"user_edits\") \"total_activity\"\n",
    "FROM \"example-wiki-merge-batch\"\n",
    "GROUP BY 1\n",
    "ORDER BY 1, 3 DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b136d9-060e-4f5b-b0ca-bf58f626ba5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Druid's [SQL Based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/index.html) enables scalable batch ingestion from a large variety of [data sources](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) and [formats](https://druid.apache.org/docs/latest/ingestion/data-formats.html). The familiarity and expressiveness of SQL enables users to quickly transform, filter and generally enhance data directly in the cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e916694-85aa-45a8-bd7a-ef14cdfa8d52",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Run the following cell to remove all data sources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5e2b3-0b6f-4d6e-afbd-3ea435dde241",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop('example-wikipedia-batch', True)\n",
    "druid.datasources.drop('example-wikipedia-3-batch', True)\n",
    "druid.datasources.drop('example-wikipedia-4-batch', True)\n",
    "druid.datasources.drop('example-wikipedia-only-human', True)\n",
    "druid.datasources.drop('example-kttm-transform-batch', True)\n",
    "druid.datasources.drop('example-kttm-nested-batch', True)\n",
    "druid.datasources.drop('example-kttm-enhanced-batch', True)\n",
    "druid.datasources.drop('example-wiki-users-batch', True)\n",
    "druid.datasources.drop('example-wiki-merge-batch', True)\n",
    "druid.datasources.drop('example-taxi-trips-rollup', True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
