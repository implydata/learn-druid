{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73dbb71f-0253-4940-a7b6-0b808839a82d",
   "metadata": {},
   "source": [
    "# Ingest data to a Druid table using SQL-based batch\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "[SQL-based](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sql-reference) ingestion reads raw data from files or other external batch sources and transforms them into time partitioned and fully indexed [Druid segment files](https://druid.apache.org/docs/latest/design/segments).\n",
    "\n",
    "In this notebook on the basics of batch ingestion in Druid, you will:\n",
    "\n",
    "- Ingest data from several files.\n",
    "- Apply some context parameters to control how the ingestion executes.\n",
    "- Incorporate some filters and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Learn Druid Project page](https://github.com/implydata/learn-druid/#readme).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "if (os.environ['DRUID_HOST'] == None):\n",
    "    druid_host=f\"http://router:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4a2e1-2fbb-420b-a052-5e80885e79c1",
   "metadata": {},
   "source": [
    "## Run a simple batch ingestion from one file\n",
    "\n",
    "SQL-based ingestion adds to or replaces data in a TABLE. Take a look at the example SQL in the cell below.\n",
    "\n",
    "* REPLACE with OVERWRITE ALL means that the whole table will be overwritten.\n",
    "* WITH sets up and defines the source, including an EXTERN [input source](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) along with [format](https://druid.apache.org/docs/latest/ingestion/data-formats.html) parser - in this case, a web source in JSON format.\n",
    "* [EXTEND](https://druid.apache.org/docs/latest/multi-stage-query/reference#extern-function) describes the input schema.\n",
    "* SELECT defines the transformations and schema of the resulting Druid table. The `__time` field is required.\n",
    "* PARTITIONED BY tells Driud how to lay out the table data internally.\n",
    "\n",
    "Run the following cell to load data from an external file into the \"example-wikipedia-batch\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47fcff-9055-4058-852f-6f5c61d07965",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"example-wikipedia-batch\"\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f811a-7795-4351-bc6b-3abb34da0116",
   "metadata": {},
   "source": [
    "Run the following SQL to see some of the data in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2584e65-c952-47f1-a885-2e5d3a4fef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "SELECT channel,  count(*) num_events\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL (\"__time\", '2016-06-27/2016-06-28')\n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "druid.display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9d0dc-2c62-4aac-a0c1-958ee4b68769",
   "metadata": {},
   "source": [
    "Run the following cell to drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963c87b-76e6-42af-a8ca-54d7a66f09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop(table_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43d5f3-0350-40e6-b5a6-737c610d7562",
   "metadata": {},
   "source": [
    "## Run a parallelized batch ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340d912-2441-48ef-b6ce-e182f57734d4",
   "metadata": {},
   "source": [
    "[Druid Input Sources](https://druid.apache.org/docs/latest/ingestion/native-batch.html#splittable-input-sources) allow you to specify multiple files as input to an ingestion job.\n",
    "\n",
    "In the following SQL, the EXTERN statement has been updated. Now there are several `uris`, each containing a different file to ingest.\n",
    "\n",
    "Run the cell to ingest data from all three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91d16b-1880-4a4c-8e8a-650075e26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"example-wikipedia-bigbatch\"\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce84c5-2c1c-4c6a-b9f4-7a22e2216ec5",
   "metadata": {},
   "source": [
    "Let's look at the data now. The quantities are 3 times larger than before because we loaded the same file three times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278c0d2-9fd5-4a7b-bb65-a6db7cf98971",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "SELECT channel, count(*) num_events\n",
    "FROM \"{table_name}\" \n",
    "WHERE TIME_IN_INTERVAL(\"__time\", '2016-06-27/2016-06-28')\n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728e627-a849-4753-9bde-382f557c2584",
   "metadata": {},
   "source": [
    "Run the following cell to drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b287c-be0e-48fa-ae39-e111ae9dd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop(table_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751316a2-bd0c-43eb-a74f-fed15782f54d",
   "metadata": {},
   "source": [
    "Use [context parameters](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters) to control how the ingestion executes.\n",
    "\n",
    "The learning environment has been [configured](https://druid.apache.org/docs/latest/operations/basic-cluster-tuning#task-count) with the capacity to run four tasks concurrently.\n",
    "\n",
    "Run the following cell to:\n",
    "\n",
    "* Create the SQL to pass to Druid as `sql`.\n",
    "* Create a request object, `request`.\n",
    "* Add a context parameter for `maxNumTasks`, set to the maximum of 4, to allow all files to be read in parallel.\n",
    "* Run the task.\n",
    "* Wait until the data has been distributed around the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55313e69-e63e-47d9-917c-2da87926ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"example-wikipedia-4-batch\"\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "request = druid.sql.sql_request(sql)\n",
    "request.add_context('maxNumTasks', 4)\n",
    "\n",
    "druid.display.run_task(request)\n",
    "druid.sql.wait_until_ready(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87e942-000a-49d3-b915-4319519c4ed1",
   "metadata": {},
   "source": [
    "Get insight into the segments that were generated in the console \"segments\" view or by using Druid's `SYS.SEGMENTS` table.\n",
    "\n",
    "Run the next cell to get information about the table that you just created.\n",
    "\n",
    "Notice that, even though there were multiple tasks ingesting the data, a shuffle stage brought all the data together into one segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b3ade-63e8-4d5f-909b-4e6e83f0115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"size\",\n",
    "  \"num_rows\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313cd12-170a-4a05-b2cd-e57ecca87e8a",
   "metadata": {},
   "source": [
    "Run the next cell to drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af97ce0-6105-474f-b487-7b9756bad676",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop(table_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f95a00-cc40-478f-aad9-cb8243f6c363",
   "metadata": {},
   "source": [
    "## Filter and transform data during ingestion\n",
    "\n",
    "Use SQL functions and filters to carry out calculations on, and filter unwanted data from, the source data.\n",
    "\n",
    "In this section you will see some examples of filtering and expressions being applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72a2e0-45fb-4ab8-9f9c-9b54f3f3225f",
   "metadata": {},
   "source": [
    "### Use WHERE to filter incoming data\n",
    "\n",
    "In situations where you need data cleansing or your only interested in a subset of the data, the ingestion job can filter the data by simply adding a WHERE clause.\n",
    "\n",
    "Run the next cell to only ingest rows where the event wasn't generated by a robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1fcaa-1e78-475b-b2ef-131aa88ead51",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"example-wikipedia-only-human\"\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "WHERE \"isRobot\"='false'\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e86f2-d84f-4fce-9f82-2286721dd119",
   "metadata": {},
   "source": [
    "Run the following cell to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c825d39-0bbd-4cd3-ae8a-760146d9fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT isRobot, channel, count(*) num_events\n",
    "FROM \"{table_name}\"\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35826f3c-4593-4ff3-ab14-8b3a22fc39b4",
   "metadata": {},
   "source": [
    "Run the next cell to drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16c069-2258-49a1-93b4-df0b03b255d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop(table_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561d663-9cfe-4a62-80cc-dcbb0ce199ff",
   "metadata": {},
   "source": [
    "### Use functions to transform data at ingestion time\n",
    "\n",
    "The SQL language provides a rich [set of functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html) that can be applied to input columns to transform the data as it is being ingested. All scalar SQL functions are available for normal ingestion. Rollup ingestion is discussed in the [Rollup Notebook](05-rollup.ipynb) which includes the use of aggregate functions at ingestion time as well.\n",
    "\n",
    "Some common functions include:\n",
    "* [Time parsing and manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#date-and-time-functions)\n",
    "* CASE statements to resolve complex logic and prepare columns for certain query patterns.\n",
    "* [String manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#string-functions).\n",
    "* Nested object (JSON) functions.\n",
    "\n",
    "Take a look at the next cell to see some of these functions being incorporated into a batch ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbddfb3-a482-477a-94a7-18a5539590ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"example-kttm-transform-batch\"\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND (\"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \"event\" TYPE('COMPLEX<json>'), \"agent\" TYPE('COMPLEX<json>'), \"client_ip\" VARCHAR, \"geo_ip\" TYPE('COMPLEX<json>'), \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \"screen\" VARCHAR, \"window\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  session, \n",
    "  number,\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  TIMESTAMPDIFF(DAY, TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W'), TIME_PARSE(\"timestamp\")) AS days_since_week_start,\n",
    "  TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W') AS week_start,\n",
    "  TIME_CEIL(TIME_PARSE(\"timestamp\"), 'P1W') AS week_end,\n",
    "  TIME_SHIFT(TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1D'),'P1D', -1) AS start_of_yesterday,\n",
    "  \n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "  \n",
    "  CASE WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0 ELSE 1 END AS adblock_count,\n",
    "  CASE WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1 ELSE 0 END AS easylist_count,\n",
    "  \n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),2) AS minor_version,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version,\n",
    "  session_length\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.display.run_task(sql)\n",
    "druid.sql.wait_until_ready(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f2278-be01-438f-8e2b-7c6c56d0ee28",
   "metadata": {},
   "source": [
    "Run the next cell to see see what time of day shows the highest user activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e787fe-65f5-4110-bc69-ce583285ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "SELECT EXTRACT( HOUR FROM \"__time\") time_hour, city, count(distinct \"session\") session_count\n",
    "FROM \"{table_name}\"\n",
    "WHERE \"city\" IS NOT NULL AND \"city\" <> ''\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da4c4c-015e-4100-b7ed-b88926c613a3",
   "metadata": {},
   "source": [
    "Drop the table by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ee22a-407f-4826-9499-afafe2d00ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop(table_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b136d9-060e-4f5b-b0ca-bf58f626ba5c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Druid's [SQL Based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/index.html) enables scalable batch ingestion from a large variety of [data sources](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) and [formats](https://druid.apache.org/docs/latest/ingestion/data-formats.html). The familiarity and expressiveness of SQL enables users to quickly transform, filter and generally enhance data directly in the cluster.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try out some more functions, like [date time](../03-query/07-functions-datetime.ipynb), [strings](../03-query/08-functions-strings.ipynb), and [CASE](../03-query/09-functions-case.ipynb).\n",
    "* Incorporate a [GROUP BY](../03-query/01-groupby.ipynb) to aggregate rows at ingestion time.\n",
    "* Work through the notebook on [PARTITIONED BY and CLUSTERED BY](./06-partitioning-data.ipynb).\n",
    "* See how Druid works with [nested data](./05-working-with-nested-columns.ipynb).\n",
    "* Deep dive into the [data types](./04-table-datatypes.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
