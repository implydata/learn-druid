{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Consume from multiple Apache Kafka topics into the same table using multi-topic ingestion\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "This notebook demonstrates how to consume events from [multiple Apache Kafka topics]((https://druid.apache.org/docs/latest/development/extensions-core/kafka-supervisor-reference#ingesting-from-multiple-topics)). You will:\n",
    "\n",
    "- Create a topic and initiate a data feed on it\n",
    "- Create a multi-topic ingestion job that users `topicPattern` to find new topics dynamically\n",
    "- Create an additional topic that fits the same topic name pattern \n",
    "- Query multiple topic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 28.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "<!-- Profiles are:\n",
    "`druid-jupyter` - just Jupyter and Druid\n",
    "`all-services` - includes Jupyter, Druid, and Kafka\n",
    " -->\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "\n",
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "rest_client = druid.rest\n",
    "\n",
    "# client for Data Generator API\n",
    "datagen = druidapi.rest.DruidRestClient(\"http://datagen:9999\")\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464c204-bbf1-4a94-ae02-2356b606049f",
   "metadata": {},
   "source": [
    "## Create a multi-topic ingestion supervisor\n",
    "\n",
    "In this section, you will populate a Kafka topic and then create a supervisor that uses multi-topic Kafka ingestion to begin reading the data.\n",
    "\n",
    "### Generate example data and populate a Kafka topic\n",
    "\n",
    "Run the cell below to use the data generator to produce sample data and post it into a Kafka topic, `social-twitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63edf2-ffd0-48c1-bfb7-8651019d1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": \"gen_social_twitter\",\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": \"social-twitter\" },\n",
    "    \"config_file\": \"social/social_posts.json\", \n",
    "    \"time\":\"1h\",\n",
    "    \"concurrency\":100,\n",
    "    \"time_type\":\"REAL\"\n",
    "}\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd176f7-c443-41ec-af7a-1e50d69fae01",
   "metadata": {},
   "source": [
    "### Start a multi-topic supervisor\n",
    "\n",
    "Run the following cell to create an object to contain the ingestion specification.\n",
    "\n",
    "* [`ioConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig) and [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig) set connection and execution parameters.\n",
    "* The [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema) section is set to use automatic schema discovery, and to use hourly partitions.\n",
    "\n",
    "Notice that the `ioConfig` uses a `topicPattern`, followed by a regular expression. The supervisor will therefore create consumers to read from any topics that match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c5192-367e-4305-acfc-e6b946e5aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_ingestion_spec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": {\n",
    "      \"type\": \"kafka\",\n",
    "      \"consumerProperties\": { \"bootstrap.servers\": \"kafka:9092\" },\n",
    "      \"topicPattern\": \"social-.*\", \n",
    "      \"inputFormat\": { \"type\": \"kafka\",\"valueFormat\": { \"type\": \"json\" } } },\n",
    "    \"tuningConfig\": { \"type\": \"kafka\" },\n",
    "    \"dataSchema\": {\n",
    "      \"dataSource\": \"example-social-multitopic\",\n",
    "        \"dimensionsSpec\": { \"useSchemaDiscovery\": \"true\" },\n",
    "        \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\"},\n",
    "        \"granularitySpec\": { \"rollup\": \"false\", \"segmentGranularity\": \"hour\"} } } }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d2db8-427d-4bba-9184-4c029fcfe764",
   "metadata": {},
   "source": [
    "Run the next cell to send the spec to Druid to start the streaming ingestion supervisor, which will then spawn ingestion tasks to consume from the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee991b-a7de-484e-b8f1-52f24bc69c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { 'Content-Type': 'application/json' }\n",
    "supervisor = rest_client.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)\n",
    "druid.sql.wait_until_ready('example-social-multitopic', verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91b8bb-052c-421d-968f-e4cfbff9a51e",
   "metadata": {},
   "source": [
    "You can see the supervisor job in the [Druid Console Supervisor View](http://localhost:8888/unified-console.html#supervisors). Click the magnifying glass icon to view the status of the job. The `startingOffsets` property lists kafka partitions that the job is consuming from, each partition is identified by `\"<topic-name>:<partition #>\"` which will list all partitions from all topics that have been discovered :\n",
    "```\n",
    "{\n",
    "  \"dataSource\": \"social_media\",\n",
    "  \"stream\": \"social-.*\",\n",
    "  \"partitions\": 1,\n",
    "  \"replicas\": 1,\n",
    "  \"durationSeconds\": 3600,\n",
    "  \"activeTasks\": [\n",
    "    {\n",
    "      \"id\": \"index_kafka_social_media_77e5722b1640edd_cnnmjgpc\",\n",
    "      \"startingOffsets\": {\n",
    "        \"social-twitter:0\": 0   <<<<<<<<<<<<<<<<<<< SEE DISCOVERED TOPICS HERE\n",
    "      },\n",
    "      \"startTime\": \"2023-11-03T20:52:41.173Z\",\n",
    "      ...\n",
    "}\n",
    "```\n",
    "\n",
    "The following query shows the last few minutes of activity from topics being captured so far. The data only includes `kafka.topic` = `social-twitter`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda117fb-14ed-4198-a5a7-6b1f2a461b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    SELECT TIME_FLOOR(\"__time\", 'PT1M') as \"minute\", \n",
    "       \"kafka.topic\",   SUM(views) as \"total_views\" \n",
    "    FROM \"example-social-multitopic\" \n",
    "    WHERE __time >= CURRENT_TIMESTAMP - INTERVAL '5' MINUTE\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 1 DESC, 3 DESC\n",
    "    LIMIT 5\n",
    "'''\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1c903-dfd4-4b1d-855b-8e7ce0fafae7",
   "metadata": {},
   "source": [
    "## Create a second Kafka topic\n",
    "\n",
    "The following cell initiates a second topic called `social-linkedin` and begins streaming data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f04f6c-3112-4029-a7fa-da4364f8b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_request = {\n",
    "    \"name\": \"gen_social_linkedin\",\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": \"social-linkedin\" },\n",
    "    \"config_file\": \"social/social_posts.json\", \n",
    "    \"time\":\"1h\",\n",
    "    \"concurrency\":500,\n",
    "    \"time_type\":\"REAL\"\n",
    "}\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers)\n",
    "time.sleep(1) # avoid race between start of the job and its status being available\n",
    "response = datagen.get('/status/gen_social_linkedin')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbacd15f-ad66-452c-b3a6-d8f96b68ba90",
   "metadata": {},
   "source": [
    "Running the next cell will show that there are two jobs running and posting data to two different topics - `social-twitter` and `social-linkedin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d214977-9ba3-48e7-b93d-16054b3a2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.get_json('/jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8c0de-1c13-4cca-b81a-bf69a3b2429a",
   "metadata": {},
   "source": [
    "### Query the multi-topic data\n",
    "\n",
    "Try the following query a few times. It will initially only show `social-twitter` activity, when the supervisor picks up the new topic you will see it appear. It can take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    SELECT TIME_FLOOR(\"__time\", 'PT1M') as \"minute\", \n",
    "       \"kafka.topic\",   SUM(views) as \"total_views\" \n",
    "    FROM \"example-social-multitopic\" \n",
    "    WHERE __time >= CURRENT_TIMESTAMP - INTERVAL '5' MINUTE\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 1 DESC, 3 DESC\n",
    "    LIMIT 5\n",
    "'''\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98846a26-f46d-47ef-ad4c-e09a096367c6",
   "metadata": {},
   "source": [
    "## Cleanup \n",
    "The following cell stops data generation, ingestion jobs and removes the datasource from Druid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dec76f-c5f8-4787-ad20-c25e069b4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stop streaming generator: [{datagen.post('/stop/gen_social_linkedin','',require_ok=False)}]\")\n",
    "print(f\"Stop streaming generator: [{datagen.post('/stop/gen_social_twitter','',require_ok=False)}]\")\n",
    "\n",
    "print(f'Pause streaming ingestion: [{druid.rest.post(\"/druid/indexer/v1/supervisor/example-social-multitopic/suspend\",\"\", require_ok=False)}]')\n",
    "print(f'Shutting down running tasks ...')\n",
    "tasks = druid.tasks.tasks(state='running', table='example-social-multitopic')\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table='example-social-multitopic')       \n",
    "print(f'Reset offsets for re-runnability: [{druid.rest.post(\"/druid/indexer/v1/supervisor/example-social-multitopic/reset\",\"\", require_ok=False)}]')\n",
    "print(f'Terminate streaming ingestion: [{druid.rest.post(\"/druid/indexer/v1/supervisor/example-social-multitopic/terminate\",\"\", require_ok=False)}]')\n",
    "\n",
    "print(f\"Drop datasource: [{druid.datasources.drop('example-social-multitopic')}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96687b-6b35-40ed-baec-ef739bdb80b5",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "This tutorial showed you how to create a Kafka topic using a Python client for Kafka, send a simulated stream of data to Kafka using a data generator, and query and visualize results over time. For more information, see the following resources:\n",
    "\n",
    "* [Apache Kafka ingestion](https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html)\n",
    "* [Querying data](https://druid.apache.org/docs/latest/tutorials/tutorial-query.html)\n",
    "* [Tutorial: Run with Docker](https://druid.apache.org/docs/latest/tutorials/docker.html)"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
