{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Filtering incoming stream data using native functions\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "During streaming ingestion, filters can be applied to incoming data by using native filters inside the `transformSpec`. This tutorial demonstrates how to apply some filters as [filters](https://druid.apache.org/docs/latest/querying/filters) against a stream of events.\n",
    "\n",
    "In this tutorial you perform the following tasks:\n",
    "\n",
    "- Set up a streaming ingestion from Apache Kafka.\n",
    "- Create two alternative tables from the same topic that contain filtered versions of the source data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "    \n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
    "Run the next cell to set up the connection to Apache Kafka and to the Data Generator, and to import some helper functions for later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import kafka\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "datagenUrl = \"http://datagen:9999\"\n",
    "\n",
    "generalHeaders = {'Content-Type': 'application/json'}\n",
    "\n",
    "if (os.environ['KAFKA_HOST'] == None):\n",
    "    kafka_host=f\"kafka:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b7439-ad21-4808-96b1-8e3c992fa51e",
   "metadata": {},
   "source": [
    "### Start a data stream\n",
    "\n",
    "Run the following cell to use the learn-druid Data Generator to create a stream that we can consume from.\n",
    "\n",
    "This will create clickstream sample data for an hour and publish it to a topic in Apache Kafka for Apache Druid to consume from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec019-7145-4005-bb85-ea25eda7bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"example-clickstream-filters\"\n",
    "\n",
    "job_name=\"example_clickstream\"\n",
    "\n",
    "target = {\n",
    "    \"type\":\"kafka\",\n",
    "    \"endpoint\": kafka_host,\n",
    "    \"topic\": topic_name\n",
    "}\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": topic_name,\n",
    "    \"target\": target,\n",
    "    \"config_file\": \"clickstream/clickstream.json\",\n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":10,\n",
    "    \"time_type\": \"REAL\"\n",
    "}\n",
    "\n",
    "requests.post(f\"{datagenUrl}/start\", json.dumps(datagen_request), headers=generalHeaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284bc813-dd75-49aa-bac2-10d1016fff46",
   "metadata": {},
   "source": [
    "Run the next cell to see a sample of the raw data being emitted from the Data Generator.\n",
    "\n",
    "This cell uses a simple consumer to subscribe to the topic and to show the first 5 rows that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128c2a4-588b-4bf6-a0f9-2f002f0ecdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    " bootstrap_servers=kafka_host\n",
    ")\n",
    "\n",
    "consumer.subscribe(topics=topic_name)\n",
    "count = 0\n",
    "\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "\n",
    "consumer.unsubscribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dde5e8-237e-4531-84c2-8647d92ceaea",
   "metadata": {},
   "source": [
    "## Build a native ingestion specification\n",
    "\n",
    "Run the following cell to create an [ioConfig](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig) object that sets the connection to the topic from Apache Druid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb89079-7e2a-404b-be85-d9fc7c97d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
    "      \"type\": \"kafka\",\n",
    "      \"consumerProperties\": {\n",
    "        \"bootstrap.servers\": \"kafka:9092\"\n",
    "      },\n",
    "      \"topic\": topic_name,\n",
    "      \"inputFormat\": {\n",
    "        \"type\": \"json\"\n",
    "      },\n",
    "      \"useEarliestOffset\": \"false\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41302f5-5a99-4038-8b87-c13b9838b403",
   "metadata": {},
   "source": [
    "Run the next cell to create a very simple [tuningConfig](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig) object for the tuning configuration for the ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8cf7d-913e-442f-ade7-7eb28d98f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningConfig = { \"type\": \"kafka\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee01e6-d620-4ca7-9f2b-d49ab839ca5a",
   "metadata": {},
   "source": [
    "Run the next cell to create a series of objects that will ultimately be used to define the table through a [dataSchema](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). These include:\n",
    "\n",
    "* A [timestampSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec) which uses the `time` column from the generated data as the primary timestamp.\n",
    "* A [dimensionsSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimensionsspec). In this example, we manually specify all of the columns from the incoming data.\n",
    "* A [granularitySpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#granularityspec) which puts data into daily segment files without any ingestion-time aggregation ([rollup](https://druid.apache.org/docs/latest/ingestion/rollup)).\n",
    "\n",
    "In the final statement, the `dataSchema` is built from these three objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804095a-1b9f-48dd-9c13-11c1c083e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = topic_name\n",
    "\n",
    "dataSchema_timestampSpec = {\n",
    "    \"column\": \"time\",\n",
    "    \"format\": \"iso\"\n",
    "    }\n",
    "\n",
    "dataSchema_dimensionsSpec = {\n",
    "        \"dimensions\": [\n",
    "          \"user_id\",\n",
    "          \"event_type\",\n",
    "          \"client_ip\",\n",
    "          \"client_device\",\n",
    "          \"client_lang\",\n",
    "          \"client_country\",\n",
    "          \"referrer\",\n",
    "          \"keyword\",\n",
    "          \"product\"\n",
    "        ]\n",
    "      }\n",
    "\n",
    "dataSchema_granularitySpec = {\n",
    "        \"queryGranularity\": \"none\",\n",
    "        \"rollup\": \"false\",\n",
    "        \"segmentGranularity\": \"day\"\n",
    "      }\n",
    "\n",
    "dataSchema = {\n",
    "      \"dataSource\": table_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e24a56-3d9e-4589-8d0c-113a5b5ec8e9",
   "metadata": {},
   "source": [
    "Now run the next cell to create the final native [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c5ebe-c785-4c2f-ba2e-26de0f6ab8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50d1bb-1bc9-4ef1-a1a7-637cc3bb4616",
   "metadata": {},
   "source": [
    "Start the ingestion of the raw data from Apache Kafka by submitting this object to Apache Druid by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5125-1096-462a-a637-cd8f438a2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc897578-48d7-4b8c-9ece-dab4390e2336",
   "metadata": {},
   "source": [
    "Run the following cell to wait until the ingestion has started and the new table is ready for query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830e767-ad9a-47c0-80e2-4f19cd4a20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Filter data using an equality filter\n",
    "\n",
    "In this section you will use a [equality filter](https://druid.apache.org/docs/latest/querying/filters#equality-filter) to create a table that only contains records for where someone searches for a product.\n",
    "\n",
    "Run the following cell to get a preview of the data that we want to have in our new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bed8e-bc49-4ba8-85d2-8a9bfa9232f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE \"event_type\" = 'search'\n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "print(sql)\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c81b63-777b-414f-81a9-2b9ccee214e5",
   "metadata": {},
   "source": [
    "Run the next cell to create a new object that represents the [`transformSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transformspec). This will be added to the dataSchema in the ingestion specification, instructing Druid to apply a filter to the incoming data as it arrives.\n",
    "\n",
    "Here, only one [filter](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#filter) will be applied to the data as it arrives.\n",
    "\n",
    "* The `type` of `selector` looks for an exact match.\n",
    "* The check will be against the `dimension` of `event_type`, looking for a `value` of \"search\".\n",
    "\n",
    "Only rows that pass this test will be added to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562b1f3-4265-4553-aabe-7e7d30b21ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"filter\":\n",
    "    {\n",
    "              \"type\": \"selector\",\n",
    "              \"dimension\": \"event_type\",\n",
    "              \"value\": \"search\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37edb9-d891-4c1a-8223-fcfd7d5e3d16",
   "metadata": {},
   "source": [
    "Now run the following cell to build a new ingestion specification:\n",
    "\n",
    "* A new table will be created, as set in `table_searches` and then used in the `dataSource` name in the `dataSchema`.\n",
    "* The `dataSchema` is updated using the new table name, `timestampSpec` and `granularitySpec` and the updated `dimensionsSpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6503f-d0d3-41ae-a0d5-b55d6e05e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_searches = topic_name + \"-search\"\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_searches,\n",
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776f9b9-527f-4a17-bc6a-4c6b34f7353e",
   "metadata": {},
   "source": [
    "Review the output above and you will see where the `transforms` have been added inside the `dataSchema`.\n",
    "\n",
    "Submit the new specification for ingestions from this Apache Kafka topic by running the cell below. As well as submitting the new ingestion task, it will print \"ready to go\" when the table is ready for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c488ec1-e107-4fa1-8cb5-711150069a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)\n",
    "\n",
    "druid.sql.wait_until_ready(table_searches, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124ea80-4faa-4d16-82a8-d509e3733fa1",
   "metadata": {},
   "source": [
    "Run the query below to see the effect this has had on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c1228-70ed-47d8-bf24-35f6d7323f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "print(time_now)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_searches}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT30S/''' + time_now + '''')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "print(\"This data is filtered at ingestion time:\")\n",
    "display.sql(sql)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT30S/''' + time_now + '''')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "print(\"This data is unfiltered:\")\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09a052-180d-4cb4-80a7-32b746642ee7",
   "metadata": {},
   "source": [
    "## Filter data using an in filter\n",
    "\n",
    "In this section you will use a [in filter](https://druid.apache.org/docs/latest/querying/filters#equality-filter) to create a table that only contains actions where someone adds or drops an item from their cart.\n",
    "\n",
    "Run the following cell which executes SQL to give a preview of the data destined for the new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE \"event_type\" IN ('add_to_cart', 'drop_from_cart')\n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94579c-b84e-43ae-89af-e1e0fb2d7da3",
   "metadata": {},
   "source": [
    "Use an \"explain\" against a Druid SQL statement to view the native representation of the query. From here, you are able to pinpoint the specific filter that has been applied. You can use the Druid console to \"explain\" an SQL statement in the query tab.\n",
    "\n",
    "Run the following cell to use the druid API to explain the SQL query above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367aabe5-9b37-4dd2-a5c1-960ba849b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00032a2e-1d73-4dbd-88ff-697db342ecdc",
   "metadata": {},
   "source": [
    "Leveraging the `filter` section above, run the next cell to create a new [`transformSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transformspec) that contains this filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094b74f-4e1d-43af-8c77-74d3bca82630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"filter\": {\n",
    "        \"type\": \"in\",\n",
    "        \"dimension\": \"event_type\",\n",
    "        \"values\": [\n",
    "          \"add_to_cart\",\n",
    "          \"drop_from_cart\"\n",
    "        ]\n",
    "      }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3921d-d771-46f3-a295-38bf7beb74d1",
   "metadata": {},
   "source": [
    "Run this cell to build an `ingestionSpec` object, this time including the `transformSpec` above. It will also resurrect the `event_type` column to enable add and drop actions to be differentiated in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d911e-ecf2-4461-9c24-84168b0f7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_cart = topic_name + \"-cart\"\n",
    "\n",
    "dataSchema_dimensionsSpec = {\n",
    "        \"dimensions\": [\n",
    "          \"user_id\",\n",
    "          \"event_type\",\n",
    "          \"client_ip\",\n",
    "          \"client_device\",\n",
    "          \"client_lang\",\n",
    "          \"client_country\",\n",
    "          \"referrer\",\n",
    "          \"keyword\",\n",
    "          \"product\"\n",
    "        ]\n",
    "      }\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_cart,\n",
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf892a9-b180-4db0-9e15-ca8f1f6c436e",
   "metadata": {},
   "source": [
    "Submit the new specification for ingestions from this Apache Kafka topic by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c93431-4418-4bfd-b970-b0354e728849",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)\n",
    "\n",
    "druid.sql.wait_until_ready(table_cart, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780708a3-da27-4195-a450-f53007a4f289",
   "metadata": {},
   "source": [
    "Run the query below to see the effect this has had on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cf5ab-cba3-43d4-a43d-9b095311d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "print(time_now)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_cart}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT1M/''' + time_now + '''')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "print(\"This data is filtered at ingestion time:\")\n",
    "display.sql(sql)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT1M/''' + time_now + '''')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "print(\"This data is unfiltered:\")\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ceca5-cecc-4710-8092-9aa0f41488af",
   "metadata": {},
   "source": [
    "Notice above that \"view_cart\" is not being ingested into the table.\n",
    "\n",
    "Run the following cell to switch from an \"in\" type filter to a \"[like](https://druid.apache.org/docs/29.0.1/querying/filters/#like-filter)\" filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c0fda-6905-474d-9549-10b6672f533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"filter\": {\n",
    "        \"type\": \"like\",\n",
    "        \"dimension\": \"event_type\",\n",
    "        \"pattern\" : \"%cart%\"\n",
    "      }\n",
    "}\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_cart,\n",
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)\n",
    "\n",
    "druid.sql.wait_until_ready(table_cart, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b8ac5-59fd-43c1-8547-03c98a8ae5d1",
   "metadata": {},
   "source": [
    "Wait for a few moments for the old tasks to terminate, and for new tasks to start using this new configuration.\n",
    "\n",
    "When done, run the next cell to see the effect.\n",
    "\n",
    "Notice that, in the method used in this example, \"view cart\" actions are only included from this point forward in the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c82bc-0f18-444e-979c-a794f2d69c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "print(time_now)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_cart}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT1M/''' + time_now + '''')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "print(\"This data is filtered at ingestion time:\")\n",
    "display.sql(sql)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT1M/''' + time_now + '''')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "print(\"This data is unfiltered:\")\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to stop the data generator, stop ingestion from the topic, and to remove the table used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stop streaming generator: [{requests.post(f'{druid_host}/stop/{topic_name}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{topic_name}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "        \n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_name}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_searches)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_searches)\n",
    "        \n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_searches}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_searches}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_searches)}]\")\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_cart)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_cart)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_cart}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{table_cart}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_cart)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Filters can be applied to data from Apache Kafka as soon as it arrives.\n",
    "* Typical SQL WHERE filtering has native counterparts that you can use as a filter in the `transformSpec`.\n",
    "* Unless the topic offset is reset manually, expressions only apply to new data as it arrives.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try using [logical expression filters](https://druid.apache.org/docs/latest/querying/filters#logical-expression-filters) to add AND and OR conditions in your filters.\n",
    "* Read about more advanced filters, such as [regular expression](https://druid.apache.org/docs/latest/querying/filters#regular-expression-filter) and [expression](https://druid.apache.org/docs/latest/querying/filters#expression-filter) filters.\n",
    "* Check out the notebook on transforming data at ingestion time using [expressions](13-native-transforms.ipynb) and then combine what you've learned here with an [extraction filter](https://druid.apache.org/docs/latest/querying/filters#extraction-filter).\n",
    "* Re-run this notebook, but manually hard reset the supervisor between posting a new ingestion specification. You can do this either with a [POST](https://druid.apache.org/docs/latest/api-reference/supervisor-api#reset-a-supervisor) or [through the console](https://druid.apache.org/docs/latest/operations/web-console#supervisors). What do you expect to happen?\n",
    "* Refer to the documentation on [native transform expressions](https://druid.apache.org/docs/latest/querying/math-expr)."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
