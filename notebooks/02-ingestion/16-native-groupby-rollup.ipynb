{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Aggregating source data by using rollup with streaming ingestion\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Data streams often contain many millions of rows of data at a very high time precision. This level of detail is often just not needed - let alone fit onto a user's screen! With batch ingestion, a GROUP BY can be incorporated into the SQL to aggregate incoming rows. In streaming ingestion, a similar mechanism exists called \"[rollup](https://druid.apache.org/docs/latest/ingestion/rollup)\".\n",
    "\n",
    "This tutorial shows examples of some rollup functionality against an example dataset incoming data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 30.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b769122-c5a4-404e-9ef8-9c0ebd97695a",
   "metadata": {},
   "source": [
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
    "###Â Set up a connection to Apache Kafka\n",
    "\n",
    "<!-- Include these cells if your notebook uses Kafka. -->\n",
    "\n",
    "Run the next cell to set up the connection to Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffed9a-87a5-4b26-9f06-972fdbccd55a",
   "metadata": {},
   "source": [
    "### Set up a connection to the data generator\n",
    "\n",
    "<!-- Include these cells if your notebook uses the data generator. -->\n",
    "\n",
    "Run the next cell to set up the connection to the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807280f-b387-4ffb-8bc3-764c5b511458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "datagen_host = \"http://datagen:9999\"\n",
    "datagen_headers = {'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b732-c843-4483-896d-c7972ba9e4be",
   "metadata": {},
   "source": [
    "### Start data generation to a Kafka topic\n",
    "\n",
    "Run the following cell to instruct the data generator to start producing data and display the status of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ef424-cd35-416c-9cd0-14f71d43d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_topic = \"example-clickstream-rollup\"\n",
    "datagen_job = datagen_topic\n",
    "datagen_config = \"social/social_posts.json\"\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": datagen_job,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": datagen_topic  },\n",
    "    \"config_file\": datagen_config, \n",
    "    \"concurrency\":100\n",
    "}\n",
    "\n",
    "requests.post(f\"{datagen_host}/start\", json.dumps(datagen_request), headers=datagen_headers)\n",
    "requests.get(f\"{datagen_host}/status/{datagen_job}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edceb30b-7278-43e3-ae7d-a2f4b7052a94",
   "metadata": {},
   "source": [
    "### Take a look at a sample of the data\n",
    "\n",
    "Run the cell below to show the data that you will use in this notebook by connecting to and printing out raw events from the Kafka stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a343d-ea6b-4797-8da9-517c0e7fbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=kafka_host)\n",
    "consumer.subscribe(topics=datagen_topic)\n",
    "count = 0\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "consumer.unsubscribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fedf1f-566f-4501-be65-d20e216d2c59",
   "metadata": {},
   "source": [
    "### Set re-usable elements for the ingestion specification\n",
    "\n",
    "Ingest data from an Apache Kafka topic into Apache Druid by submitting an [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html) to the [streaming ingestion supervisor API](https://druid.apache.org/docs/latest/api-reference/supervisor-api).\n",
    "\n",
    "An ingestion specification contains three parts:\n",
    "\n",
    "* [`ioConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig).\n",
    "* [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig).\n",
    "* [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema).\n",
    "\n",
    "Run the cell below to create objects to represent the `ioConfig` and `tuningConfig` that you will re-use throughout the notebook, and a variable to hold a table name that we will put the data from Kafka into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8734c-e968-45de-a1bf-197d3bffb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "    \"topic\": datagen_topic,\n",
    "    \"inputFormat\": { \"type\": \"json\" },\n",
    "    \"useEarliestOffset\": \"true\" }\n",
    "\n",
    "tuningConfig = { \"type\": \"kafka\" }\n",
    "\n",
    "table_name = datagen_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9746574-8b1f-4a1b-882a-3ac996d063e4",
   "metadata": {},
   "source": [
    "## Create a table with aggregated source data\n",
    "\n",
    "Aggregate streaming data at ingestion time by setting `rollup` to `true` in your ingestion specification's [`granularitySpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#granularityspec), part of the [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). This instructs Druid to apply a GROUP BY to the incoming data.\n",
    "\n",
    "In the following sections, you will:\n",
    "\n",
    "* Create an aggregated view of the source data.\n",
    "* Enrich the table with simple aggregates calculated from the source data.\n",
    "* Enrich the table with sketch objects that are used for approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f176638-05f9-40bd-8cad-6bc463eafd80",
   "metadata": {},
   "source": [
    "### Use `rollup` and `queryGranularity` to aggregate source data rows\n",
    "\n",
    "The efficiency of GROUP BY is dependent on the cardinality of all dimensions, including the timestamp.\n",
    "\n",
    "Affect the cardinality of the timestamp by flooring the source time field using `queryGranularity`. Set this to the desired level of [granularity](https://druid.apache.org/docs/latest/querying/granularities) in the `granularitySpec`.\n",
    "\n",
    "A simple approach to handling the cardinality of all other dimensions is to be specific about the dimensions you wish to keep in your in the `dimensionsSpec`. Another might be to apply an expression, such as truncation, inside a [`transformSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transformspec).\n",
    "\n",
    "Run the cell below to create a `granularitySpec`.\n",
    "\n",
    "* Rollup is enabled.\n",
    "* Incoming timestamps will be truncated via `queryGranularity` to a precision of `FIFTEEN_MINUTE`.\n",
    "* `post_title`, `views`, `upvotes` and `comments`, which all have high cardinality, are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828b7d7-87f1-4319-9df2-bfa67c887fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_granularitySpec = {\n",
    "    \"rollup\": \"true\",\n",
    "    \"queryGranularity\": \"fifteen_minute\",\n",
    "    \"segmentGranularity\": \"hour\"\n",
    "}\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "    \"granularitySpec\": dataSchema_granularitySpec,\n",
    "    \"dimensionsSpec\": {\n",
    "        \"dimensions\": [\n",
    "            \"username\",\n",
    "            \"edited\"\n",
    "            ]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac829e-5a18-49d6-a9d3-7b296586de39",
   "metadata": {},
   "source": [
    "Run the next cell to build the ingestion specification and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83b8af-e0c0-46f7-a382-d24569117f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b275e23-6bb5-4350-9dae-59dc056d5c4e",
   "metadata": {},
   "source": [
    "Run the cell below to post the ingestion specification and start ingesting from the topic. When finished, you'll see a description of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981ca7c-a330-4832-bebe-b6dfb6fc4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=druid_headers)\n",
    "sql_client.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52419086-eb70-44d2-856d-bd01733f6b0f",
   "metadata": {},
   "source": [
    "To see some of the data in the table, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98245e55-14fd-4bda-92c4-3fe24e90674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT * FROM \"{table_name}\"\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6bf3ad-78ed-48f9-bd4a-d21bee3aa1d2",
   "metadata": {},
   "source": [
    "As you might expect, you see that each time period (truncated) is grouped into `username` and then `edited`.\n",
    "\n",
    "Run the next cell to stop ingestion and drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e7304-5474-4fa0-aa21-c96d3ac866f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop table: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7969cb-c417-4e01-b037-8e6a68e24494",
   "metadata": {},
   "source": [
    "### Generate simple aggregations at rollup time\n",
    "\n",
    "Rather than dropping the all-important measures from the source data, in this section a `metricsSpec` will be introduced.\n",
    "\n",
    "A [`metricsSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#metricsspec) contains a list of [native aggregations](https://druid.apache.org/docs/latest/querying/aggregations) to apply to the raw data as it arrives. \n",
    "\n",
    "Run the cell below to create an object that will hold the `metricsSpec`. It will instruct Druid to generate new columns in the table of the name `name` using the specified `type` of aggregator (SUM, MAX, MIN) using `fieldName` from the source data.\n",
    "\n",
    "You'll see a print out of the final ingestion spec before it is submitted and, when done, a description of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad743195-2319-4869-9b59-419e2e312af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_metricsSpec = [\n",
    "    { \"name\" : \"count\", \"type\" : \"count\" },\n",
    "    { \"name\" : \"sum_views\", \"type\" : \"longSum\", \"fieldName\" : \"views\" },\n",
    "    { \"name\" : \"max_views\", \"type\" : \"longMax\", \"fieldName\" : \"views\" },\n",
    "    { \"name\" : \"min_views\", \"type\" : \"longMin\", \"fieldName\" : \"views\" },\n",
    "    { \"name\" : \"sum_upvotes\", \"type\" : \"longSum\", \"fieldName\" : \"upvotes\" },\n",
    "    { \"name\" : \"sum_comments\", \"type\" : \"longSum\", \"fieldName\" : \"comments\" }\n",
    "    ]\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "    \"granularitySpec\": dataSchema_granularitySpec,\n",
    "    \"dimensionsSpec\": {\n",
    "        \"dimensions\": [\n",
    "            \"username\",\n",
    "            \"edited\"\n",
    "            ]\n",
    "        },\n",
    "    \"metricsSpec\" : dataSchema_metricsSpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=3))\n",
    "\n",
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=druid_headers)\n",
    "sql_client.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48bfbab-2025-41d1-892e-f5092b82de9c",
   "metadata": {},
   "source": [
    "Run the following cell to see the result.\n",
    "\n",
    "Because this is live data fed from a stream, remember that the latest figures will change each time you run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69996e7f-7eba-43cc-b0dc-bdb8cfeed969",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT * FROM \"{table_name}\"\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67441aa2-126a-4f1b-90d0-76c766374333",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the next cell to stop the ingestion and drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f522f-739f-497b-b729-03b2bb361000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop table: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178ce29-b5f0-4b09-a0fd-ad2a899dc13b",
   "metadata": {},
   "source": [
    "### Speed up approximation by generating Apache Datasketch objects at ingestion time\n",
    "\n",
    "The remaining high-cardinality column in the data is `username`. Similarly, IP addresses, session Ids, device Ids, and so on, can also be high cardinality and effect aggregation.\n",
    "\n",
    "When users do not need to access the raw values, use [Apache Datasketches](https://datasketches.apache.org/) to improve the effectiveness of rollup and speed up COUNT DISTINCT - and other operations - even further.\n",
    "\n",
    "Run the cell below to replace the `username` with a new column, `username_hll`, using the [HLL Sketch](https://druid.apache.org/docs/latest/development/extensions-core/datasketches-hll) build aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d475ac-6100-426a-9080-1a943d346c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_metricsSpec = [\n",
    "    { \"name\" : \"count\", \"type\" : \"count\" },\n",
    "    { \"name\" : \"views_sum\", \"type\" : \"longSum\", \"fieldName\" : \"views\" },\n",
    "    { \"name\" : \"views_max\", \"type\" : \"longMax\", \"fieldName\" : \"views\" },\n",
    "    { \"name\" : \"views_min\", \"type\" : \"longMin\", \"fieldName\" : \"views\" },\n",
    "    { \"name\" : \"upvotes_sum\", \"type\" : \"longSum\", \"fieldName\" : \"upvotes\" },\n",
    "    { \"name\" : \"comments_sum\", \"type\" : \"longSum\", \"fieldName\" : \"comments\" },\n",
    "    { \"name\" : \"username_hll\", \"type\" : \"HllSketchBuild\", \"fieldName\" : \"username\" }\n",
    "    ]\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "    \"granularitySpec\": dataSchema_granularitySpec,\n",
    "    \"dimensionsSpec\": {\n",
    "        \"dimensions\": [\n",
    "            \"edited\"\n",
    "            ]\n",
    "        },\n",
    "    \"metricsSpec\" : dataSchema_metricsSpec\n",
    "    }\n",
    "\n",
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967dbe6f-d1dc-4657-a323-8e802fc325c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the next cell to post this ingestion specification to Druid and start building the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e8b6a-6efa-4f16-bc05-4d555ec94722",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=druid_headers)\n",
    "sql_client.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d31155-68a8-4eae-91c6-74f4c4ebc879",
   "metadata": {},
   "source": [
    "Run the next cell to take a peek at the data.\n",
    "\n",
    "Using `queryGranularity` to truncate the timestamp, being specific about the `dimensions`, and adding a `metricsSpec` rather than storing raw values, you have created a fully aggregated table that maintains only essential attributes of each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7285643-4f7a-4f38-9ad9-ac37b134a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sql=f'''\n",
    "SELECT * FROM \"{table_name}\"\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4328d-f95d-4bed-a580-308b34ad3de5",
   "metadata": {},
   "source": [
    "### Ingest a table containing objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2febda-8e7c-4716-9277-e87520c0eeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8a6ed0-a77a-466c-aa65-48049c9bacf6",
   "metadata": {},
   "source": [
    "## Filtering source data to be included\n",
    "\n",
    "https://druid.apache.org/docs/latest/querying/filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a27b0-5e99-4d26-ab49-3e92238fe388",
   "metadata": {},
   "source": [
    "## Filter out rows based on metrics\n",
    "\n",
    "https://druid.apache.org/docs/latest/querying/having"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the XXX used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop table: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You learned this\n",
    "* Remember this\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* See examples of transformations on streaming data in the notebook on [native transforms](./13-native-transforms.ipynb).\n",
    "* Learn more about approximation in Druid by looking at the notebooks on approximate [ranking](../03-query/02-approx-ranking.ipynb), [COUNT DISTINCT](../03-query/03-approx-count-distinct.ipynb), and [distribution](../03-query/04-approx-distribution.ipynb).\n",
    "* See more examples of [generating Data Sketches](./03-generating-sketches.ipynb) by looking at the related notebook for batch ingestion."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
