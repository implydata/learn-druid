{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Transforming incoming stream data using native functions\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "During streaming ingestion, you can transform incoming data using Apache Druid native functions within the `transformSpec`. This tutorial demonstrates how to apply these functions as [transforms](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transforms) from a stream of events.\n",
    "\n",
    "In this tutorial, you perform the following tasks:\n",
    "\n",
    "- Set up a streaming ingestion from Apache Kafka.\n",
    "- Apply some example transformations to update incoming data.\n",
    "- Create a new dimension using data from other dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
    "###Â Set up a connection to Apache Kafka\n",
    "\n",
    "Run the next cell to set up the connection to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2bf68-28ed-4b46-b320-40c73cd7f9b8",
   "metadata": {},
   "source": [
    "### Set up a connection to the Data Generator\n",
    "\n",
    "Run the next cell to set up the connection to the Data Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2f012-85b2-48c7-9714-b3d96055c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "datagen_host = \"http://datagen:9999\"\n",
    "datagen_headers = {'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b7439-ad21-4808-96b1-8e3c992fa51e",
   "metadata": {},
   "source": [
    "## Create a table using streaming ingestion\n",
    "\n",
    "In this section, you use the data generator to generate a stream of messages into a Kafka topic. Next, you set up an on-going ingestion into Druid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cd909-ef3e-4ae3-bf81-4f1950ca3c65",
   "metadata": {},
   "source": [
    "### Use the data generator to populate a Kafka topic\n",
    "\n",
    "Run the following cell to instruct the data generator to start producing data.\n",
    "\n",
    "This creates clickstream sample data for an hour and publishes it to a Kafka topic for Druid to consume from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec019-7145-4005-bb85-ea25eda7bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_topic = \"example-clickstream-transforms\"\n",
    "datagen_job = f\"{datagen_topic}\"\n",
    "datagen_config = \"social/social_posts.json\"\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": datagen_job,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": datagen_topic },\n",
    "    \"config_file\": \"clickstream/clickstream.json\",\n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":10,\n",
    "    \"time_type\": \"REAL\"\n",
    "}\n",
    "\n",
    "print(datagen_request)\n",
    "\n",
    "requests.post(f\"{datagen_host}/start\", json.dumps(datagen_request), headers=datagen_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1446106-36be-4cd3-a2b1-0ca2d9e945a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f\"{datagen_host}/status/{datagen_job}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737176a-6c32-427b-b004-bb3294c2a3ff",
   "metadata": {},
   "source": [
    "### Use streaming ingestion to populate the table\n",
    "\n",
    "Ingest data from the Kafka topic into Druid by submitting an [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html) to the [streaming ingestion supervisor API](https://druid.apache.org/docs/latest/api-reference/supervisor-api).\n",
    "\n",
    "Run the next cell to set up the [`ioConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig) and [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig) properties of the ingestion specification. These configurations connect to the same Kafka host used by the data generator and consume the JSON data being published to the data generator topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb89079-7e2a-404b-be85-d9fc7c97d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "    \"topic\": datagen_topic,\n",
    "    \"inputFormat\": { \"type\": \"json\" },\n",
    "    \"useEarliestOffset\": \"true\" }\n",
    "\n",
    "tuningConfig = { \"type\": \"kafka\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee01e6-d620-4ca7-9f2b-d49ab839ca5a",
   "metadata": {},
   "source": [
    "Run the next cell to create a series of objects that will ultimately be used to define the table through [dataSchema](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). These include:\n",
    "\n",
    "* [timestampSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec): uses the `time` column from the generated data as the primary timestamp.\n",
    "* [dimensionsSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimensionsspec): specifies all of the columns from the incoming data.\n",
    "* [granularitySpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#granularityspec): puts data into daily segment files without any ingestion-time aggregation ([rollup](https://druid.apache.org/docs/latest/ingestion/rollup)).\n",
    "\n",
    "In the final statement, the `dataSchema` is built from these three objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804095a-1b9f-48dd-9c13-11c1c083e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_timestampSpec = { \"column\": \"time\", \"format\": \"iso\" }\n",
    "dataSchema_granularitySpec = { \"rollup\": \"false\", \"segmentGranularity\": \"day\" }\n",
    "dataSchema_dimensionsSpec = {\n",
    "        \"dimensions\": [\n",
    "          \"user_id\",\n",
    "          \"event_type\",\n",
    "          \"client_ip\",\n",
    "          \"client_device\",\n",
    "          \"client_lang\",\n",
    "          \"client_country\",\n",
    "          \"referrer\",\n",
    "          \"keyword\",\n",
    "          \"product\"\n",
    "        ]\n",
    "      }\n",
    "\n",
    "table_name = datagen_topic\n",
    "\n",
    "dataSchema = {\n",
    "      \"dataSource\": table_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50d1bb-1bc9-4ef1-a1a7-637cc3bb4616",
   "metadata": {},
   "source": [
    "Run the next cell to submit the ingestion spec to Druid and start ingesting raw data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5125-1096-462a-a637-cd8f438a2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Transform data using string functions\n",
    "\n",
    "In this section, you use a [native expression](https://druid.apache.org/docs/latest/querying/math-expr) to transform new data as it arrives. While this example uses a string function, the same mechanism applies to other native functions, including numeric, IP, and date and time functions.\n",
    "\n",
    "Take a look at the current data by running the following SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00032a2e-1d73-4dbd-88ff-697db342ecdc",
   "metadata": {},
   "source": [
    "Add a [`transformSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transformspec) object to the ingestion specification to turn all country names into upper case as data arrives.\n",
    "\n",
    "Run the following cell to add an \"upper\" [expression](https://druid.apache.org/docs/latest/querying/math-expr#string-functions) to a collection (`transforms`) inside a new object that will then be incorporated into the ingestion specification.\n",
    "\n",
    "* The upper `expression` is calculated using `client_country`.\n",
    "* The `name` instructs Druid to write the result back into `client_country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094b74f-4e1d-43af-8c77-74d3bca82630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"transforms\":\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"client_country\",\n",
    "            \"expression\": \"upper(client_country)\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3921d-d771-46f3-a295-38bf7beb74d1",
   "metadata": {},
   "source": [
    "Now run this cell to rebuild the `ingestionSpec` object, this time including the `transformSpec` object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d911e-ecf2-4461-9c24-84168b0f7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestion_spec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf892a9-b180-4db0-9e15-ca8f1f6c436e",
   "metadata": {},
   "source": [
    "Review the output above to see where the `transforms` have been added within the `dataSchema`.\n",
    "\n",
    "Run the next cell to submit the new specification for ingestions from this Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c93431-4418-4bfd-b970-b0354e728849",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb66c3-9cd2-4b82-a95b-af47874b8321",
   "metadata": {},
   "source": [
    "Wait for a moment or two. This will allow Druid to apply the new configuration for the ingestion by shutting down the existing task, and starting a new ingestion task with the transforms.\n",
    "\n",
    "Then, run the following query to see the effect this has had on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cf5ab-cba3-43d4-a43d-9b095311d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "print(time_now)\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_now}')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ceca5-cecc-4710-8092-9aa0f41488af",
   "metadata": {},
   "source": [
    "Since the new ingestion specification continued where the old one finished, the function has only been applied to new data.\n",
    "\n",
    "Run the following cell to see the values for `client_country` in data five minutes ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f55492-3682-4d60-b6c0-0a8d77b8066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_then = (datetime.now() -  timedelta(hours=0, minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_then}')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280251e-2594-45ec-a098-87e2da61be5c",
   "metadata": {},
   "source": [
    "## Use CASE to generate NULL values\n",
    "\n",
    "In this section, you use a case function to catch raw data that has an \"unknown\" or \"none\" value and replace that value with NULL.\n",
    "\n",
    "* case_searched() checks if the value of `keyword` is \"None\". If it is, null is returned, otherwise the existing value is returned.\n",
    "* The same method is then used on `product` and `referrer`.\n",
    "* In each case, the `name` means that each time, the data from each evaluated dimensions is being overwritten.\n",
    "\n",
    "Run the next cell to change the `transformSpec` object, rebuild the `dataSchema`, and build the final `ingestionSpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575a357-cfc2-4e71-baf6-03f449fe4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"transforms\":\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"keyword\",\n",
    "            \"expression\": \"case_searched((\\\"keyword\\\" == 'None'),null,\\\"keyword\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"product\",\n",
    "            \"expression\": \"case_searched((\\\"product\\\" == 'None'),null,\\\"product\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"referrer\",\n",
    "            \"expression\": \"case_searched((\\\"referrer\\\" == 'unknown'),null,\\\"referrer\\\")\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestion_spec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af961b1b-7b1b-40b0-b064-930a4b0b24c2",
   "metadata": {},
   "source": [
    "Run the next cell to submit the updated ingestion specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a28ea7-f7a1-443b-9b41-90cfa713de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc99dae-f89c-4a23-9a12-54c95ff759f4",
   "metadata": {},
   "source": [
    "Wait for a moment or two. This allows Druid to apply the new configuration for the ingestion by shutting down the existing task and starting a new ingestion task with the transforms.\n",
    "\n",
    "Run the following cell a number of times to see how it affects the data in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0e2e1-498c-49fc-af91-17b1935b63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"keyword\",\n",
    "  \"referrer\",\n",
    "  \"product\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_now}')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a6da9-94b7-4db4-a022-a1e77676b1b9",
   "metadata": {},
   "source": [
    "## Use a function to add a new column\n",
    "\n",
    "Since all the examples above use the same `name` as an existing column, the supplied `expression` overwrites the data in that column. In this example, you use a new column name to create a new dimension from the incoming data.\n",
    "\n",
    "This is a two-step process: first, add the new column to the table schema, then set the function to evaluate.\n",
    "\n",
    "Run the next cell to achieve this.\n",
    "\n",
    "* The `dimensionsSpec` has a new dimension called \"time_friendly\".\n",
    "* The `transformSpec` has a new transform that applies a date formatting function.\n",
    "* The `dataSchema` is then built from these new objects.\n",
    "* The `ingestionSpec` is then built up from the new `dataSchema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d0ea6-bddb-4f47-bfa9-c845ca51dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_dimensionsSpec = {\n",
    "        \"dimensions\": [\n",
    "          \"user_id\",\n",
    "          \"event_type\",\n",
    "          \"client_ip\",\n",
    "          \"client_device\",\n",
    "          \"client_lang\",\n",
    "          \"client_country\",\n",
    "          \"referrer\",\n",
    "          \"keyword\",\n",
    "          \"product\",\n",
    "          \"time_friendly\"\n",
    "        ]\n",
    "      }\n",
    "\n",
    "dataSchema_transformSpec = {\n",
    "    \"transforms\":\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"keyword\",\n",
    "            \"expression\": \"case_searched((\\\"keyword\\\" == 'None'),null,\\\"keyword\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"product\",\n",
    "            \"expression\": \"case_searched((\\\"product\\\" == 'None'),null,\\\"product\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"referrer\",\n",
    "            \"expression\": \"case_searched((\\\"referrer\\\" == 'unknown'),null,\\\"referrer\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"time_friendly\",\n",
    "            \"expression\": \"timestamp_format(\\\"__time\\\",'E dd MM yyyy','UTC')\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestion_spec, indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80b220-e61b-4af5-bf3b-1a2dac73d3f7",
   "metadata": {},
   "source": [
    "Run the next cell to submit this new ingestion specification for the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bacc4-7cc9-4808-a665-62289ce44032",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc861c-3fa2-4421-8acf-5606543c420d",
   "metadata": {},
   "source": [
    "While waiting for the new ingestion to be applied, try this trick for finding the exact expression to put into your ingestion specifications.\n",
    "\n",
    "* Go to [http://localhost:8888/] (http://localhost:8888/) to open the Druid console.\n",
    "* Switch to the query view and build a SQL statement against the table using a function you know well.\n",
    "* Instead of running the SQL, use the button with the three dots to find \"Explain SQL query\".\n",
    "* Notice the \"virtual columns\" section contains the equivallent native expression.\n",
    "\n",
    "Now run this cell to see the result of your updated ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74f5cb-1480-4c90-9846-afd8d52d5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"time_friendly\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_now}')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc06e50-15dd-49d0-8362-2be5527c8e1e",
   "metadata": {},
   "source": [
    "Run the next cell to see that the same query for an older time period returns NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc47e15-a35d-43c5-9dc2-ba4a742b5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_then = (datetime.now() -  timedelta(hours=0, minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"time_friendly\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_then}')\n",
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a39b0-6d36-45e8-98be-a4ea62de355c",
   "metadata": {},
   "source": [
    "In the example above, a new column was created from the primary timestamp. As noted in the [ingestion spec reference](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec), you can use the same mechanism to overwrite the timestamp (`__time`). For example, to create a compliant format from non-standard datetime representations before it lands in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to stop the data generator, stop ingestion from the topic, and remove the table used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You can apply functions to Kafka data as soon as it arrives.\n",
    "* SQL functions have native counterparts that you can use as a transform in `transformSpec`.\n",
    "* Expressions can be used to overwrite data or to create new columns.\n",
    "* Unless the topic offset is reset manually, expressions only apply to new data as it arrives.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try to use some more native functions such as numeric, IP, and more complex string functions.\n",
    "* Re-run this notebook, but manually hard reset the supervisor between posting a new ingestion specification. You can do this either with a [POST](https://druid.apache.org/docs/latest/api-reference/supervisor-api#reset-a-supervisor) request or [through the console](https://druid.apache.org/docs/latest/operations/web-console#supervisors). What do you expect to happen?\n",
    "* Review the documentation on [native transform expressions](https://druid.apache.org/docs/latest/querying/math-expr)."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
