{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Transforming incoming stream data using native functions\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "During streaming ingestion, functions can be applied to incoming data by using native functions inside the `transformSpec`. This tutorial demonstrates how to apply these functions as [transforms](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transforms) from a stream of events.\n",
    "\n",
    "In this tutorial you perform the following tasks:\n",
    "\n",
    "- Set up a streaming ingestion from Apache Kafka.\n",
    "- Apply some example transformations to update incoming data.\n",
    "- Create a new dimension based on data from other dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
<<<<<<< HEAD
    "### Set up a connection to Apache Druid\n",
=======
    "### Set up and connect to the learning environment\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
<<<<<<< HEAD
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
=======
    "import requests\n",
    "from datetime import datetime, timedelta\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
<<<<<<< HEAD
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
=======
    "    \n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "###Â Set up a connection to Apache Kafka\n",
    "\n",
    "Run the next cell to set up the connection to Apache Kafka."
=======
    "Run the next cell to set up the connection to Apache Kafka and to the Data Generator, and to import some helper functions for later in the tutorial."
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2bf68-28ed-4b46-b320-40c73cd7f9b8",
   "metadata": {},
   "source": [
    "### Set up a connection to the Data Generator\n",
    "\n",
    "Run the next cell to set up the connection to the Data Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2f012-85b2-48c7-9714-b3d96055c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "datagen_host = \"http://datagen:9999\"\n",
    "datagen_headers = {'Content-Type': 'application/json'}"
=======
    "import requests\n",
    "import json\n",
    "import os\n",
    "import kafka\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "datagenUrl = \"http://datagen:9999\"\n",
    "\n",
    "generalHeaders = {'Content-Type': 'application/json'}\n",
    "\n",
    "if (os.environ['KAFKA_HOST'] == None):\n",
    "    kafka_host=f\"kafka:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b7439-ad21-4808-96b1-8e3c992fa51e",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Create a table using streaming ingestion\n",
    "\n",
    "In this section, you use the data generator to generate a stream of messages into a Apache Kafka topic. Next, you'll set up an on-going ingestion into Druid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cd909-ef3e-4ae3-bf81-4f1950ca3c65",
   "metadata": {},
   "source": [
    "### Use the data generator to populate a Kafka topic\n",
    "\n",
    "Run the following cell to instruct the data generator to start producing data.\n",
=======
    "### Start a data stream\n",
    "\n",
    "Run the following cell to use the learn-druid Data Generator to create a stream that we can consume from.\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "\n",
    "This will create clickstream sample data for an hour and publish it to a topic in Apache Kafka for Apache Druid to consume from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec019-7145-4005-bb85-ea25eda7bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "datagen_topic = \"example-clickstream-transforms\"\n",
    "datagen_job = f\"{datagen_topic}\"\n",
    "datagen_config = \"social/social_posts.json\"\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": datagen_job,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": datagen_topic },\n",
=======
    "topic_name = \"example-clickstream-transforms\"\n",
    "\n",
    "job_name=\"example_clickstream\"\n",
    "\n",
    "target = {\n",
    "    \"type\":\"kafka\",\n",
    "    \"endpoint\": kafka_host,\n",
    "    \"topic\": topic_name\n",
    "}\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": topic_name,\n",
    "    \"target\": target,\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "    \"config_file\": \"clickstream/clickstream.json\",\n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":10,\n",
    "    \"time_type\": \"REAL\"\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "print(datagen_request)\n",
    "\n",
    "requests.post(f\"{datagen_host}/start\", json.dumps(datagen_request), headers=datagen_headers)"
=======
    "requests.post(f\"{datagenUrl}/start\", json.dumps(datagen_request), headers=generalHeaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284bc813-dd75-49aa-bac2-10d1016fff46",
   "metadata": {},
   "source": [
    "Run the next cell to see a sample of the raw data being emitted from the Data Generator.\n",
    "\n",
    "This cell uses a simple consumer to subscribe to the topic and to show the first 5 rows that appear."
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "f1446106-36be-4cd3-a2b1-0ca2d9e945a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f\"{datagen_host}/status/{datagen_job}\").json()"
=======
   "id": "7128c2a4-588b-4bf6-a0f9-2f002f0ecdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    " bootstrap_servers=kafka_host\n",
    ")\n",
    "\n",
    "consumer.subscribe(topics=topic_name)\n",
    "count = 0\n",
    "\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "\n",
    "consumer.unsubscribe()"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "8737176a-6c32-427b-b004-bb3294c2a3ff",
   "metadata": {},
   "source": [
    "### Use streaming ingestion to populate the table\n",
    "\n",
    "Ingest data from an Apache Kafka topic into Apache Druid by submitting an [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html) to the [streaming ingestion supervisor API](https://druid.apache.org/docs/latest/api-reference/supervisor-api).\n",
    "\n",
    "Run the next cell to set up the [`ioConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig) and [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig). These will connect to the same Kafka host being used by the data generator and consume the JSON being pushed into the data generator topic."
=======
   "id": "89dde5e8-237e-4531-84c2-8647d92ceaea",
   "metadata": {},
   "source": [
    "## Build a native ingestion specification\n",
    "\n",
    "Run the following cell to create an [ioConfig](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig) object that sets the connection to the topic from Apache Druid."
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb89079-7e2a-404b-be85-d9fc7c97d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
<<<<<<< HEAD
    "    \"type\": \"kafka\",\n",
    "    \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "    \"topic\": datagen_topic,\n",
    "    \"inputFormat\": { \"type\": \"json\" },\n",
    "    \"useEarliestOffset\": \"true\" }\n",
    "\n",
=======
    "      \"type\": \"kafka\",\n",
    "      \"consumerProperties\": {\n",
    "        \"bootstrap.servers\": \"kafka:9092\"\n",
    "      },\n",
    "      \"topic\": \"example-clickstream-transforms\",\n",
    "      \"inputFormat\": {\n",
    "        \"type\": \"json\"\n",
    "      },\n",
    "      \"useEarliestOffset\": \"false\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41302f5-5a99-4038-8b87-c13b9838b403",
   "metadata": {},
   "source": [
    "Run the next cell to create a very simple [tuningConfig](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig) object for the tuning configuration for the ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8cf7d-913e-442f-ade7-7eb28d98f724",
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "tuningConfig = { \"type\": \"kafka\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee01e6-d620-4ca7-9f2b-d49ab839ca5a",
   "metadata": {},
   "source": [
    "Run the next cell to create a series of objects that will ultimately be used to define the table through a [dataSchema](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). These include:\n",
    "\n",
    "* A [timestampSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec) which uses the `time` column from the generated data as the primary timestamp.\n",
    "* A [dimensionsSpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dimensionsspec). In this example, we manually specify all of the columns from the incoming data.\n",
    "* A [granularitySpec](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#granularityspec) which puts data into daily segment files without any ingestion-time aggregation ([rollup](https://druid.apache.org/docs/latest/ingestion/rollup)).\n",
    "\n",
    "In the final statement, the `dataSchema` is built from these three objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804095a-1b9f-48dd-9c13-11c1c083e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "dataSchema_timestampSpec = { \"column\": \"time\", \"format\": \"iso\" }\n",
    "dataSchema_granularitySpec = { \"rollup\": \"false\", \"segmentGranularity\": \"day\" }\n",
=======
    "dataSchema_timestampSpec = {\n",
    "    \"column\": \"time\",\n",
    "    \"format\": \"iso\"\n",
    "    }\n",
    "\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "dataSchema_dimensionsSpec = {\n",
    "        \"dimensions\": [\n",
    "          \"user_id\",\n",
    "          \"event_type\",\n",
    "          \"client_ip\",\n",
    "          \"client_device\",\n",
    "          \"client_lang\",\n",
    "          \"client_country\",\n",
    "          \"referrer\",\n",
    "          \"keyword\",\n",
    "          \"product\"\n",
    "        ]\n",
    "      }\n",
    "\n",
<<<<<<< HEAD
    "table_name = datagen_topic\n",
    "\n",
    "dataSchema = {\n",
    "      \"dataSource\": table_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}"
=======
    "dataSchema_granularitySpec = {\n",
    "        \"queryGranularity\": \"none\",\n",
    "        \"rollup\": \"false\",\n",
    "        \"segmentGranularity\": \"day\"\n",
    "      }\n",
    "\n",
    "dataSchema = {\n",
    "      \"dataSource\": topic_name,\n",
    "      \"timestampSpec\": dataSchema_timestampSpec,\n",
    "      \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "      \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e24a56-3d9e-4589-8d0c-113a5b5ec8e9",
   "metadata": {},
   "source": [
    "Now run the next cell to create the final native [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c5ebe-c785-4c2f-ba2e-26de0f6ab8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50d1bb-1bc9-4ef1-a1a7-637cc3bb4616",
   "metadata": {},
   "source": [
    "Start the ingestion of the raw data from Apache Kafka by submitting this object to Apache Druid by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5125-1096-462a-a637-cd8f438a2074",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
=======
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc897578-48d7-4b8c-9ece-dab4390e2336",
   "metadata": {},
   "source": [
    "Run the following cell to wait until the ingestion has started and the new table is ready for query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830e767-ad9a-47c0-80e2-4f19cd4a20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.sql.wait_until_ready(topic_name, verify_load_status=False)\n",
    "print(\"Ready to go!\")"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Transform data using string functions\n",
    "\n",
    "In this section you will use a [native expression](https://druid.apache.org/docs/latest/querying/math-expr) to apply a transformation to new data as it arrives. While this example uses a string function, the same mechanism applies to other native functions, including numeric, IP, and date and time functions.\n",
    "\n",
<<<<<<< HEAD
    "Take a look at the current data by running the following SQL."
=======
    "Take a look at the data as is by running the following SQL."
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "sql=f'''\n",
=======
    "sql='''\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
<<<<<<< HEAD
    "FROM \"{table_name}\"\n",
=======
    "FROM \"example-clickstream-transforms\"\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "LIMIT 10\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00032a2e-1d73-4dbd-88ff-697db342ecdc",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Add a [`transformSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transformspec) to the ingestion specification to turn all country names into upper case as it arrives.\n",
    "\n",
    "Run the following cell to add an \"upper\" [expression](https://druid.apache.org/docs/latest/querying/math-expr#string-functions) to a collection (`transforms`) inside a new object that will then be incorporated into the ingestion specification.\n",
=======
    "Run the next cell to create a new object that represents the [`transformSpec`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#transformspec). This will be added to the dataSchema in the ingestion specification, instructing Druid to apply expressions to the incoming data as it arrives.\n",
    "\n",
    "Here, the collection of transformations (`transforms`) contains only one [expression](https://druid.apache.org/docs/latest/querying/math-expr#string-functions) that will be applied to the data as it arrives - upper().\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "\n",
    "* The upper() `expression` is calculated using `client_country`.\n",
    "* The `name` instructs Druid to write the result back into `client_country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094b74f-4e1d-43af-8c77-74d3bca82630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"transforms\":\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"client_country\",\n",
    "            \"expression\": \"upper(client_country)\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3921d-d771-46f3-a295-38bf7beb74d1",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Now run this cell to rebuild the `ingestionSpec` object, this time including the `transformSpec` above."
=======
    "Now you will send this updated specification for ingestion from the topic to Druid.\n",
    "\n",
    "Run this cell to rebuild the `ingestionSpec` object, this time including the `transformSpec` above."
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d911e-ecf2-4461-9c24-84168b0f7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = {\n",
<<<<<<< HEAD
    "    \"dataSource\": table_name,\n",
=======
    "    \"dataSource\": topic_name,\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
<<<<<<< HEAD
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestion_spec, indent=5))"
=======
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf892a9-b180-4db0-9e15-ca8f1f6c436e",
   "metadata": {},
   "source": [
    "Review the output above and you will see where the `transforms` have been added inside the `dataSchema`.\n",
    "\n",
    "Submit the new specification for ingestions from this Apache Kafka topic by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c93431-4418-4bfd-b970-b0354e728849",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb66c3-9cd2-4b82-a95b-af47874b8321",
   "metadata": {},
   "source": [
    "Wait for a moment or two. This will allow Druid to apply the new configuration for the ingestion by shutting down the existing task, and starting a new ingestion task with the transforms.\n",
    "\n",
    "Then run the query below to see the effect this has had on the data."
=======
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbe923-3ec7-4a20-b004-9e76054c6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the query below to see the effect this has had on the data."
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cf5ab-cba3-43d4-a43d-9b095311d05b",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from datetime import datetime, timedelta\n",
    "\n",
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "print(time_now)\n",
    "\n",
    "sql=f'''\n",
=======
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "print(time_now)\n",
    "\n",
    "sql='''\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
<<<<<<< HEAD
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_now}')\n",
=======
    "FROM \"example-clickstream-transforms\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/''' + time_now + '''')\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ceca5-cecc-4710-8092-9aa0f41488af",
   "metadata": {},
   "source": [
    "Since the new ingestion specification continued where the old one finished, the function has only been applied to new data.\n",
    "\n",
    "Run the following cell to see the values for `client_country` in data five minutes ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f55492-3682-4d60-b6c0-0a8d77b8066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_then = (datetime.now() -  timedelta(hours=0, minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
<<<<<<< HEAD
    "sql=f'''\n",
=======
    "sql='''\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "SELECT\n",
    "  \"__time\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\"\n",
<<<<<<< HEAD
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_then}')\n",
=======
    "FROM \"example-clickstream-transforms\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/''' + time_then + '''')\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280251e-2594-45ec-a098-87e2da61be5c",
   "metadata": {},
   "source": [
    "## Use CASE to generate NULL values\n",
    "\n",
    "In this section you will use a case function to catch raw data that has an \"unknown\" or \"none\" value. When caught, you will instead write NULL into the data.\n",
    "\n",
    "* case_searched() checks if the value of `keyword` is \"None\". If it is, null is returned, otherwise the existing value is returned.\n",
    "* The same method is then used on `product` and `referrer`.\n",
    "* In each case, the `name` means that each time, the data from each evaluated dimensions is being overwritten.\n",
    "\n",
    "Run the next cell to change the `transformSpec` object, to rebuild the `dataSchema`, and then build the final `ingestionSpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575a357-cfc2-4e71-baf6-03f449fe4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_transformSpec = {\n",
    "    \"transforms\":\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"keyword\",\n",
    "            \"expression\": \"case_searched((\\\"keyword\\\" == 'None'),null,\\\"keyword\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"product\",\n",
    "            \"expression\": \"case_searched((\\\"product\\\" == 'None'),null,\\\"product\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"referrer\",\n",
    "            \"expression\": \"case_searched((\\\"referrer\\\" == 'unknown'),null,\\\"referrer\\\")\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataSchema = {\n",
<<<<<<< HEAD
    "    \"dataSource\": table_name,\n",
=======
    "    \"dataSource\": topic_name,\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
<<<<<<< HEAD
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestion_spec, indent=5))"
=======
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af961b1b-7b1b-40b0-b064-930a4b0b24c2",
   "metadata": {},
   "source": [
    "Now submit the updated ingestion specification by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a28ea7-f7a1-443b-9b41-90cfa713de02",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
=======
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc99dae-f89c-4a23-9a12-54c95ff759f4",
   "metadata": {},
   "source": [
    "Wait for a moment or two. This will allow Druid to apply the new configuration for the ingestion by shutting down the existing task, and starting a new ingestion task with the transforms.\n",
    "\n",
    "Run the following cell a number of times to see what effect this has had on the data in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0e2e1-498c-49fc-af91-17b1935b63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
<<<<<<< HEAD
    "sql=f'''\n",
=======
    "sql='''\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "SELECT\n",
    "  \"__time\",\n",
    "  \"keyword\",\n",
    "  \"referrer\",\n",
    "  \"product\"\n",
<<<<<<< HEAD
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_now}')\n",
=======
    "FROM \"example-clickstream-transforms\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/''' + time_now + '''')\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a6da9-94b7-4db4-a022-a1e77676b1b9",
   "metadata": {},
   "source": [
    "## Use a function to add a new column\n",
    "\n",
    "Since all the examples above have the same `name` as an existing column, the supplied `expression` results in the data in that column being overwritten. In this example, you will use a new column name in `name` to create a new dimension that uses data from incoming data.\n",
    "\n",
    "This is a two step process: first, to add the new column to the schema of that table, and second to set the function to evaluate.\n",
    "\n",
    "Run the next cell to achieve this.\n",
    "\n",
    "* The `dimensionsSpec` has a new dimension called \"time_friendly\".\n",
    "* The `transformSpec` has a new transform that applies a date formatting function.\n",
    "* The `dataSchema` is then built from these new objects.\n",
    "* The `ingestionSpec` is then built up from the new `dataSchema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d0ea6-bddb-4f47-bfa9-c845ca51dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema_dimensionsSpec = {\n",
    "        \"dimensions\": [\n",
    "          \"user_id\",\n",
    "          \"event_type\",\n",
    "          \"client_ip\",\n",
    "          \"client_device\",\n",
    "          \"client_lang\",\n",
    "          \"client_country\",\n",
    "          \"referrer\",\n",
    "          \"keyword\",\n",
    "          \"product\",\n",
    "          \"time_friendly\"\n",
    "        ]\n",
    "      }\n",
    "\n",
    "dataSchema_transformSpec = {\n",
    "    \"transforms\":\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"keyword\",\n",
    "            \"expression\": \"case_searched((\\\"keyword\\\" == 'None'),null,\\\"keyword\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"product\",\n",
    "            \"expression\": \"case_searched((\\\"product\\\" == 'None'),null,\\\"product\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"referrer\",\n",
    "            \"expression\": \"case_searched((\\\"referrer\\\" == 'unknown'),null,\\\"referrer\\\")\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"expression\",\n",
    "            \"name\": \"time_friendly\",\n",
    "            \"expression\": \"timestamp_format(\\\"__time\\\",'E dd MM yyyy','UTC')\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataSchema = {\n",
<<<<<<< HEAD
    "    \"dataSource\": table_name,\n",
=======
    "    \"dataSource\": topic_name,\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "    \"timestampSpec\": dataSchema_timestampSpec,\n",
    "    \"transformSpec\" : dataSchema_transformSpec,\n",
    "    \"dimensionsSpec\": dataSchema_dimensionsSpec,\n",
    "    \"granularitySpec\": dataSchema_granularitySpec\n",
    "    }\n",
    "\n",
<<<<<<< HEAD
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestion_spec, indent=5))"
=======
    "ingestionSpec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": ioConfig,\n",
    "    \"tuningConfig\": tuningConfig,\n",
    "    \"dataSchema\": dataSchema\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(ingestionSpec, indent=5))"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80b220-e61b-4af5-bf3b-1a2dac73d3f7",
   "metadata": {},
   "source": [
    "Run the next cell to submit this new ingestion specification for the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bacc4-7cc9-4808-a665-62289ce44032",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
=======
    "supervisor = requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestionSpec), headers=generalHeaders)\n",
    "print(supervisor.status_code)"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc861c-3fa2-4421-8acf-5606543c420d",
   "metadata": {},
   "source": [
    "While waiting for the new ingestion to be applied, try this trick for finding the exact expression to put into your ingestion specifications.\n",
    "\n",
    "* Open the Druid console - in the learning environment, this is [http://localhost:8888/](http://localhost:8888/).\n",
    "* Switch to the query view and build a SQL statement against the table using a function you know well.\n",
    "* Instead of running the SQL, use the button with the three dots to find \"Explain SQL query\".\n",
    "* Notice the \"virtual columns\" section contains the equivallent native expression.\n",
    "\n",
    "Now run this cell to see the result of your updated ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74f5cb-1480-4c90-9846-afd8d52d5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
<<<<<<< HEAD
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"time_friendly\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_now}')\n",
=======
    "sql='''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"time_friendly\"\n",
    "FROM \"example-clickstream-transforms\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/''' + time_now + '''')\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc06e50-15dd-49d0-8362-2be5527c8e1e",
   "metadata": {},
   "source": [
    "Running the cell below you will see that the same query for an older time period results in NULL values being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc47e15-a35d-43c5-9dc2-ba4a742b5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_then = (datetime.now() -  timedelta(hours=0, minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
<<<<<<< HEAD
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"time_friendly\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/{time_then}')\n",
=======
    "sql='''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"time_friendly\"\n",
    "FROM \"example-clickstream-transforms\"\n",
    "WHERE TIME_IN_INTERVAL(__time,'PT15S/''' + time_then + '''')\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "ORDER BY __time DESC\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a39b0-6d36-45e8-98be-a4ea62de355c",
   "metadata": {},
   "source": [
    "In the example above, a new column was created from the primary timestamp. As noted in [documentation](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#timestampspec), the same mechanism can be used to overwrite the timestamp (`__time`) when, for example, needing to create a compliant format from non-standard datetime representations before it lands in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to stop the data generator, stop ingestion from the topic, and to remove the table used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
=======
    "print(f\"Stop streaming generator: [{requests.post(f'{druid_host}/stop/{topic_name}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{topic_name}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=topic_name)\n",
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
<<<<<<< HEAD
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
=======
    "    tasks = druid.tasks.tasks(state='running', table=topic_name)\n",
    "        \n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{topic_name}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{topic_name}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(topic_name)}]\")"
>>>>>>> 1474085 (Create 13-native-transforms.ipynb)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Functions can be applied to data from Apache Kafka as soon as it arrives.\n",
    "* SQL functions have native counterparts that you can use as a transform in the `transformSpec`.\n",
    "* Expressions can be used to overwrite data or to create new columns.\n",
    "* Unless the topic offset is reset manually, expressions only apply to new data as it arrives.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try to use some more native functions such as numeric, IP, and more complex string functions.\n",
    "* Re-run this notebook, but manually hard reset the supervisor between posting a new ingestion specification. You can do this either with a [POST](https://druid.apache.org/docs/latest/api-reference/supervisor-api#reset-a-supervisor) or [through the console](https://druid.apache.org/docs/latest/operations/web-console#supervisors). What do you expect to happen?\n",
    "* Refer to the documentation on [native transform expressions](https://druid.apache.org/docs/latest/querying/math-expr)."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
