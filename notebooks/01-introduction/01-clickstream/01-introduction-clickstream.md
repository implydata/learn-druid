#Â Introduction to clickstream

## Background

IT teams in government and commercial organizations began to recognise the value of log data generated by website-hosting technologies like [Apache HTTP Server](https://httpd.apache.org/) and [Microsoft Internet Information Services](https://www.iis.net) some time ago.

For many years, this server log data remained the sole purvue of IT teams wanting to meet OLAs and SLAs, and to prevent and investigate security incidents.

As the internet became critical to business, data like this became valuable for measuring business success - things that have before been used to measure the success of physical channels:

* What kind of people are landing on our product page?
* Where are most of our visitors from?
* What are people's usage habits?
* Do we have return visitors?
* How long does it take for someone to buy our services?
* What questions are people asking about our products when they arrive?

As website visitors, we are all being coaxed toward achieving a particular goal that the owning organization has in mind for us, whether that's placing an order for a product, signing up for a service, or passing on contact information for sales leads.

A _conversion_ is when a visitor achieves one of these goals - and each conversion is assigned a _conversion value_ depending on how important achieving that goal is. Behind that conversion is a _journey_ - understanding and tweaking that journey is a critical reason for doing analytics on clickstream data.

A single journey is understood in the context of a _session_. Along the journey a _visitor_ undertakes a number of _user actions_.

Clickstream analytics concerns questions like:

* What visitor demographics visit our site most?
* Which types of visitor achieve high value goals?
* What actions did someone take prior to a conversion?
* What conversions take longest to achieve?

These are important questions for a number of organizations, including auction websites, news publishers, and video streaming services.

For example, for news publishers, the front page remains the most important real-estate on a website. Clickstream analytics helps advertising campaign managers understand whether the right demographic is reaching this front page as a campaign runs - and whether the campaign has resulted in longer sessions on the site overall.

Server logs, as are used often by IT, are not enough data to answer today's clickstream questions. Event data from a number of different channels need to be combined together to get the big picture, including telephone interactions, mobile app stats, social media interactions, and even physical store transactions.

Clickstream now concerns the entire customer lifecycle: how the attention of the _visitor_ was grabbed, what _user actions_ took place to achieve _conversion_ through a _session_ - no matter the channel - and how that _visitor_ was made loyal.

> "Clickstream analytics puts us closer to our users, and if you know what your users want, you are better able to serve them."

## Challenges

Analytics on clickstream is generally difficult for these reasons:

* It's hard to collect the data.
   * Data can come in from multiple sources, so a database needs to be able to draw from multiple real-time and batch streams - both at ingestion and query time.
   * Data can be very large, so a database must not only be able to scale with the ingestion velocity, but index and compress the data efficiently both for long-term storage and for speedy computation.
* It's hard to query the data.
   * The websites, mobile apps, and other channels change over time. A database needs to be adaptable to changing integrations and schemas.
   * Filtering and statistical needs of uses are often unpredictable, or lead to a large number of reports that have to be maintained. A database needs to be able to cope with a range of query profiles that can be executed in a number of different ways depending on needs.
   * A broad number of clickstream queries concern distinct counts, especially of visitors. Databases need to have ways to calculate distinct counts quickly.
   * Statistics very often concern intersections, unions, and differences - visitors who used a channel today but not in the last 30 days. A database needs to be able to carry out set operations quickly enough for the answers to still be relevant.

The index for this guide signposts you to functionality in Apache Druid that many adoptees are using to solve for these challenges.

## Entities and events

Clickstream data tells the story of what visitors came to use a channel, what they were like, what they did, and whether a goal was achieved.

Clickstream event data aggregates information from one or more entities. Common entities include:

| Entity | Definition | Value |
| --- | --- | --- |
| Visitor | A user.| Driving loyalty. Informing advertising and promotion strategies. Testing churn-reducing tactics. |
| Session | A user's journey. It might include data about what happened before the journey started (like the referring site). | Personalizing "grazing and hunting" experiences. Improving navigation. Attempting to build "first visit, first buy" to prevent attrition. |
| Page | A place in a channel where a visitor can take some actions. | Changing the products or services shown depending on their journey so far. |
| Action | User actions that took place on a page, also known as a click or a hit. | Building up a picture of common journeys. Measuring system responsiveness. |

Events are typically added to, and read from, an event hub such as Apache Kafka, Amazon Kinesis, or Azure Event Hub. Event hubs allow data to be collected globally in one place, and to scale up to hundreds of thousands, if not millions of recorded user actions.

## Pipeline

Event data in web server logs, like W3C Extended Log Files and NCSA Common Format Log Files, is often a starting point for clickstream analytics:

* The date and time of a web request.
* The IP address of the server itself.
* What service was being requested (`POST`, `GET`...)
* The address of the content.
* The IP address of the requesting client.
* What the client is (the "user agent").
* The number of bytes returned.

Network logs might also be used, whether passively or proactively using packet sniffing.

Additional information may be captured from code embedded on the client (visitor) side. Client-side event generation is often richer, able to bring in more information that simply what's being requested by a server.

* Javascript code.
* Pixels.
* Embedded components.

Apache Druid can ingest this data directly, whether from a event hub or in batch, making it queryable quickly.

Before _all_ entities can be analysed, however, events need additional processing using something like Apache Flink or Apache Spark.

* Clickstream data is stateless, it's often very easy to know when a _session_ started, but not so easy to know when a session ended.
* Clickstream data is often anonymous, so _visitor_ data needs to be enriched by joining to an internal database and / or to online enrichment services.

The results of this processing may be posted into data lake technologies, or posted directly into stream event hubs for immediate analysis.

* For _session_ analysis, enough time must elapse for the session to end before certain analysis can be done, such as calculating average session length by a particular _visitor_ demographic.
* For _action_ data, the data must be made available quickly enough for decisions to be taken in a timely manner. For example, A/B testing of a new navigation structure, or determining the effectiveness of a campaign for a flash sale.

Apache Druid ingestion-time transformations allow for row-wise functions to be applied to data as it arrives. More complex enrichment and processing is possible in batch using MSQ. Examples are given in the main portion of the guide.