{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Change table data by using compaction\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Through compaction, whether manual or running automatically, you can change the schema of a table, filter data out, and apply data transformations - whether that is the entire table, or just data of a particular age.\n",
    "\n",
    "This tutorial demonstrates how to work with [compaction](https://druid.apache.org/docs/latest/data-management/compaction) to apply different dimension schemes and to apply transformations.\n",
    "\n",
    "In this tutorial you perform the following tasks:\n",
    "\n",
    "- Create a table using batch ingestion.\n",
    "- Run a compaction task to remove dimensions for a particular time period in the data.\n",
    "- Run a compaction task to apply an expression to the data in a table.\n",
    "- Run a compaction task to remove all data that matches a particular criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 30.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b769122-c5a4-404e-9ef8-9c0ebd97695a",
   "metadata": {},
   "source": [
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5a2ac-f153-4678-8672-e1ec83a7c309",
   "metadata": {},
   "source": [
    "### Import additional modules\n",
    "\n",
    "Run the following cell to import additional Python modules that you will use to call Druid APIs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c87bb-266f-479b-99ad-90486aaf36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "druid_headers = { 'Content-Type': 'application/json' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Create a table using batch ingestion\n",
    "\n",
    "Run the following cell to create a table using batch ingestion. Specific dimensions are selected from the source data.\n",
    "\n",
    "When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'example-wikipedia-compaction'\n",
    "\n",
    "sql='''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  \"namespace\",\n",
    "  \"page\",\n",
    "  \"user\",\n",
    "  \"channel\",\n",
    "  \"added\",\n",
    "  \"deleted\",\n",
    "  \"commentLength\",\n",
    "  \"isRobot\",\n",
    "  \"isAnonymous\",\n",
    "  \"regionIsoCode\",\n",
    "  \"countryIsoCode\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY HOUR\n",
    "'''\n",
    "\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready(f'{table_name}')\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a4eff-369b-416b-98f4-ddabcae59e9c",
   "metadata": {},
   "source": [
    "## Apply changes to data using compaction\n",
    "\n",
    "Compaction is a special type of native [Druid task](https://druid.apache.org/docs/latest/ingestion/tasks#all-task-types) that, like streaming ingestion, uses JSON specifications to define behaviors. Each contains:\n",
    "\n",
    "* An [ioConfig](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-io-configuration), defining what the source data is for the job.\n",
    "* A [tuningConfig](https://druid.apache.org/docs/latest/ingestion/native-batch#tuningconfig), detailing specific controls.\n",
    "* And elements to control what happens to the data (as you would find in a [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema) in streaming ingestion).\n",
    "  * The dimensions to put into the resulting data given in a `dimensionsSpec`.\n",
    "  * Any filters or calculations to do on the data as listed in the `transformsSpec`.\n",
    "  * Any aggregation that should be done, as given in the `metricsSpec` when `rollup` is enabled.\n",
    " \n",
    "In the cells that follow you will see various examples of how to use the `dimensionsSpec` and `transformsSpec` to affect table data as part of compaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93baea-be1d-4d3d-86a0-eac17ce79fd3",
   "metadata": {},
   "source": [
    "### Use dimensionsSpec to add or remove columns\n",
    "\n",
    "Amend the dimensions in the table at compaction time by using a [`dimensionsSpec`](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-dimensions-spec). Options include removing (`dimensionExclusions`) or explicitly including (`dimensions`) specific columns.\n",
    "\n",
    "As each segment defines its own schema, use the SYS.SEGMENTS table to view the dimensions of the table in each partition by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905520bd-c615-4070-b186-0e5bb2036cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT DISTINCT\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"dimensions\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e765785-d545-45e8-afa7-c552254d3236",
   "metadata": {},
   "source": [
    "The cell below will construct a compaction task specification that includes a [`dimensionsSpec`](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-dimensions-spec) to remove specific columns.\n",
    "\n",
    "* The `ioConfig` is represented by `compaction_ioConfig`. It contains an `inputSpec` that has a restriction on the `interval` so that this task only affects data between 19:00 and 20:00.\n",
    "* The `granularitySpec` matches the original PARTITIONED BY.\n",
    "* The `dimensionsSpec` contains an explicit list of `dimensionExclusions` - these are what will be removed.\n",
    "\n",
    "Finally, the `compaction_spec` object uses these objects to create the final JSON compaction specification.\n",
    "\n",
    "Run the cell to print out the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4834b9-d940-4290-a365-b03830e7b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_ioConfig_inputSpec = {\n",
    "    \"type\" : \"interval\",\n",
    "    \"interval\" : \"2016-06-27T19:00:00/PT1H\" }\n",
    "\n",
    "compaction_ioConfig = {\n",
    "    \"type\" : \"compact\",\n",
    "    \"inputSpec\" : compaction_ioConfig_inputSpec }\n",
    "\n",
    "compaction_granularitySpec = { \"segmentGranularity\" : \"HOUR\" }\n",
    "\n",
    "compaction_dimensionsSpec = {\n",
    "    \"dimensionExclusions\" : [ \"namespace\", \"isAnonymous\", \"user\" ] }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec,\n",
    "    \"dimensionsSpec\": compaction_dimensionsSpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4decc7-57d0-4c84-be4b-606580298969",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c356f-c356-4ada-840c-ad6eeb6fc459",
   "metadata": {},
   "source": [
    "Run this cell below to follow along as the compaction task runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fdc93-5a6b-4bd8-aa94-fd185fff0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT DISTINCT\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"dimensions\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead1d15-a329-4dd2-ad26-7a4d9eb8d3c0",
   "metadata": {},
   "source": [
    "When completed, you will see that the data between 1900 and 2000 no longer contains all the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c313b-e68a-462e-8b15-89ff2694e8c8",
   "metadata": {},
   "source": [
    "### Use a transformsSpec to filter out data\n",
    "\n",
    "Incorporate a [native filter](https://druid.apache.org/docs/latest/querying/filters) into a `transformsSpec` during compaction to retain or remove rows that match a particular condition.\n",
    "\n",
    "Run the following cell to retrieve some data from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc033e1-af30-421d-89d3-ce2aef156cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT TIME_FLOOR(\"__time\",'PT1H'),\n",
    "   channel,\n",
    "   COUNT(*) AS \"events\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL (\"__time\",'2016-06-27T00:00/P1D')\n",
    "AND \"channel\" = '#en.wikipedia'\n",
    "GROUP BY 1, 2\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055131e7-e696-4a2c-8988-8424244b5afd",
   "metadata": {},
   "source": [
    "Run the cell below to construct a compaction specification that only retains events where the `channel` is `#en.wikipedia`. Notice that this change will only affect events in the table between 1000 and 1100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d22722-db2a-4597-81d0-eea3899dfc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_ioConfig_inputSpec = {\n",
    "    \"type\" : \"interval\",\n",
    "    \"interval\" : \"2016-06-27T10:00:00/PT1H\" }\n",
    "\n",
    "compaction_ioConfig = {\n",
    "    \"type\" : \"compact\",\n",
    "    \"inputSpec\" : compaction_ioConfig_inputSpec }\n",
    "\n",
    "compaction_granularitySpec = { \"segmentGranularity\" : \"HOUR\" }\n",
    "\n",
    "compaction_transformSpec = {\n",
    "    \"filter\":\n",
    "        {\n",
    "            \"type\": \"selector\",\n",
    "            \"dimension\": \"channel\",\n",
    "            \"value\": \"#en.wikipedia\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec,\n",
    "    \"transformSpec\": compaction_transformSpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c721bf8-24fb-492f-8eb9-1d51e9dc310d",
   "metadata": {},
   "source": [
    "Run the next cell to submit the compaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4b07d-85f1-4427-bffa-adbc70377734",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2b6c9-edf0-4db8-9e62-dc78fafe2b45",
   "metadata": {},
   "source": [
    "Run the cell below to see how this has affected the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6e8-3426-452d-8163-b020a5e232d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT TIME_FLOOR(\"__time\",'PT1H'),\n",
    "   channel,\n",
    "   COUNT(*) AS \"events\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL (\"__time\",'2016-06-27T10:00/PT1H')\n",
    "GROUP BY 1, 2\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd862e-2342-47eb-9800-a88e41d62a83",
   "metadata": {},
   "source": [
    "When completed, the time period selected will _only_ contain rows for a particular channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e80db3-0321-4a3b-87e8-966ac5a07537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply a GROUP BY (rollup) through compaction\n",
    "\n",
    "Use metricsSpec and https://druid.apache.org/docs/latest/data-management/manual-compaction/#compaction-granularity-spec / rollup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the XXX used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for batch ingested tables\n",
    "\n",
    "druid.datasources.drop(f\"{table_name}\")\n",
    "\n",
    "# Use this when doing streaming with the data generator\n",
    "\n",
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You learned this\n",
    "* Remember this\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try this out on your own data\n",
    "* Solve for problem X that is't covered here\n",
    "* Read docs pages\n",
    "* Watch or read something cool from the community\n",
    "* Do some exploratory stuff on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d3362-b1a4-47a4-a782-9773c216b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some useful code elements that you can re-use.\n",
    "\n",
    "# When just wanting to display some SQL results\n",
    "sql = f'''SELECT * FROM \"{table_name}\" LIMIT 5'''\n",
    "display.sql(sql)\n",
    "\n",
    "# When ingesting data and wanting to describe the schema\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready('{table_name}')\n",
    "display.table('{table_name}')\n",
    "\n",
    "# When you want to show the native version of a SQL statement\n",
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))\n",
    "\n",
    "# When you want a simple plot\n",
    "df = pd.DataFrame(sql_client.sql(sql))\n",
    "df.plot(x='x-axis', y='y-axis', marker='o')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.show()\n",
    "\n",
    "# When you want to add some query context parameters\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateTopN\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "# When you want to compare two different sets of results\n",
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3\n",
    "\n",
    "# When you want to see some messages from a Kafka topic\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=kafka_host)\n",
    "consumer.subscribe(topics=datagen_topic)\n",
    "count = 0\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "consumer.unsubscribe()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
