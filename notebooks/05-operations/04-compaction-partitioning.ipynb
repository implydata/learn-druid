{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Optimize table data layout by partitioning and clustering using compaction\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Through compaction, whether manual or running automatically, you can change the number and size of segments that make up a table.\n",
    "\n",
    "This tutorial demonstrates how to work with [compaction](https://druid.apache.org/docs/latest/data-management/compaction) to partition and cluster the segments for an existing table\n",
    "\n",
    "In this tutorial you perform the following tasks:\n",
    "\n",
    "- Create a table using batch ingestion with a very high number of segments.\n",
    "- Run a PARTITION-style compaction job to reduce the number of segments by increasing their size.\n",
    "- Run a CLUSTER-style compaction job to change the secondary partitioning scheme of an existing table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 30.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b769122-c5a4-404e-9ef8-9c0ebd97695a",
   "metadata": {},
   "source": [
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5a2ac-f153-4678-8672-e1ec83a7c309",
   "metadata": {},
   "source": [
    "### Import additional modules\n",
    "\n",
    "Run the following cell to import additional Python modules that you will use to call Druid APIs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c87bb-266f-479b-99ad-90486aaf36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "druid_headers = { 'Content-Type': 'application/json' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Create a table using batch ingestion\n",
    "\n",
    "<!-- Use these cells if you are using batch ingestion for your notebook. -->\n",
    "\n",
    "Run the following cell to create a table using batch ingestion. Notice {the use of X as a timestamp | only required columns are ingested | WHERE / expressions / GROUP BY are front-loaded | partitions on X period and clusters by Y}.\n",
    "\n",
    "When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'example-wikipedia-compaction'\n",
    "\n",
    "sql='''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  \"namespace\",\n",
    "  \"page\",\n",
    "  \"user\",\n",
    "  \"channel\",\n",
    "  \"added\",\n",
    "  \"deleted\",\n",
    "  \"commentLength\",\n",
    "  \"isRobot\",\n",
    "  \"isAnonymous\",\n",
    "  \"regionIsoCode\",\n",
    "  \"countryIsoCode\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY HOUR\n",
    "'''\n",
    "\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready(f'{table_name}')\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc16e5c-1865-4fd6-9f5d-c552acb5261a",
   "metadata": {},
   "source": [
    "## View the layout of a table\n",
    "\n",
    "Use Druid's `SYS.SEGMENTS` table to get information about a TABLE's segments. Run the cell below to see the segments created by the ingestion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4067f-7ba1-46cb-94a2-ea7a7561a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"num_rows\",\n",
    "  \"size\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3062ac4-7070-4c82-8e41-585a88cd64e2",
   "metadata": {},
   "source": [
    "Since you used PARTITIONED BY HOUR, you will see one segment per hour for the entire ingested data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a4eff-369b-416b-98f4-ddabcae59e9c",
   "metadata": {},
   "source": [
    "## Apply changes to data layout through compaction\n",
    "\n",
    "Compaction is a special type of native [Druid task](https://druid.apache.org/docs/latest/ingestion/tasks#all-task-types) that, like streaming ingestion, uses JSON specifications to define behaviors. Each contains:\n",
    "\n",
    "* An [ioConfig](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-io-configuration), defining what the source data is for the job.\n",
    "* A [tuningConfig](https://druid.apache.org/docs/latest/ingestion/native-batch#tuningconfig), detailing specific controls.\n",
    "* And elements to control what happens to the data (as you would find in a [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema) in streaming ingestion).\n",
    "  * The dimensions to put into the resulting data given in a `dimensionsSpec`.\n",
    "  * Any filters or calculations to do on the data as listed in the `transformsSpec`.\n",
    "  * Any aggregation that should be done, as given in the `metricsSpec` when `rollup` is enabled.\n",
    " \n",
    "In the cells that follow you will see various examples of how to use compaction to effect segment layout and table data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65295788-9dde-4154-9bf8-9a7cb33a2951",
   "metadata": {},
   "source": [
    "### Apply a different PARTITION BY scheme\n",
    "\n",
    "Rows per segment being very small is [one reason](https://druid.apache.org/docs/latest/data-management/compaction#compaction-guidelines) to run a compaction job to change the partitioning scheme.\n",
    "\n",
    "To affect the PARTITION BY scheme in compaction you will use a [`granularitySpec`](https://druid.apache.org/docs/latest/data-management/manual-compaction/#compaction-granularity-spec) and a daily primary partitioning scheme by setting the `segmentGranularity` to DAY.\n",
    "\n",
    "Run the next cell to build up a JSON ingestion specification for a compaction job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917a75e-ec43-418c-b180-c33d1ecc27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_ioConfig_inputSpec = {\n",
    "    \"type\" : \"interval\",\n",
    "    \"interval\" : \"1970/2070\" }\n",
    "\n",
    "compaction_ioConfig = {\n",
    "    \"type\" : \"compact\",\n",
    "    \"inputSpec\" : compaction_ioConfig_inputSpec }\n",
    "\n",
    "compaction_granularitySpec = { \"segmentGranularity\" : \"DAY\" }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8eb56-d0e9-4e91-a3a0-f6adc29fced9",
   "metadata": {},
   "source": [
    "Submit the task by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda670d-90b3-44b0-b082-fded4dca5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738958b-2727-480b-9982-89b75851fd20",
   "metadata": {},
   "source": [
    "This task should not take too long to run. Take a look at the segments for the table by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df72f7d-0f59-41b9-ba4b-1d4cf8947b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"num_rows\",\n",
    "  \"size\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2419441-af03-4d60-8a2c-f1322bfd77b0",
   "metadata": {},
   "source": [
    "### Apply a different CLUSTERED BY scheme through compaction\n",
    "\n",
    "Use compaction to apply a data clustering scheme to your table, enabling greater parallelisation and pruning of filtering operations on the dimensions in question. This is particularly important for streaming ingestion.\n",
    "\n",
    "During table creation, no CLUSTERED BY clause was used. Apply a clustering scheme to the table by running a compaction task. The following sewction will apply a [`partitionsSpec`](https://druid.apache.org/docs/latest/ingestion/native-batch-simple-task#partitionsspec) inside the compaction job's [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/native-batch-simple-task#tuningconfig) so that the clustering schemes are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69761f3f-3370-4073-8a47-a4acbb235986",
   "metadata": {},
   "source": [
    "#### See the table layout when hash partitioning is used\n",
    "\n",
    "The `compaction_tuningConfig_partitionsSpec` object contains the configuration needed to partition the table using hashing against the `channel` dimension.\n",
    "\n",
    "This is placed inside the `compaction_tuningConfig` object, which has:\n",
    "\n",
    "* Sets the type to `index_parallel` - this processes the table using the native batch ingestion pipeline, similar to native streaming.\n",
    "* Enables [perfect roll-up](https://druid.apache.org/docs/latest/ingestion/rollup#perfect-rollup-vs-best-effort-rollup) - this is required when partitioning by specific dimensions.\n",
    "\n",
    "These two objects are then incorporated into a new section, `tuningConfig`, in the compaction spec, and submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06dbe8-45ea-4916-8bd7-ed5ca023710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_tuningConfig_partitionsSpec = {\n",
    "    \"type\" : \"hashed\",\n",
    "    \"partitionDimensions\" : [\n",
    "        \"channel\" ] }\n",
    "\n",
    "compaction_tuningConfig = {\n",
    "    \"type\" : \"index_parallel\",\n",
    "    \"forceGuaranteedRollup\" : \"true\",\n",
    "    \"partitionsSpec\" : compaction_tuningConfig_partitionsSpec }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"tuningConfig\" : compaction_tuningConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))\n",
    "\n",
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a25bc-c854-48d5-8f1e-295c0dbb6430",
   "metadata": {},
   "source": [
    "Run the following cell to see the segments for the table. Notice that the `shard_spec` now shows that [Murmur32 hash function](murmur3_32_abs) was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ffa83-776d-42ad-ab9b-63d084c39318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"shard_spec\",\n",
    "  \"num_rows\",\n",
    "  \"size\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e64b7e-6fb0-4058-9982-67c6d221af6e",
   "metadata": {},
   "source": [
    "#### See the table layout when multi-dimension range partitioning is used\n",
    "\n",
    "Now apply the compaction again, this time using [multi-dimension range partitioning](https://druid.apache.org/docs/latest/ingestion/native-batch/#multi-dimension-range-partitioning), which effectively creates a periodic range index across the dimensions.\n",
    "\n",
    "Run the next cell.\n",
    "\n",
    "Notice that the `partitionsSpec` has been changed to use `range`-type partitioning, and that multiple dimensions will be used. For the purposes of this notebook, a (for example purposes only!) target of 10000 rows per segment has been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28554eec-faef-4c13-b554-0710cc48ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_tuningConfig_partitionsSpec = {\n",
    "    \"type\" : \"range\",\n",
    "    \"partitionDimensions\" : [\n",
    "        \"isRobot\", \"channel\" ],\n",
    "    \"targetRowsPerSegment\" : 10000 }\n",
    "\n",
    "compaction_tuningConfig = {\n",
    "    \"type\" : \"index_parallel\",\n",
    "    \"forceGuaranteedRollup\" : \"true\",\n",
    "    \"partitionsSpec\" : compaction_tuningConfig_partitionsSpec }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"tuningConfig\" : compaction_tuningConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))\n",
    "\n",
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b1c14-6fae-4b6c-b6f9-5a0ed097e030",
   "metadata": {},
   "source": [
    "See what the table segments look like by running the SQL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a7c9-e329-4e64-bb12-fdf3f3581c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"shard_spec\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f2c56-3a93-4714-a287-83d789e64fee",
   "metadata": {},
   "source": [
    "For each PARTITION (which is DAY) there is now a set of files for a range of values shown in the `shard_spec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93baea-be1d-4d3d-86a0-eac17ce79fd3",
   "metadata": {},
   "source": [
    "## Change a table schema through compaction\n",
    "\n",
    "As each segment defines its own schema, use the SYS.SEGMENTS table to view the dimensions of the table by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905520bd-c615-4070-b186-0e5bb2036cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT DISTINCT\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"dimensions\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e765785-d545-45e8-afa7-c552254d3236",
   "metadata": {},
   "source": [
    "By using the [`dimensionsSpec`](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-dimensions-spec) in a compaction task, remove some dimensions from the table for a particular hour in the data.\n",
    "\n",
    "The cell below will construct a compaction task specification using a [`dimensionsSpec`](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-dimensions-spec) so that some of the original dimensions are removed, but _only_ between 19:00 and 20:00.\n",
    "\n",
    "Run the cell to print out the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4834b9-d940-4290-a365-b03830e7b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_ioConfig_inputSpec = {\n",
    "    \"type\" : \"interval\",\n",
    "    \"interval\" : \"2016-06-27T19:00:00/PT1H\" }\n",
    "\n",
    "compaction_ioConfig = {\n",
    "    \"type\" : \"compact\",\n",
    "    \"inputSpec\" : compaction_ioConfig_inputSpec }\n",
    "\n",
    "compaction_granularitySpec = { \"segmentGranularity\" : \"HOUR\" }\n",
    "\n",
    "compaction_dimensionsSpec = {\n",
    "    \"dimensionExclusions\" : [ \"namespace\", \"isAnonymous\", \"user\" ] }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"tuningConfig\" : compaction_tuningConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec,\n",
    "    \"dimensionsSpec\": compaction_dimensionsSpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aadcfc-71cd-4209-93e5-2a6b688bd092",
   "metadata": {},
   "source": [
    "Taking a look at the specification, notice:\n",
    "\n",
    "* The `interval` of the `inputSpec` (ie, what data will be processed) uses a period of one hour after 7pm.\n",
    "* Range partitioning will still be applied.\n",
    "* The `granularitySpec` is set to `HOUR` - this is required since the table must be broken down small enough for the `interval` to be applied.\n",
    "* The `namespace`, `isAnonymous`, and `user` dimensions will be removed from the table.\n",
    "\n",
    "Run the cell below to begin the compaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4decc7-57d0-4c84-be4b-606580298969",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c356f-c356-4ada-840c-ad6eeb6fc459",
   "metadata": {},
   "source": [
    "Run this cell to see what the new table's segments look like, complete with a list of their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fdc93-5a6b-4bd8-aa94-fd185fff0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT DISTINCT\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"shard_spec\",\n",
    "    \"dimensions\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778154e-19f9-4671-98c2-bc291a39aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "And, sure enough, the data is missing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6e8-3426-452d-8163-b020a5e232d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"__time\",\n",
    "  \"namespace\",\n",
    "  \"page\",\n",
    "  \"user\",\n",
    "  \"channel\",\n",
    "  \"isRobot\",\n",
    "  \"isAnonymous\"\n",
    "FROM \"{table_name}\"\n",
    "WHERE TIME_IN_INTERVAL(__time, '2016-06-27T18:59:30/PT1M')\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e80db3-0321-4a3b-87e8-966ac5a07537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply a GROUP BY (rollup) through compaction\n",
    "\n",
    "Use metricsSpec and https://druid.apache.org/docs/latest/data-management/manual-compaction/#compaction-granularity-spec / rollup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the XXX used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for batch ingested tables\n",
    "\n",
    "druid.datasources.drop(f\"{table_name}\")\n",
    "\n",
    "# Use this when doing streaming with the data generator\n",
    "\n",
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You learned this\n",
    "* Remember this\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try this out on your own data\n",
    "* Solve for problem X that is't covered here\n",
    "* Read docs pages\n",
    "* Watch or read something cool from the community\n",
    "* Do some exploratory stuff on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d3362-b1a4-47a4-a782-9773c216b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some useful code elements that you can re-use.\n",
    "\n",
    "# When just wanting to display some SQL results\n",
    "sql = f'''SELECT * FROM \"{table_name}\" LIMIT 5'''\n",
    "display.sql(sql)\n",
    "\n",
    "# When ingesting data and wanting to describe the schema\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready('{table_name}')\n",
    "display.table('{table_name}')\n",
    "\n",
    "# When you want to show the native version of a SQL statement\n",
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))\n",
    "\n",
    "# When you want a simple plot\n",
    "df = pd.DataFrame(sql_client.sql(sql))\n",
    "df.plot(x='x-axis', y='y-axis', marker='o')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.show()\n",
    "\n",
    "# When you want to add some query context parameters\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateTopN\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "# When you want to compare two different sets of results\n",
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3\n",
    "\n",
    "# When you want to see some messages from a Kafka topic\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=kafka_host)\n",
    "consumer.subscribe(topics=datagen_topic)\n",
    "count = 0\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "consumer.unsubscribe()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
