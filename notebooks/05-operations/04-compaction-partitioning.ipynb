{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Optimize table data layout by partitioning and clustering using compaction\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Through compaction, whether manual or running automatically, you can change the number and size of segments that make up a table.\n",
    "\n",
    "This tutorial demonstrates how to work with [compaction](https://druid.apache.org/docs/latest/data-management/compaction) to partition and cluster the segments for an existing table.\n",
    "\n",
    "In this tutorial you perform the following tasks:\n",
    "\n",
    "- Create a table using batch ingestion with a very high number of segments.\n",
    "- Re-partition your data using a compaction job, reducing the number of segments by increasing their size.\n",
    "- Re-cluster your data by changing the secondary partitioning scheme of an existing table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 29.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b769122-c5a4-404e-9ef8-9c0ebd97695a",
   "metadata": {},
   "source": [
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5a2ac-f153-4678-8672-e1ec83a7c309",
   "metadata": {},
   "source": [
    "### Import additional modules\n",
    "\n",
    "Run the following cell to import additional Python modules that you will use to call Druid APIs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c87bb-266f-479b-99ad-90486aaf36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Create a table using batch ingestion\n",
    "\n",
    "Run the following cell to create a table using batch ingestion. Notice that this ingestion will partition the incoming data by hour.\n",
    "\n",
    "When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'example-wikipedia-compaction'\n",
    "\n",
    "sql='''\n",
    "REPLACE INTO \"''' + table_name + '''\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  \"namespace\",\n",
    "  \"page\",\n",
    "  \"user\",\n",
    "  \"channel\",\n",
    "  \"added\",\n",
    "  \"deleted\",\n",
    "  \"commentLength\",\n",
    "  \"isRobot\",\n",
    "  \"isAnonymous\",\n",
    "  \"regionIsoCode\",\n",
    "  \"countryIsoCode\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY HOUR\n",
    "'''\n",
    "\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready(f'{table_name}')\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc16e5c-1865-4fd6-9f5d-c552acb5261a",
   "metadata": {},
   "source": [
    "## View the layout of a table\n",
    "\n",
    "Use Druid's `SYS.SEGMENTS` table to get information about a TABLE's segments. Run the cell below to see the segments created by the ingestion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4067f-7ba1-46cb-94a2-ea7a7561a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"size\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3062ac4-7070-4c82-8e41-585a88cd64e2",
   "metadata": {},
   "source": [
    "Since you used PARTITIONED BY HOUR, you will see one segment per hour for the entire ingested data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a4eff-369b-416b-98f4-ddabcae59e9c",
   "metadata": {},
   "source": [
    "## Apply changes to data layout through compaction\n",
    "\n",
    "Compaction is a special type of native [Druid task](https://druid.apache.org/docs/latest/ingestion/tasks#all-task-types) that, like streaming ingestion, uses JSON specifications to define behaviors. Each contains:\n",
    "\n",
    "* An [ioConfig](https://druid.apache.org/docs/latest/data-management/manual-compaction#compaction-io-configuration), defining the connection to the source data.\n",
    "* A [tuningConfig](https://druid.apache.org/docs/latest/ingestion/native-batch#tuningconfig), detailing specific controls.\n",
    "* What happens to the incoming data (as you would find in a [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema) in streaming ingestion).\n",
    "  * The dimensions to put into the resulting data given in a `dimensionsSpec`.\n",
    "  * Any filters or calculations to do on the data as listed in the `transformsSpec`.\n",
    "  * Any aggregation that should be done, as given in the `metricsSpec` when `rollup` is enabled.\n",
    " \n",
    "In the cells that follow you will see various examples of how to use compaction to effect segment layout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65295788-9dde-4154-9bf8-9a7cb33a2951",
   "metadata": {},
   "source": [
    "### Apply a different PARTITION BY scheme\n",
    "\n",
    "Rows per segment being very small is [one reason](https://druid.apache.org/docs/latest/data-management/compaction#compaction-guidelines) to run a compaction job to change the partitioning scheme.\n",
    "\n",
    "To affect the PARTITION BY scheme in compaction you will use a [`granularitySpec`](https://druid.apache.org/docs/latest/data-management/manual-compaction/#compaction-granularity-spec) and a daily primary partitioning scheme by setting the `segmentGranularity` to DAY.\n",
    "\n",
    "Run the next cell to build up a JSON ingestion specification for a compaction job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917a75e-ec43-418c-b180-c33d1ecc27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_ioConfig_inputSpec = {\n",
    "    \"type\" : \"interval\",\n",
    "    \"interval\" : \"1970/2070\" }\n",
    "\n",
    "compaction_ioConfig = {\n",
    "    \"type\" : \"compact\",\n",
    "    \"inputSpec\" : compaction_ioConfig_inputSpec }\n",
    "\n",
    "compaction_granularitySpec = { \"segmentGranularity\" : \"DAY\" }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8eb56-d0e9-4e91-a3a0-f6adc29fced9",
   "metadata": {},
   "source": [
    "Submit the task by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda670d-90b3-44b0-b082-fded4dca5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738958b-2727-480b-9982-89b75851fd20",
   "metadata": {},
   "source": [
    "Wait for a moment for the task to run.\n",
    "\n",
    "Keep running the cell below, and you will see the table data layout change to just one segment for the entire table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df72f7d-0f59-41b9-ba4b-1d4cf8947b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"size\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2419441-af03-4d60-8a2c-f1322bfd77b0",
   "metadata": {},
   "source": [
    "### Apply a different CLUSTERED BY scheme through compaction\n",
    "\n",
    "During the initial ingestion that you performed, notice that there was no CLUSTERED BY clause and, in this way, mimics what happens with streaming ingestion where secondary partitioning is not applied.\n",
    "\n",
    "Use the [`partitionsSpec`](https://druid.apache.org/docs/latest/ingestion/native-batch-simple-task#partitionsspec) inside a compaction job's [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/native-batch-simple-task#tuningconfig) to apply a data clustering scheme to your table, enabling greater parallelisation and pruning of filtering operations on the dimensions you cluster by.\n",
    "\n",
    "In the sections that follow, you will apply two techniques: hash and multi-dimension range partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69761f3f-3370-4073-8a47-a4acbb235986",
   "metadata": {},
   "source": [
    "#### See the table layout when hash partitioning is used\n",
    "\n",
    "To set up hash partitioning, a `partitionSpec` needs to be added to the `tuningConfig` of the compaction job definition.\n",
    "\n",
    "Run the next cell to create a `compaction_tuningConfig_partitionsSpec` object. Notice that it contains the configuration needed to partition the table using hashing against the `channel` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017fa95-a7b9-4ba8-95ba-9525211e0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_tuningConfig_partitionsSpec = {\n",
    "    \"type\" : \"hashed\",\n",
    "    \"partitionDimensions\" : [\n",
    "        \"channel\" ] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c12e12-4508-49cb-936c-b399f555abb3",
   "metadata": {},
   "source": [
    "Now define a `compaction_tuningConfig` object that will act as the `tuningConfig`.\n",
    "\n",
    "For compaction, this:\n",
    "\n",
    "* Sets the type to `index_parallel` - this instructs the compaction job to process the table data using native tasks.\n",
    "* Enables [perfect roll-up](https://druid.apache.org/docs/latest/ingestion/rollup#perfect-rollup-vs-best-effort-rollup) - this is required when partitioning by specific dimensions.\n",
    "\n",
    "These two objects are then incorporated into a new section, `tuningConfig`, in the compaction spec, and submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6e5b7-bdc7-42eb-b0cf-fd2f567be0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_tuningConfig = {\n",
    "    \"type\" : \"index_parallel\",\n",
    "    \"forceGuaranteedRollup\" : \"true\",\n",
    "    \"partitionsSpec\" : compaction_tuningConfig_partitionsSpec }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8b737-abb9-4c03-b683-7c60bdfa2cbc",
   "metadata": {},
   "source": [
    "Run the next cell to incorporate this new section into the compaction specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06dbe8-45ea-4916-8bd7-ed5ca023710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"tuningConfig\" : compaction_tuningConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469dc61-16a5-4920-9a94-e736f024d777",
   "metadata": {},
   "source": [
    "Run the next cell to submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f9621-5beb-4278-a3ba-d4050bae96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a25bc-c854-48d5-8f1e-295c0dbb6430",
   "metadata": {},
   "source": [
    "Run the following cell to watch as the compaction job applies a new data layout to the table.\n",
    "\n",
    "When finished, you will see that the `shard_spec` shows the [Murmur32 hash function](murmur3_32_abs) was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ffa83-776d-42ad-ab9b-63d084c39318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"shard_spec\",\n",
    "  \"num_rows\",\n",
    "  \"size\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e64b7e-6fb0-4058-9982-67c6d221af6e",
   "metadata": {},
   "source": [
    "#### See the table layout when multi-dimension range partitioning is used\n",
    "\n",
    "Now apply the compaction again, this time using [multi-dimension range partitioning](https://druid.apache.org/docs/latest/ingestion/native-batch/#multi-dimension-range-partitioning), which effectively creates a periodic range index across the dimensions.\n",
    "\n",
    "Notice that the `partitionsSpec` will use `range`-type partitioning, and that multiple dimensions will be used. For the purposes of this notebook, a (for example purposes only!) target of 10000 rows per segment has been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28554eec-faef-4c13-b554-0710cc48ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "compaction_tuningConfig_partitionsSpec = {\n",
    "    \"type\" : \"range\",\n",
    "    \"partitionDimensions\" : [\n",
    "        \"isRobot\", \"channel\" ],\n",
    "    \"targetRowsPerSegment\" : 10000 }\n",
    "\n",
    "compaction_tuningConfig = {\n",
    "    \"type\" : \"index_parallel\",\n",
    "    \"forceGuaranteedRollup\" : \"true\",\n",
    "    \"partitionsSpec\" : compaction_tuningConfig_partitionsSpec }\n",
    "\n",
    "compaction_spec = {\n",
    "    \"type\": \"compact\",\n",
    "    \"dataSource\": table_name,\n",
    "    \"ioConfig\": compaction_ioConfig,\n",
    "    \"tuningConfig\" : compaction_tuningConfig,\n",
    "    \"granularitySpec\": compaction_granularitySpec\n",
    "}\n",
    "\n",
    "print(json.dumps(compaction_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b741f-1d6f-46eb-8455-ab185228ec2a",
   "metadata": {},
   "source": [
    "Run the next cell to submit the compaction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d02b88-7813-4459-b794-b7ca5e0102da",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(f\"{druid_host}/druid/indexer/v1/task\", json.dumps(compaction_spec), headers=druid_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b1c14-6fae-4b6c-b6f9-5a0ed097e030",
   "metadata": {},
   "source": [
    "As before, run the cell below to watch as the new layout is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a7c9-e329-4e64-bb12-fdf3f3581c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=f'''\n",
    "SELECT\n",
    "  \"start\",\n",
    "  \"end\",\n",
    "  \"shard_spec\"\n",
    "FROM sys.segments\n",
    "WHERE datasource = '{table_name}'\n",
    "ORDER BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f2c56-3a93-4714-a287-83d789e64fee",
   "metadata": {},
   "source": [
    "When the task is finished, you will see two segment files, with ranges of partition values shown in the `shard_spec` - the first runs from the start of the data (\"start\":null) through to isRobot:false; channel:#pl.wikipedia, the second from that same position through to the end (\"end\":null). Consequently, queries that compare values below / above this cut line will be parallelised across two segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to drop the table used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Compaction processes existing table data.\n",
    "* Compaction can run in manual or automatic mode.\n",
    "* Manually apply a different primary partitioning by running a compaction task using `segmentGranularity`.\n",
    "* Incorporate a `partitionsSpec` to apply partitioning on other dimensions.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Read about [time-based partitioning](https://druid.apache.org/docs/latest/multi-stage-query/concepts#partitioning-by-time) in Druid.\n",
    "* Also read about performance and storage [impacts of clustering](https://druid.apache.org/docs/latest/multi-stage-query/concepts#clustering) table data.\n",
    "* Check out the docs on [compaction](https://druid.apache.org/docs/latest/data-management/compaction), and how to apply automatic compaction.\n",
    "* Try out different partitioning methods and assess the segment layout.\n",
    "* Move on to pushing your compaction specification to automatic compaction for the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b8856-da3a-4b77-9679-ed331edeb12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
