{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# (Result) by (action) using (feature)\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "\n",
    "Introductory paragraph - for example:\n",
    "\n",
    "This tutorial demonstrates how to work with [feature](link to feature doc). In this tutorial you perform the following tasks:\n",
    "\n",
    "- Task 1\n",
    "- Task 2\n",
    "- Task 3\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid XX.0.0 or later.\n",
    "\n",
    "<!-- Profiles are:\n",
    "`druid-jupyter` - just Jupyter and Druid\n",
    "`all-services` - includes Jupyter, Druid, and Kafka\n",
    " -->\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `<PLACE PROFILE NAME HERE>` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b769122-c5a4-404e-9ef8-9c0ebd97695a",
   "metadata": {},
   "source": [
    "### Set up a connection to Apache Druid\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "druid_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdbee0-62da-4fd3-84e1-f66b8c0150b3",
   "metadata": {},
   "source": [
    "###Â Set up a connection to Apache Kafka\n",
    "\n",
    "<!-- Include these cells if your notebook uses Kafka. -->\n",
    "\n",
    "Run the next cell to set up the connection to Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075de81-04c9-4b23-8253-20a15d46252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffed9a-87a5-4b26-9f06-972fdbccd55a",
   "metadata": {},
   "source": [
    "### Set up a connection to the Data Generator\n",
    "\n",
    "<!-- Include these cells if your notebook uses the data generator. -->\n",
    "\n",
    "Run the next cell to set up the connection to the Data Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807280f-b387-4ffb-8bc3-764c5b511458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "datagen_host = \"http://datagen:9999\"\n",
    "datagen_headers = {'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d6b39-6551-4b2a-bdfb-9606aa92c853",
   "metadata": {},
   "source": [
    "<!-- Include these cells if you need additional Python modules -->\n",
    "\n",
    "### Import additional modules\n",
    "\n",
    "Run the following cell to import additional Python modules that you will use to X, Y, Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c2524-0eba-4bc6-84ed-da3a25aa5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your modules here, remembering to align this with the prerequisites section\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Create a table using batch ingestion\n",
    "\n",
    "<!-- Use these cells if you are using batch ingestion for your notebook. -->\n",
    "\n",
    "Run the following cell to create a table using batch ingestion. Notice {the use of X as a timestamp | only required columns are ingested | WHERE / expressions / GROUP BY are front-loaded | partitions on X period and clusters by Y}.\n",
    "\n",
    "When completed, you'll see a description of the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace example-dataset-topic with a unique table name for this notebook.\n",
    "\n",
    "# - Always prefix your table name with `example-`\n",
    "# - If using the standard example datasets, use the following standard values for `dataset`:\n",
    "\n",
    "#     wikipedia       wikipedia\n",
    "#     koalas          KoalasToTheMax one day\n",
    "#     koalanest       KoalasToTheMax one day (nested)\n",
    "#     nyctaxi3        NYC Taxi cabs (3 files)\n",
    "#     nyctaxi         NYC Taxi cabs (all files)\n",
    "#     flights         FlightCarrierOnTime (1 month)\n",
    "\n",
    "# Remember to apply good data modelling practice to your INSERT / REPLACE.\n",
    "\n",
    "table_name = 'example-dataset-topic'\n",
    "\n",
    "sql=f'''\n",
    "REPLACE INTO {table_name}...\n",
    "'''\n",
    "\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready(f'{table_name}')\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ff0b2-ee98-40d6-b0b7-77274e34881d",
   "metadata": {},
   "source": [
    "## Create a table using streaming ingestion\n",
    "\n",
    "In this section, you use the data generator to generate a stream of messages into a Apache Kafka topic. Next, you'll set up an on-going ingestion into Druid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b732-c843-4483-896d-c7972ba9e4be",
   "metadata": {},
   "source": [
    "### Use the data generator to populate a Kafka topic\n",
    "\n",
    "Run the following cell to instruct the data generator to start producing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ef424-cd35-416c-9cd0-14f71d43d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more information on the available configurations and settings for the data generator, see the dedicated notebook in \"99-contributing\"\n",
    "\n",
    "datagen_job = f\"{table_name}\"\n",
    "datagen_topic = f\"{table_name}\"\n",
    "datagen_config = \"social/social_posts.json\"\n",
    "\n",
    "datagen_request = {\n",
    "    \"name\": datagen_job,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": datagen_topic  },\n",
    "    \"config_file\": datagen_config, \n",
    "    \"time\":\"10m\",\n",
    "    \"concurrency\":100\n",
    "}\n",
    "\n",
    "requests.post(f\"{datagen_host}/start\", json.dumps(datagen_request), headers=datagen_headers)\n",
    "display(requests.get(f\"{datagen_host}/status/{datagen_job}\").json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fedf1f-566f-4501-be65-d20e216d2c59",
   "metadata": {},
   "source": [
    "### Use streaming ingestion to populate the table\n",
    "\n",
    "Ingest data from an Apache Kafka topic into Apache Druid by submitting an [ingestion specification](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html) to the [streaming ingestion supervisor API](https://druid.apache.org/docs/latest/api-reference/supervisor-api).\n",
    "\n",
    "Run the next cell to set up the [`ioConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#ioconfig), [`tuningConfig`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#tuningconfig), and [`dataSchema`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec#dataschema). Notice {the use of X as a timestamp | only required columns are ingested | WHERE / expressions / GROUP BY are front-loaded | partitions on X period and clusters by Y}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8734c-e968-45de-a1bf-197d3bffb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioConfig = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"consumerProperties\": { \"bootstrap.servers\": kafka_host },\n",
    "    \"topic\": datagen_topic,\n",
    "    \"inputFormat\": { \"type\": \"json\" },\n",
    "    \"useEarliestOffset\": \"true\" }\n",
    "\n",
    "tuningConfig = { \"type\": \"kafka\" }\n",
    "\n",
    "# Replace example-dataset-topic with a unique table name for this notebook.\n",
    "\n",
    "# - Always prefix your table name with `example-`\n",
    "# - If using the standard example datasets, use the following standard values for `dataset`:\n",
    "\n",
    "#     social/socialposts            social\n",
    "#     clickstream/clickstream       clickstream\n",
    "\n",
    "# Remember to apply good data modelling practice to your data schema.\n",
    "\n",
    "table_name = 'example-dataset-topic'\n",
    "\n",
    "dataSchema = {\n",
    "    \"dataSource\": table_name,\n",
    "    \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "    \"granularitySpec\": { \"rollup\": \"false\", \"segmentGranularity\": \"hour\" },\n",
    "    \"dimensionsSpec\": { \"useSchemaDiscovery\" : \"true\"}\n",
    "    }\n",
    "\n",
    "ingestion_spec = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"spec\": {\n",
    "        \"ioConfig\": ioConfig,\n",
    "        \"tuningConfig\": tuningConfig,\n",
    "        \"dataSchema\": dataSchema\n",
    "    }\n",
    "}\n",
    "\n",
    "requests.post(f\"{druid_host}/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=druid_headers)\n",
    "druid.sql.wait_until_ready(table_name, verify_load_status=False)\n",
    "display.table(f'{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c9b88-837d-4c80-a28d-36184ba63355",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Awesome!\n",
    "\n",
    "The main body of your notebook goes here!\n",
    "\n",
    "### This is a step\n",
    "\n",
    "Here things get done\n",
    "\n",
    "### And so is this!\n",
    "\n",
    "Wow! Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove the XXX used in this notebook from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for batch ingested tables\n",
    "\n",
    "druid.datasources.drop(f\"{table_name}\")\n",
    "\n",
    "# Use this when doing streaming with the data generator\n",
    "\n",
    "print(f\"Stop streaming generator: [{requests.post(f'{datagen_host}/stop/{datagen_job}','')}]\")\n",
    "print(f'Pause streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/suspend\",\"\")}]')\n",
    "\n",
    "print(f'Shutting down running tasks ...')\n",
    "\n",
    "tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "while len(tasks)>0:\n",
    "    for task in tasks:\n",
    "        print(f\"...stopping task [{task['id']}]\")\n",
    "        druid.tasks.shut_down_task(task['id'])\n",
    "    tasks = druid.tasks.tasks(state='running', table=table_name)\n",
    "\n",
    "print(f'Reset offsets for re-runnability: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/reset\",\"\")}]')\n",
    "print(f'Terminate streaming ingestion: [{requests.post(f\"{druid_host}/druid/indexer/v1/supervisor/{datagen_topic}/terminate\",\"\")}]')\n",
    "print(f\"Drop datasource: [{druid.datasources.drop(table_name)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* You learned this\n",
    "* Remember this\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Try this out on your own data\n",
    "* Solve for problem X that is't covered here\n",
    "* Read docs pages\n",
    "* Watch or read something cool from the community\n",
    "* Do some exploratory stuff on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d3362-b1a4-47a4-a782-9773c216b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some useful code elements that you can re-use.\n",
    "\n",
    "# When just wanting to display some SQL results\n",
    "sql = f'''SELECT * FROM \"{table_name}\" LIMIT 5'''\n",
    "display.sql(sql)\n",
    "\n",
    "# When ingesting data and wanting to describe the schema\n",
    "display.run_task(sql)\n",
    "sql_client.wait_until_ready('{table_name}')\n",
    "display.table('{table_name}')\n",
    "\n",
    "# When you want to show the native version of a SQL statement\n",
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))\n",
    "\n",
    "# When you want a simple plot\n",
    "df = pd.DataFrame(sql_client.sql(sql))\n",
    "df.plot(x='x-axis', y='y-axis', marker='o')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.show()\n",
    "\n",
    "# When you want to add some query context parameters\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateTopN\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "# When you want to compare two different sets of results\n",
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3\n",
    "\n",
    "# When you want to see some messages from a Kafka topic\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=kafka_host)\n",
    "consumer.subscribe(topics=datagen_topic)\n",
    "count = 0\n",
    "for message in consumer:\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    print (\"%d:%d: v=%s\" % (message.partition,\n",
    "                            message.offset,\n",
    "                            message.value))\n",
    "consumer.unsubscribe()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
